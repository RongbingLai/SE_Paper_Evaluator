Script started on Thu May 16 15:31:10 2024
[1m[7m%[27m[1m[0m                                                                                                                                   [0m[27m[24m[J(base) crystalalice@LostRabbits-MacBook-Pro SE_Paper_Evaluator % [K[?2004h[?2004l
[1m[7m%[27m[1m[0m                                                                                                                                   [0m[27m[24m[J(base) crystalalice@LostRabbits-MacBook-Pro SE_Paper_Evaluator % [K[?2004h[?2004l
 cd /Users/crystalalice/Desktop/ICSHP_Research/SE_[1m[7m%[27m[1m[0m                                                                                                                                   [0m[27m[24m[J(base) crystalalice@LostRabbits-MacBook-Pro SE_Paper_Evaluator % [K[?2004h  cd /Users/crystalalice/Desktop/ICSHP_Research/SE_Paper_Evaluator  [K;; /usr/bin/env /Users/crystalalice/.local/share/virtualenvs/SE_Paper_Evaluator-ClTlGgRY/bin/python /Users/crystalalice/.vscode/exte [Knnsions/ms-python.debugpy-2024.6.0-darwin-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher 57622 -- /Users/crystalalice/Deskt [Koop/ICSHP_Research/SE_Paper_Evaluator/main.py [?2004l
/Users/crystalalice/.local/share/virtualenvs/SE_Paper_Evaluator-ClTlGgRY/lib/python3.11/site-packages/langchain/_api/module_import.py:87: LangChainDeprecationWarning: Importing GuardrailsOutputParser from langchain.output_parsers is deprecated. Please replace the import with the following:
from langchain_community.output_parsers.rail_parser import GuardrailsOutputParser
  warnings.warn(
/Users/crystalalice/.local/share/virtualenvs/SE_Paper_Evaluator-ClTlGgRY/lib/python3.11/site-packages/smashed/utils/warnings.py:27: RuntimeWarning: smashed.interfaces.simple is deprecated; import from smashed.mappers instead.
  cls._warn(message, RuntimeWarning)


[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3mThought: I need to start by fetching all section titles from the manuscript.
Action: Fetch All Section Titles
Action Input: /Users/crystalalice/Desktop/ICSHP_Research/SE_paper/Software_Documentation_Issues_Unveiled.pdf[0m  0%|                                                                                                       | 0/12 [00:00<?, ?it/s]0.01s - Debugger warning: It seems that frozen modules are being used, which may
0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off
0.00s - to python to disable frozen modules.
0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.
  8%|███████▉                                                                                       | 1/12 [00:00<00:07,  1.57it/s] 17%|███████████████▊                                                                               | 2/12 [00:01<00:05,  1.71it/s] 25%|███████████████████████▊                                                                       | 3/12 [00:01<00:05,  1.68it/s] 33%|███████████████████████████████▋                                                               | 4/12 [00:02<00:04,  1.76it/s] 42%|███████████████████████████████████████▌                                                       | 5/12 [00:02<00:03,  1.83it/s] 50%|███████████████████████████████████████████████▌                                               | 6/12 [00:03<00:03,  1.90it/s] 58%|███████████████████████████████████████████████████████▍                                       | 7/12 [00:03<00:02,  1.97it/s] 67%|███████████████████████████████████████████████████████████████▎                               | 8/12 [00:04<00:02,  1.96it/s] 75%|███████████████████████████████████████████████████████████████████████▎                       | 9/12 [00:04<00:01,  1.89it/s] 83%|██████████████████████████████████████████████████████████████████████████████▎               | 10/12 [00:05<00:01,  1.80it/s] 92%|██████████████████████████████████████████████████████████████████████████████████████▏       | 11/12 [00:06<00:00,  1.80it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.67it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.78it/s]
[36;1m[1;3m['I. I NTRODUCTION', 'II. R ELATED W ORK', 'API documentation', 'III. E MPIRICAL S TUDY D ESIGN', 'A. Data Collection', 'B. Data Analysis', 'C. Replication Package', 'IV. R ESULTS D ISCUSSION', 'A. Information Content (What)', 'B. Information Content (How)', 'C. Tool Related', 'D. Process Related', 'V. T HREATS TO V ALIDITY', 'VI. C ONCLUSIONS', 'A CKNOWLEDGMENT', 'R EFERENCES', 'A RTIFACTS ’ R EFERENCES'][0m[32;1m[1;3mAction: Fetch Section Content by Title
Action Input: {"section_title": "I. I NTRODUCTION", "path": "/Users/crystalalice/Desktop/ICSHP_Research/SE_paper/Software_Documentation_Issues_Unveiled.pdf"}[0m[33;1m[1;3mGood old documentation,
the ideal companion of any
software system, is intended to provide stakeholders with useful
knowledge about the system and related processes. Depending
on the target audience, the contents of documentation varies.
For example, technical documentation (e.g., API reference
guides) describes information about the design, code, interfaces
and functionality of software to support developers in their
tasks, while user documentation (e.g., user manuals) explains
to end-users how they should use the software application.
Despite the undeniable practical beneﬁts of documentation
during software development and evolution activities [1]–[4],
its creation and maintenance have been often neglected [1],
[3]–[7], leading to inadequate and even inexistent documenta-
tion. These and other aspects of documentation (e.g., needs,
learning obstacles) have been investigated through interviews
with and surveys of practitioners, with the general goal of
identifying the root causes of documentation issues (e.g.,
inaccuracy, outdatedness). To address these issues (at least
partially), different approaches and tools have been proposed
to aid developers during software documentation, including its
automatic generation [1], [8]. For example, a recent proposal by
Robillard et al. [9] suggests a paradigm shift towards systems
that automatically generate documentation in response to a
developer’s query, while considering her working context.
However, to achieve high-quality automatic documentation
systems, we require ﬁrst a deep understanding of software
practitioners’ needs. Although existing studies have revealed
some of these needs, their results are limited by the low
number and lack of diversity of practitioners questioned and
documentation artifacts analyzed.
To overcome these limitations, we qualitatively analyzed
different types of artifacts from diverse data sources and
identiﬁed the issues that developers face when dealing with
documentation. Speciﬁcally, we mined open source software
repositories and examined a set of 878 artifacts corresponding
to development emails, programming forum discussions, issues
and pull requests related to software documentation. For
each artifact, we determined the reported issue, the type of
documentation presenting it, and the proposed solution, as well
as the documentation tools discussed. Based on our analysis,
we built a comprehensive taxonomy consisting of 162 types of
documentation issues linked to (i) the information it contains,
(ii) how the information is presented, (iii) the documentation
process and (iv) documentation tool support. We describe and
exemplify each category of documentation issues. We also
discuss their implications in software research and practice,
deriving actionable items needed to address them.
[0m[32;1m[1;3mAction: Generate Review
Action Input: Good old documentation, the ideal companion of any software system, is intended to provide stakeholders with useful knowledge about the system and related processes. Depending on the target audience, the contents of documentation varies. For example, technical documentation (e.g., API reference guides) describes information about the design, code, interfaces and functionality of software to support developers in their tasks, while user documentation (e.g., user manuals) explains to end-users how they should use the software application. Despite the undeniable practical benefits of documentation during software development and evolution activities, its creation and maintenance have been often neglected, leading to inadequate and even nonexistent documentation. These and other aspects of documentation (e.g., needs, learning obstacles) have been investigated through interviews with and surveys of practitioners, with the general goal of identifying the root causes of documentation issues (e.g., inaccuracy, outdatedness). To address these issues (at least partially), different approaches and tools have been proposed to aid developers during software documentation, including its automatic generation. For example, a recent proposal by Robillard et al. suggests a paradigm shift towards systems that automatically generate documentation in response to a developer's query, while considering her working context. However, to achieve high-quality automatic documentation systems, we require first a deep understanding of software practitioners' needs. Although existing studies have revealed some of these needs, their results are limited by the low number and lack of diversity of practitioners questioned and documentation artifacts analyzed. To overcome these limitations, we qualitatively analyzed different types of artifacts from diverse data sources and identified the issues that developers face when dealing with documentation. Specifically, we mined open source software repositories and examined a set of 878 artifacts corresponding to development emails, programming forum discussions, issues and pull requests related to software documentation. For each artifact, we determined the reported issue, the type of documentation presenting it, and the proposed solution, as well as the documentation tools discussed. Based on our analysis, we built a comprehensive taxonomy consisting of 162 types of documentation issues linked to (i) the information it contains, (ii) how the information is presented, (iii) the documentation process and (iv) documentation tool support. We describe and exemplify each category of documentation issues. We also discuss their implications in software research and practice, deriving actionable items needed to address them.[0m

[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

5. WHAT TYPE OF CONTRIBUTION? In this section, we describe the types of contributions that researchers 
claim in their papers, and provide the reasoning for the classification 
scheme and how abstracts might fall into each code. While some of the 
categories remain t he same compared to the Shaw study, we made some 
modifications to account for changes in the research community over 
the intervening years. This subsection will also explain the classification 
procedure and provide the quantitative and qualitative results for the 
analysis of contributions to the research community. 5.1 What kinds of results do researchers 
generate? The types of new results/contributions that are derived from software 
engineering research can take on many forms. Shaw provided seven 
types of cont ributions, a s shown in Table 3 [24]. Table 1. Types of software engineering research questions, adapted from [24] (Changes in Bold)  
Type of questio n Examples  
Method or means of development  How can we do/create/modify/evolve (or automate doing) X? What is a better way to do/create/modify/evolve X? Method for analysis or evaluation  How can I evaluate the quality/correctness of X? How do I choose between X and Y? Design, evaluation, or analysis of a particular 
instance  How do we evaluate Y, and when is it determined to be “good”? What is  a property X of 
artifact/method Y? What is a (better) design, implementation, maintenance, or adaptation for application X? How does X compare to Y? What is the current state of X / practice of Y? Generalization or characterization  Given X, what will Y (necessarily) be? What, exactly, do we mean by X? What are its important characteristics? What is a good formal/empirical model for X? What are the varieties of X, how are they related? Feasibility study or exploration  Does X even exist, and if so what is it like? Is it possible to accomplish X at all?

The study emphasizes the need for further experimentation in the area. 

Of these, only 5 
papers were accepted. We used a logistic regression model to analyze 
whether collaboration impacts the acceptance of papers by controlling 
for the number of authors, number of unique affiliations, number of 
unique countries, and the number of previously published ICSE papers 
of the authors. The only statistically significant factor is the number of 
previously published ICSE papers by the authors (p -value = 0.0008). From the results presented in this section, we argue that  ICSE draws in 
research from a diverse community with sub missions coming from 44 
unique countries and 465 unique organizations. Furthermore, the 2016 
acceptance data shows no significant favorable bias towards PC and PB 
members. The only factor that increases a paper’s chances of acceptance 
is the number of pape rs published by the authors at ICSE previously. We 
hypothesize that senior researchers submit higher quality papers or 
papers that fit better at ICSE. Table 7. Submissions and acceptance rates for research question/contribution combinations  
Research Question  Procedure 
or 
technique  Qualitativ
e or 
descriptive 
model  Empirical 
model  Analytic 
model  Tool or 
notation  Specific 
solution  
/prototype  
/answer/  
judgment  Empirical 
Report  
Method for means of 
development  128 (16%)  10 (20%)  12 (25%)  29 (10%)  42 (18%)  2 (0%)  3 (33%)  
Method for analysis or 
evaluation  48 (23%)  3 (0%)  10 (0%)  11 (18%)  10 (50%)  3 (33%)  5 (40%)  
Evaluation or Analysis of 
instance  11 (18%)  2 (0%)  4 (50%)  9 (33%)  15 (7%)  7 (14%)  25 (44%)  
Generalization or 
characterization  2 (0%)  7 (29%)  2 (0%)  2 (0%)  5 (20%)  0 (0%)  69 (25%)  
Feasibility study or 
exploration  6 (17%)  0 (0%)  1 (0%)  3 (0%)  9 (11%)  2 (0%)  1 (0%)  
Table 8. Submissions and acceptance rates for research Contribution/validation combinations  
Submitted (% 
Accepted)  Analysis  Evaluation  Experience  Example  Underspecified  Persuasion  Blatant 
Assertion  
Procedure or technique  31 (26%)  70 (31%)  3 (33%)  32 (9%)  45 (9%)  8 (0%)  6 (0%)  
Qualitative or descriptive 
model  1 (0%)  6 (50%)  2 (0%)  2 (0%)  5 (0%)  3 (33%)  3 (0%)  
Empirical model  12 (25%)  10 (20%)  0 (0%)  2 (0%)  1 (0%)  2 (0%)  2 (0%)  
Analytic model  10 (30%)  8 (25%)  1 (0%)  7 (14%)  9 (22%)  11 (0%)  8 (0%)  
Tool or notation  6 (17%)  27 (30%)  4 (50%)  10 (0%)  25 (20%)  3 (0%)  8 (0%)  
Specific 
solution/prototype/  
answer/judgment  2 (50%)  5 (20%)  1 (0%)  2 (0%)  3 (0%)  0 (0%)  1 (0%)  
Report  10 (30%)  62 (44%)  8 (13%)  6 (0%)  6 (0%)  8 (0%)  3 (0%)  

and in development and extension (still more time is spent in basic research, but it is 
very difficult to identify the beginning of this phase). As a result, full understanding of research strategy must account for the accumulation of evidence over time as well as for the form and content of individual projects and papers. The IMPACT project [3] is tracing the path from research into practice. The objec-
tives of the project include identifying the kinds of contributions that have substantial impact and the types of research that are successful.

The number of research questions also plays a major role; it is important to restrict them to few. Another important influence factor is the number of stakeholders involved. The more stakeholders are involved, the more potential interest conflicts about research questions will arise, which increase the amount of communication involved. Thus, the number of stakeholders is a factor that may contribute exponentially to the total effort. Design : Time/effort increases basically with (a) the complexity of the conceptual 
model; (b) the number of contacts based on validity goals and anticipated response 
rate; and (c) the length of the questionnaire/interview (i.e., number and complexity of 
questions). The main influence factor for design is the complexity of the conceptual 
model, which is, in turn, influenced by the number and complexity of research 
questions. In our experience, the research questions contribute exponentially to the 
conceptual model; that is, even one additional question can increase the complexity of the conceptual model significantly. In our case, the conceptual model was quite 
complex, which resulted in a large and complex questionnaire (many conditional 
questions). This increased the design effort significantly. Therefore, the complexity of the conceptual model has an exponential influence on design effort. 

9. Is there evidence that the 
E/QE can be used by other researchers / practitioners? Do the authors discuss whether or how the findi ngs can be transferred to other populations, or 
consider other ways in whic h the research can be used? To what extent do authors interpret results in the context of other studies / the existing body of 
knowledge?

What was the result? • If the concept of similarity ha s been used in the internal re plications, for example, with 
regard to the subjects or the experimental un its, have the possible differences been tested 
for statistical significance? What was the result? Findings of the experiment • How can the result of the experiment be explained? • Does the result obtained contradict or support the results of earlier experiments? • What other experiments could be run on the ba sis of this one to further investigate the 
results yielded? 

In what way is it better? What real problem 
does it solve? Does it scale? If your contribution is principally the synthesis or 
integration of other results or components, be clear about 
why the synthesis is itself a contribution. What is novel, 
exciting, or nonobvious about the integration? Did you 
generalize prior results? Did you find a better 
representation? Did your research improve the individual 
results or components as well as integrating them? A 
paper that simply reports on using numerous elements 
together is not enough, even if it's well-engineered. There 
must be an idea or lesson or model that the reader can take 
from the paper and apply to some other situation. If your paper is chiefly a report on experience 
applying research results to a practical problem, say what 
the reader can learn from the experience. Are your 
conclusions strong and well-supported? Do you show 
comparative data and/or statistics? An anecdotal report on 
a single project is usually not enough. Also, if your report 
mixes additional innovation with validation through 
experience, avoid confusing your discussion of the 
innovation with your report on experience. After all, if 
you changed the result before you applied it, you're 
evaluating the changed result. And if you changed the 
result while you were applying it, you may have 
confounded the experiences with the two versions. If a tool plays a featured role in your paper, what is 
the role of the tool? Does it simply support the main 
contribution, or is the tool itself a principal contribution, 
or is some aspect of the tool's use or implementation the 
main point? Can a reader apply the idea without the tool? If the tool is a central part of result, what is the technical 
innovation embedded in the tool or its implementation? If a system implementation plays a featured role in 
your paper, what is the role of the implementation? Is the 
system sound? Does it do what you claim it does? What 
ideas does the system demonstrate? • If the implementation illustrates an architecture or 
design strategy, what does it reveal about the 
architecture? What was the design rationale? What 
were the design tradeoffs? What can the reader apply 
to a different implementation? • If the implementation demonstrates an 
implementation technique, how does it help the 
reader use the technique in another setting? • If the implementation demonstrates a capability or 
performance improvement, what concrete evidence 
does it offer to support the claim? • If the system is itself the result, in what way is it a 
contribution to knowledge? Does it, for example, 
show you can do something that no one has done 
before (especially if people doubted that this could 
be done)? 731 


It may be a complementary approach. However, it is important to be aware of the difference and also to consider how this affects the statistical analysis, si nce most statistical methods have developed 

Our thinking was that higher numbers of defects would correspond to students 
paying more attention to performing a serious review. However, instead it seemed 
to motivate students to disregard the process we wanted to study (or at least augment it with additional activities) in order to report as many potential defects as possible. Thus results were unrepresentative of what would have been observed in a normal application of the review technique. We have rectified this mistake on later experiments by grading based on process conformance (as assessed by the creation of intermediate artifacts) as well as on the soundness of the evaluation of the technology done by the subjects after the experiment. 6 Conclusions and Future Work 
In this paper, we have reported on a few empirical studies that we have carried out with students. Even though most of these studies were initially conceived for research reasons, we believe that it is important to keep in mind and study the impact of these studies on the students’ education. This paper is a preliminary report in which we have described our experiences and 
lessons learned, but a lot more work needs to be done. Here, we outline some possible 
research directions. Impact of empirical studies on other actors . In this paper, we have focused on the 
researchers’ and the students’ viewpoint s. However, other actors may play an 
important role in the process and be affected by it. For one thing, we have assumed throughout the paper that the researcher and the instructor are the same person. This is 
not necessarily the case, but even so, the same person is playing two different roles, 
with two different sets of goals and responsibilities. As a researcher, one needs to 
provide scientifically sound results to the research community and the industry; as an 
instructor, one needs to provide his or he r students with the best education possible 
for their future activities. The instructor’s goals may conflict with the researcher’s 
goals, so a sensible trade-off should be sought. By the same token, when carrying out 
an empirical study in an industrial envi ronment, one should always try to minimize 
the risk that the experimental activities disrupt the normal course of business. This 
minimization of risk can be seen in the examples presented in Sections 2.4 and 2.2. By running and debugging the experiment in the university classroom, researchers 
were able to have a positive experience when they moved it to and industrial setting. Costs vs.

Question: What does 'contribution' mean in the context of this research?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2   After 
developing a research question, you will need to consider whether it can be answered through the existing published literature or whether new data must be collected. Reviewing the litera-ture is only manageable if the question has clear boundaries. In research, we are often contribut-
ing a tiny step to the existing knowledge, rather than making huge leaps. Small contributions are better than no changes at all. So a long, speciﬁ  c question is 
likely to be preferable to a short vague question. To illustrate these points 
Table  1  provides some speciﬁ  c 
examples, and in the next section we discuss some key considerations. CONSIDERATIONS: 
RELEVANCE, ORIGINALITY AND RIGOUR 
 Here, we highlight some things 
to think about as you develop your research question, building on the points raised above. These analyses are not intended to be undertaken in any speciﬁ  c order. The considerations fall into three main categories (Figure  1 ), which 
can be thought of as broadly con-cerning the relevance, originality and rigour of the research ques-tion, and which are interrelated and partly inspired by the UK Research Excellence Framework. 3   
You may start with any one of the categories and revisit each one multiple times. In doing so, you may change your research question slightly or completely.

The study emphasizes the need for further experimentation in the area. 

We provide an overview of these perspectives
in Section 6. Typically, for technical scientific results, peer reviewers are often expected to
comment on the novelty of the work, the soundness and reliability of the work, and the pre-
sentation of the work. These expectations are summarized by the maxim “is it new, and is it
true?” (MacAuley 2012 ).

and in development and extension (still more time is spent in basic research, but it is 
very difficult to identify the beginning of this phase). As a result, full understanding of research strategy must account for the accumulation of evidence over time as well as for the form and content of individual projects and papers. The IMPACT project [3] is tracing the path from research into practice. The objec-
tives of the project include identifying the kinds of contributions that have substantial impact and the types of research that are successful.

The rest of this 
section addresses  scope and then tackles two frequently  asked questions  
about this work. Section  2 explores  historical  work on writing  ‘good’ ar- 
ticles and methodological  guidance,  coupled  with how other data-driven  
disciplines  have addressed  experimental  reliability  concerns. Next, in 
Section  3 we describe  our approach  to identifying  “bad smells ”. This is 
followed  by Section  4 that lists some key symptoms  or ‘smells’  in ap- 
proximate  order of importance. Finally,  in Section  5 we consider  the 
implications  for the future and how we, the research  community,  might 
collectively  take ownership  of these “smells  ”, re-prioritise,  re-order,  add 
and subtract  “smells  ”. 1.1. Scope 
Software  analytics  research  is relatively  new and growing  fast, so 
much of our advice relates to articles  discussing  induction  from soft- 
ware project data. We discuss problems  with empirical  data analysis  
via experiments,  quasi-experiments  and correlation  studies [91] rather 
than issues relating  to e.g., qualitative  studies. Other articles  should be 
consulted  for tips and traps relating  to qualitative  research  and experi- 
mentation  involving  human participants,  e.g., [25,60,84,102]  . Finally,  in order to delineate  the scope of this article we feel it helpful 
to add some remarks  concerning  the role of theory in software  analyt- 
ics type research. Naturally  much analytics  research  will be data-driven  
or inductive  in nature. In such circumstances  the role of theory or de- 
ductive  reasoning  will be secondary. This of course begs the question  of 
36 

&Selection strategy —where to seek data? The objective of the study may be, for example, exploratory, descriptive, explanatory, or
improving. The objective is naturally more generally formulated and less precise than infixed research designs. The objective is initially more like a focus point which evolvesduring the study. The research questions state what is needed to know in order to fulfill theobjective of the study. Similar to the objective, the research questions evolve during thestudy and are narrowed to specific research questions during the study iterations(Andersson and Runeson 2007b ). The case may in general be virtually anything which is a “contemporary phenomenon in
its real-life context ”(Yin 2003 ). In software engineering, the case may be a software
development project, which is the most straightforward choice. It may alternatively be an
individual, a group of people, a process, a product, a policy, a role in the organization, an
event, a technology, etc. The project, individual, group etc. may also constitute a unit ofanalysis within a case. In the information systems field, the case may be “individuals,
groups …or an entire organization. Alternatively, the unit of analysis may be a specific
project or decision ”(Benbasat et al. 1987 ). Studies on “toy programs ”or similarly are of
course excluded due to its lack of real-life context. Yin ( 2003 ) distinguishes between
holistic case studies , where the case is studied as a whole, and embedded case studies
where multiple units of analysis are studied within a case, see Fig. 1. Whether to define a
study consisting of two cases as holistic or embedded depends on what we define as thecontext and research goals. In our XP example, two projects are studied in two differentcompanies in two different application domains, both using agile practices (Karlström and
Runeson 2006 ). The projects may be considered two units of analysis in an embedded case
study if the context is software companies in general and the research goal is to study agilepractices. On the contrary, if the context is considered being the specific company orapplication domain, they have to be seen as two separate holistic cases. Benbasat et al.comment on a specific case study, “Even though this study appeared to be a single-case,
embedded unit analysis, it could be considered a multiple-case design, due to thecentralized nature of the sites. ”(Benbasat et al.

24 The Role of Experimentation in Scientific and Technological Research 
and so on. A technological investigation is, in this sense, more prescribed than a 
scientific investigation. It is also more limited, in that it may end when it has led 
to an adequate solution of a technical problem”. Anyone working in scientific and t echnical research accepts, as a working 
hypothesis, that the world is a cosmos not a chaos, that is, th ere are natural laws 
that can be comprehended and thought out. In the case of SE, researchers can be 
said to assume that precepts that desc ribe and prescribe the optimum means of 
building software can be discovered  and established. As Pfleeger (1999) said, 
the basis of all empirical software engineering is “if we look long enough and hard enough, we will find rational rules that  show us the best ways to build the 
best software”. For a body of knowledge to be considered scientific, its truth and validity must be proven.

How can one judgeit? It is very diﬃcult to give a satisfactory and deﬁnitiveanswer to such questions without the consensus of a greatpart of the community, which requires time and greateﬀort. A possible alternative is to summarize and organizethe written information currently available on the matter.And this is precisely the aim of this study. 2. Methodology
The present description of the major types of research
papers published in SE is based exclusively on existing writ-
ten sources, and in particular on the following:(1) Published, relevant papers, such as, for example,
Zelkowitz and Wallace (1998) orSjøberg et al. (2005) .

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

Are the case and its units of analysis well defined? 1, 5, 2841. Is the suitability of the case to address the research questions clearly motivated? 8, 9, 1442. Is the case study based on theory or linked to existing literature?

Question: How is 'precisely' interpreted in terms of research scope and focus?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

This document does  not go to this level of detail. The critical issue in any systematic review is  to ask the right question. In this context, 
the right question is usually one that: 
• Is meaningful and important to practitione rs as well as researchers. For example, 
researchers might be interested in whether a specific analysis technique leads to a 
significantly more accurate estimate of re maining defects after design inspections. However, a practitioner might want to know whether adopting a specific analysis 
technique to predict remaining defects is more effective than expert opinion at 
identifying design documents that require re-inspection. • Will lead either to changes in current software engineering practice or to 
increased confidence in the value of curre nt practice. For example, researchers 
and practitioners would like  to know under what conditi ons a project can safely 
adopt agile technologies and under what conditions it should not. • Will identify discrepancies between commonly held beliefs and reality. Nonetheless, there are systematic reviews that ask questions that are primarily of 
interest to researchers. Such  reviews ask questions that identify and/or scope future 
research activities. For example, a systema tic review in a PhD thesis should identify 
the existing basis for the research stud ent’s work and make it clear where the 
proposed research fits into the current body of knowledge. Examples 
 
Kitchenham et al. 190H[21] had three research questions: 
Question 1: What evidence is there that cross-company estimation models are not 
significantly different from within-company estimation models for predicting effort for 
software/Web projects? Question 2: What characteristics of the study data sets and the data analysis methods used in 
the study affect the outcome of within- a nd cross-company effort estimation accuracy 
studies? Question 3: Which experimental procedure is most appropriate for studies comparing within- 
and cross-company estimation models? Jørgensen 191H[17] had two research questions: 
1. Should we expect more accurate effort estimates when applying expert judgment or 
models? 2. When should software development effort estimates be based on expert judgment, 
when on models, and when on a combination of expert judgment and models? In both cases, the authors were aw are from previous research that  results were mixed, so in 
each case they added a question aimed at invest igating the conditions under which different 
results are obtained. 5.3.2 Question Structure 
Medical guidelines recommend considering a question about the effectiveness of a 
treatment from three viewpoints:  
• The population, i.e. the people affected by the intervention.

3. Impact Project. "Determining the impact of software engineering research upon practice. Panel summary, Proc. 23rd International Conference on Software Engineering (ICSE 2001) , 
2001  
4. William Newman. A preliminary analysis of the products of HCI research, using pro forma 
abstracts. Proc 1994 ACM SIGCHI Human Factors in Computer Systems Conference (CHI 
'94), pp.278-284. 5. Samuel Redwine, et al. DoD Related Software Technology Requirements, Practices, and 
Prospects for the Future . IDA Paper P-1788, June 1984. 6. S. Redwine & W. Riddle. Software technology maturation. Proceedings of the Eighth Inter-
national Conference on Software Engineering,  May 1985, pp. 189-200.

 33% (38) 
 8% (9)  65% (74) 
 36% (41) 
 25% (29) 
 12% (14) 
 14% (16) 
 4% (4)  25% (15) 
 7% (4)  74% (45) 
 52% (32) 
 26% (16) 
 13% (8) 
 8% (5) 
 5% (3) Award
NoAward
OtherI carefully calibrate my epistemological
perspective with the authorsI do not validate these artifacts.I do not review these types of papersI demand and/or access data/ scripts and
experimental packageI attempt to run the source code/
container etc. on my machineI check for data consistencyI check for data availability
0% 20% 40% 60%
Number of respondents using that approach
Fig.8 Validate artifacts and claims
look at the chain of evidence from observation to conclusion and consider whether it is clear
and unbroken. I consider different quality criteria and threats to validity depending on the
type of study and its (often implicit) philosophical position.”
Characteristics of impactful papers Figure 9shows the distribution of how respondents
judge the impact of papers. It is interesting to observe that the top three characteristics are
novelty ,relevance ,a n d methodological rigor , which can sometimes be in conflict with each
other.

2. What, precisely, was your contribution? Before reporting what you did, explain what problem 
you set out to solve or what question you set out to answer 
--and why this is important. 2.1 What kinds of questions do software 
engineers investigate? Generally speaking, software engineering researchers 
seek better ways to develop and evaluate sottware. Devel- 
opment includes all the synthetic activities that involve 
creating and modifying the software, including the code, 
design documents, documentation, etc. Evaluation includes all the analytic activities associated with predict- 
ing, determining, and estimating properties of the software 
systems, including both functionality and extra-functional 
properties such as performance or reliability. Software engineering research answers questions about 
methods of development or analysis, about details of 
designing or evaluating a particular instance, about gener- 
alizations over whole classes of systems or techniques, or 
about exploratory issues concerning existence or feasibil- 
ity. Table 1 lists the types of research questions that are 
asked by software engineering research papers and 
provides specific question templates. Table 1. Types of software engineering research questions 
Type of question Examples 
Method or means of How can we do/create/modify/evolve (or automate doing) X? development What is a better way to do/create/modify/evolve X?

Basics of Software Engi neeri ng Experi ment ation 291 
  
Solution : The effect of experience and of the interaction 
is significant, whereas the effect of the technique is not; 
Technique I used by an experienced developer  
10.7.3. The effect of two modelling techni ques (-A, +A), used by experienced and 
inexperienced people (-B, +B), working in two different domains (-C, +C), 
on small-sized problems (-D, +D) is to be examined. Table 10.33 contains 
a measure of the effort put into gi ven development projects with these 
characteristics. What factors and interactions have significant effects?

Open reviews (e.g., OpenReviews.net) are post-publication reviews in
which the reviewer’s identity is usually known to the author. The International Conference
on Software Engineering conducts post-mortems of each year’s review process. Although
primarily focusing on the mechanism for deciding on paper acceptance, these reports also
explain types of papers accepted and what review information was important in making a
decision16. A recent series by Jacopo Soldani and colleagues in ACM SIGSOFT’s Software
Engineering Notes (Soldani et al. 2020 ) summarizes the “pains and gains” of peer review
and provides discussion about the concerns many scientists have with peer review, such as
workload. 6.2 PeerReviewCriteria
It is common for a peer review request to include some criteria by which the paper is to
be judged. These criteria are particularly important when the paper is a non-standard (i.e.,
non-technical research) format. For example, the Empirical Software Engineering Journal
allows for Technical Papers and Engineering in Practice papers. The Journal of Systems
and Software accepts technical papers, New Ideas and Trends, and In Practice papers. The
International Conference on Software Engineering (ICSE) has numerous tracks, including
Software in Society, Software Engineering in Practice, the main technical track, New Ideas
and Emerging Results, Software Engineering Education and Training, and more. Most conferences have the same or similar reviewing criteria for their research tracks,
typically summarized along the call for papers. The ICSE technical review criteria, for
instance, are available on the conference call for papers. For the 2021 edition, the criteria17
are
– Soundness: The extent to which the paper’s contributions are supported by rigorous
application of appropriate research methods. – Significance: The extent to which the paper’s contributions are important with respect
to open software engineering challenges. – Novelty: The extent to which the contribution is sufficiently original and is clearly
explained with respect to the state-of-the-art. – Verifiability: The extent to which the paper includes sufficient information to support
independent verification or replication of the paper’s claimed contributions. – Presentation: The extent to which the paper’s quality of writing meets the high standards
of ICSE [...]. Common to most review guidelines is a focus on presentation and clarity of the paper,
adherence to the conference or journal topics, and length. Increasingly guidelines are also
emphasizing the importance of constructive criticism (the Reviewer 2 problem (Peterson
16http://www.icse-conferences.org/reports.html
17https://conf.researchr.org/track/icse-2021/icse-2021-papers#Call-for-PapersEmpir Software Eng (2021) 26:  103 103Page26of29

Summary of previous reviews. Description of the software engineering technique being investigated and 
its potential importance. Review questions  Each review question should be 
specified.

56 IEEE SOFTWARE  |  WWW.COMPUTER.ORG/SOFTWARE   |  @IEEESOFTWARE
FOCUS:  SOFTWARE ENGINEERING’S 50TH ANNIVERSARYAs an engineering discipline, SE 
research should strive to impact 
practice. Favoring certain types of 
evidence over others will not suf -
fice. Instead, we require a frame -
work for aggregating the results of multiple pieces of work with differ -
ent types of evidence into action -
able practical feedback. In addition to encouraging technology trans -
fer and true research impact, such a framework can simultaneously open our field to accepting a wider variety of research, including re -
sults that constitute the less rigor -
ous (but still important!) codified knowledge that engineers use in practice. ABOUT THE AUTHORS
CLAIRE LE GOUES  is an assistant 
professor of computer science at Carnegie 
Mellon University’s Institute for Software 
Research. Her research interests lie in 
automatically reasoning about and improv -
ing software quality in real-world, evolving 
systems. Le Goues received a PhD in 
computer science from the University of 
Virginia. Contact her at clegoues@cs.cmu  
.edu, @clegoues. MARY SHAW is the Alan J. Perlis Uni -
versity Professor of Computer Science at Carnegie Mellon University’s Institute for 
Software Research. Her research focuses 
on software engineering and software de
-
sign, particularly software architecture and the design of systems used by real people. Shaw received a PhD in computer science 
from Carnegie Mellon University. She’s a 
Life Fellow of ACM and IEEE. Contact her at 
mary.shaw@cs.cmu.edu. CIERA JASPAN is a senior software engi -
neer at Google. She leads the Engineering Productivity Research team, which aims to 
identify inefficiencies in development tools 
and processes and improve the productiv
-
ity of Google engineers and engineers who use Google products. Jaspan received her 
PhD in software engineering from Carnegie 
Mellon University. Contact her at ciera@
google.com.

[19] R. Kwok, "Research impact: Altmetrics make their mark," 
Nature, vol. 500, pp. 491 -493, 2013. [20] C. Marshall, P. Brereton, and B. Kitchenham, "Tools to 
support systematic reviews in software engineering: a cross -domain 
survey using sem i-structured interviews," presented at the Proceedings 
of the 19th International Conference on Evaluation and Assessment in 
Software Engineering, Nanjing, China, 2015. [21] P. Runeson and M. Höst, "Guidelines for conducting and 
reporting case study research in software engineering," Empirical 
Software Engineering, vol. 14, pp. 131 -164, 2008. [22] A. Sampaio, "Improving Systematic Mapping Reviews," 
SIGSOFT Softw. Eng. Notes, vol. 40, pp. 1 -8, 2015. [23] M. Shaw, "What makes good research in software 
engineering?," J Software Tools for Technology Transfer STTT, vol. 4, 
pp. 1 -7, 2002. [24] M. Shaw, "Writing Good Software Engineering Research 
Papers," Proceedings of 25th Internat ional Conference on Software 
Engineering (ICSE'03), pp. 726 -736, 2003. [25] J. Singer and N. G. Vinson, "Ethical issues in empirical 
studies of software engineering," IEEE Transactions on Software 
Engineering, vol. 28, pp.

• Why should the reader care? • What larger question does this address? • What is your new result? • What new knowledge have you contributed that 
the reader can use elsewhere? • What previous work (yours or someone else's) 
do you build on? What do you provide a superior 
alternative to? • How is your result different from and better than 
this prior work? • What, precisely and in detail, is your new result? • Why should the reader believe your result? • What standard should be used to evaluate your 
claim? • What concrete evidence shows that your result 
satisfies your claim? If you answer these questions clearly, you'll probably 
communicate your result well. If in addition your result 
represents an interesting, sound, and significant contribu- 
tion to our knowledge of software engineering, you'll 
have a good chance of getting it accepted for publication 
in a conference or journal. Other fields of science and engineering have well- 
established research paradigms. For example, the 
experimental model of physics and the double-blind 
studies of medicines are understood, at least in broad 
outline, not only by the research community but also by 
the public at large. In addition to providing guidance for 
the design of research in a discipline, these paradigms 
establish the scope of scientific disciplines through a 
social and political process of "boundary setting" [5]. Software engineering, however, has not yet developed 
this sort of well-understood guidance. I previously [19, 
20] discussed early steps toward such understanding, 
including a model of the way software engineering 
techniques mature [17, 18] and critiques of the lack of 
rigor in experimental software engineering [1, 22, 23, 24, 
25]. Those discussions critique software engineering 
research reports against the standards of classical 
paradigms. The discussion here differs from those in that 
this discussion reports on the types of papers that are 
accepted in practices as good research reports. Another 
current activity, the Impact Project [7] seeks to trace the 
influence of software engineering research on practice. The discussion here focuses on the paradigms rather than 
the content of the research 
This report examines how software engineers answer 
the questions above, with emphasis on the design of the 
research project and the organization of the report. Other 
sources (e.g., [4]) deal with specific issues of technical 
writing. Very concretely, the examples here come from 
the papers submitted to ICSE 2002 and the program 
committee review of those papers. These examples report 
research results in software engineering. Conferences 
often include other kinds of papers, including experience 
reports, materials on software engineering education, and 
opinion essays. 0-7695-1877-X/03 $17.00 © 2003 IEEE 726 


Question: What criteria determine whether a research question is impactful or significant in Software Engineering?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

What is your new result? Explain precisely what you have contributed to the 
store of sot~ware engineering knowledge and how this is 
useful beyond your own project. oOOio -i, i 
80*/0 
60% 
40% 
20% -I 
Iii Dewl Question 
i:G i:~ii i , 
Analy Eval Gener Feas Total 
• Accepted [] Rejected 
Figure 2. Distribution of acceptances and rejections 
by type of research question 
3.1 What kinds of results do software engineers 
produce? The tangible contributions of software engineering 
research may be procedures or techniques for develop- 
ment or analysis; they may be models that generalize from 
specific examples, or they may be specific tools, solutions, 
or results about particular systems. Table 3 lists the types 
of research results that are reported in software engineer- 
ing research papers and provides specific examples. 3.2 Which of these are most common? By far the most common kind of ICSE paper reports a 
new procedure or technique for development or analysis. Models of various degrees of precision and formality were 
also common, with better success rates for quantitative 
than for qualitative models. Tools and notations were well 
represented, usually as auxiliary results in combination 
with a procedure or technique. Table 4 gives the distribu- 
tion of submissions to ICSE 2002, based on reading the 
abstracts (but not the papers), followed by graphs of the 
counts and distributions in Figures 3 and 4. 728 


Of course, one must have the resources to do research this way. 3 Realistic Tasks 
When conducting controlled experiments in software engineering, one should 
consider the realism and representativeness of the tasks regarding the size, complexity and duration of the involved tasks. Specifi cation, implementation and verification 
methods also vary considerably between domains, such as accounting software versus flight-control systems. In our opinion, some experimental tasks bear little resemblance to actual tasks in software engineering; others are very similar to actual tasks [32]. In between there is a continuum. Larger development tasks may take 
months, while many maintenance tasks may take only a couple of hours. 

103–112. Dyba, T., Kitchenham, B.A., Jorgensen, M., 2005. Evidence-based
software engineering for practitioners. IEEE Softw. 22 (1), 58–65. Fenton, N., 1993. How eﬀective are software engineering methods? J. Syst. Softw. 22 (2), 141–146. Fletcher, P., 1995. The role of experiments in computer science. J. Syst. Softw. 30 (1–2), 161–163. Glass, R.L., 1995. Structure-based critique of contemporary computing
research. J. Syst. Softw. 28 (1), 3–7. Glass, R.L., Vessey, I., Ramesh, V., 2002. Research in software engineer-
ing: an analysis of the literature. Information Softw. Technol. 44 (8),491–506. Glass, R.L., Ramesh, V., Vessey, I., 2004. An analysis of research in
computing disciplines. Commun. ACM 47 (6), 89–94. Goldberg, A., 1982. Editorial policy. ACM Comput. Surveys 14 (2), 151–
157. Haddow, G., Klobas, J., 2004. Communication of research to practice in
library and information science: closing the gap. Library Information
Sci.

•They interact with many other development 
techniques and procedures. For example a design method depends on a preceding requirements analysis. It must consider constraints imposed by the software and hardware platform and programming languages, timescales, and budget. It must be integrated with appropriate coding and testing techniques. Thus, it would be difficult to confirm that a design technique had a significant impact on final product reliability. In general, it is difficult to determine a causal link between a particular technique and a desired project outcome when the application of the technique and the final outcome are temporally removed from one another, and there are many other tasks and activities that could also affect the final outcome. •The immediate outcomes of a software engineering 
technique will not necessarily have a strong relationship with final project outcomes. E.g. if you 
are interested in the effect design techniques have on 
application reliability (i.e. probability of failure in a given time period under defined operational conditions), measures of the design product (or 
design process) have no obvious relationship with 
the desired outcome. There are no good surrogate 
measures of product reliability that can be measured 
at the end of the design process. There seem to be two major approaches to this issue: 1.

• What kinds of evidence can demonstrate the validity of a result, and how are 
good results distinguished from bad ones? In this paper I attempt to make generally accepted research strategies in software en-
gineering explicit by examining research in the area to identify what is widely ac-cepted in practice. 1.1   Software Technology Maturation 
Redwine and Riddle [5,6] reviewed a number of software technologies to see how 
they develop and propagate. They found that it typically takes 15-20 years for a tech-nology to evolve from concept formulation to the point where it's ready for populari-zation. They identify six typical phases: 
• Basic research . Investigate basic ideas and concepts, put initial structure on 
the problem, frame critical research questions. • Concept formulation . Circulate ideas informally, develop a research commu-
nity, converge on a compatible set of ideas, publish solutions to specific sub-problems. • Development and extension . Make preliminary use of the technology, clarify 
underlying ideas, generalize the approach. • Internal enhancement and exploration . Extend approach to another domain, 
use technology for real problems, stabilize technology, develop training mate-rials, show value in results. • External enhancement and exploration . Similar to internal, but involving a 
broader community of people who weren’t developers, show substantial evi-dence of value and applicability. • Popularization .

  12 outcomes should be specified. For example, in some cases we require interventions 
that improve some aspect of software  production without affecting another e.g. improved reliability with no increase in cost. A particular problem for soft ware engineering experiments is the widespread use of 
surrogate measures for example, defects found during system testing as a surrogate 
for quality, or coupling measures for de sign quality. Studies that use surrogate 
measures may be misleading and conclusi ons based on such studies may be less 
robust. Context 
For Software Engineering, this is the c ontext in which the co mparison takes place 
(e.g. academia or industry), the participants taking part in  the study (e.g.

have criticized common practice in the field for failing to collect, analyze, and report 
experimental measurements in research reports [9,10,11,12]. In 2001 I [8] presented preliminary sketches of some of the successful paradigms for software engineering research, drawing heavily on examples from software architecture. Scientific and engineering research fields can be characterized by identifying what 
they value: 
• What kinds of questions are "interesting"? • What kinds of results help to answer these questions, and what research 
methods can produce these results?

2   Questions, Results, and Validation in Software Engineering 
Generally speaking, software engineering researchers seek better ways to develop and 
evaluate software. They are motivated by practical problems, and key objectives of the research are often quality, cost, and timeliness of software products. This section presents a model that explains software engineering research papers by 
classifying the types of research questions they ask, the types of results they produce, 
and the character of the validation they provide. This model has evolved over several 
years.

4  Introduct ion 
  Confronted by a confusing array of op tions for producing software, software 
engineers need proof that a particular approach or technique is really better than 
another. They need to know the clear-cut be nefits of one approach versus another. They need reliable evidence that one appr oach clearly works better than another. This need to work with facts rather than assumptions is a property of any 
engineering discipline. Very few ideas in SE are matched with empirical data. Ideas, whose truthfulness has not been tested against reality, are continually assumed as evident. For example, the 
famous software crisis (Naur, 1969) was and still is more a question of a subjective 
customer and/or developer impression than a confirmed phenomenon. In fact, we can find some claims that there is no software crisis (see comments about Turski and 
Hoare opinions given in (Maibaum, 1997)). Another example is the idea that traditional engineering disciplines behave  much better than SE (Pezeé, 1997). For 
example, has any empirical study been pe rformed to compare the failure rate of 
products obtained by SE and those produced by other engineering disciplines? Table 
1.1 presents the percentage of faults in the car industry shown in a study carried out by Lex Vehicle Leasing, one of the major world vehicle hire and leasing companies, 
on a universe of 73,700 cars in England. This fault rate could call the reliability of 
the engineering used in this industry into question; however, this is not the case. Therefore, if we are really to  speak of a software crisis, we  need studies to show that 
SE is less reliable than other branches of engineering, that is, the number of faults in SE should be checked against the number of faults in other branches of engineering. Other examples of computer science theo ries that have not been tested are 
functional programming, object-oriented programming or formal methods.

 SEPTEMBER/OCTOBER 2018  |  IEEE SOFTWARE  51
imagining a dialogue between 
researchers and practitioners (see 
the sidebar). This pattern is common: engi -
neers often rely on their experience 
and a priori beliefs1 or turn to co -
workers for advice. This is better than guessing or giving up. But what 
if incompletely validated research outcomes could be distilled into re -
liable sources, intermediate between 
validated results and folk wisdom? To impact practice, SE research 
results must lead to pragmatic, ac -
tionable advice. This involves syn -
thesizing recommendations from results with different assumptions 
and levels of rigor, assigning appro -
priate levels of confidence to the rec -
ommendations. Here, we examine how these tradeoffs between rigor 
and pragmatism have been handled 
in medicine, where risk is often ac -
ceptable in the face of urgency. We propose an approach to describ -
ing SE research results with varied quality of evidence and synthesizing those results into codified knowledge 
for practitioners. This approach can 
both improve practice and increase the pace of research, especially in ex -
ploratory topics. Software Engineering 
Research Expectations over Time
When the 1968 NATO Conference 
introduced “software engineering” 
to our vocabulary,2 research often 
focused on designing and building programs. There were guidelines for writing programs; the concept of rea -
soning mathematically about a pro -
gram had just been introduced. The 
emphasis was on demonstrated capa -
bility—what we might now call feasi -
bility—rather than rigorous validation. This is visible in a sampling of 
major results of the period. For ex -
ample, Carnegie Mellon University identified a set of canonical papers published between 1968 and 2002. 3 
Several are formal analyses or em -
pirical studies, and a few are case studies. However, the majority are carefully reasoned essays that pro -
pose new approaches based on the authors’ experience and insight. The field has historically built 
on results with varying degrees of certainty. Indeed, Fred Brooks pro -
posed a “certainty-shell” structure RESEARCHERS AND PRACTITIONERS 
VOLLEY ABOUT MAKING RESEARCH USEFUL
Dear Practitioners:
The research community is actually discovering things you might find useful. Please help us organize this knowledge so that 
it’s actually useful to you.

Question: How do we define 'concrete' results in software engineering research?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

objectives, (b) demonstration of novelty and superiority
compared to related work, (c) evidence of eﬀectiveness,reproducibility, scalability, and practical relevance.Reviewers should also ascertain if papers meet quality cri-teria such as conciseness and technical precision, the con-sistent use of terminology, and the deﬁnition of keyconcepts. Focus and limitation of length must often beevaluated together, as the ACM T Database Syst R sug-
gests: ‘‘TODS would like to discourage excessively longpapers (longer than 50 doubled-spaced pages including ﬁg-ures, references, etc.) and unnecessary digressions even inshorter papers. This should help authors to focus on themost important aspects of their submission [ ...]”. For the
rest, practically the same quality criteria as for authorsare brought up, such as clarity, readability and the abilityto target as broad an audience as possible. Studying the abstracts of papers published at ICSE
2002, Shaw (2003) describes how accepted research papers
answered questions such as: What was your contribution?What is your new result? How is your result diﬀerent fromand better than prior work?, etc. According to Shaw, whenit comes to evaluating submitted papers, programme com-mittees look for interesting, novel, and exciting results. Inaddition, as SE knowledge grows incrementally, like in
other areas of science and engineering, programme com-
mittees are very interested in the authors’ interpretationof prior work. 3.2. Extended versions of conference papersA special type of articles are those based on one or
more papers previously presented at a workshop, a sym-
posium or a conference. For instance, van Vliet (2006)
is an extended version of van Vliet (2005) . In the study
previously mentioned ( Montesi and Mackenzie-Owen,
2008), comparing article genres in SE, education and
biology, it was found that extended versions of confer-ence papers played a role only in SE. A distinguishing
feature of extended papers is the double round of
reviews they need to pass before journal publication(i.e. conference and journal reviews). In a survey withauthors of extended versions and journal editors, Mon-
tesi and Mackenzie-Owen (accepted for publication)found that the extension process may change dependingon the type of conference paper to extend (empirical,theoretical, ...). Some journal instructions to authors regulate the pub-
lication of these articles. Among the other conditions, onethird of the content approximately must be new and ori-ginal, and sometimes authors must submit both versions,a description of the diﬀerences between the submittedmanuscript and the preliminary version, and referencethe previous version in the paper (SoSyM). Papers pub-lished as such must be ‘‘important ”(Theor Pract Log
Prog), ‘‘outstanding ”(ACM T Database Syst ), and ‘‘of
particularly high quality ”(J Comput Sci Technol ). New
material takes in explorations of further research, theo-rems, proofs and/or implementation details ( ACM T
Graphic ). If journals address reviewers the general tone doesn’t
change, though they oﬀer some more clues on what to lookfor. In the ACM T Database Syst -R , for instance, extended
versions of conference papers ‘‘should thoroughly consoli-date the material, should extend it to be broader, andshould more carefully cover related research ”. In addition,
the novel approach should be described more deeply, andalternatives should also be considered more comprehen-sively. In IEEE T Vis Comput Gr -R , while no new results
are required, key ideas, examples, and further elaborationare expected. This conﬁrms Shaw’s observation that in con-
ferences time and space limitations do not allow authors to
explore all the possible results of their research ( Shaw,
2003).Table 2
Excerpt from the instructions of IEE Proc-Softw
Originality: is the work scientiﬁcally rigorous, accurate and novel? Does the work contain signiﬁcant additional material to that already published?

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

• Papers not subject to peer-review. When an SLR has been published in more than one journal/conference, the most 
complete version of the survey will be used. Primary study selection process 
 
The results will be tabulated as follows: 
• Number of papers per year per source 
• Number of candidate pape rs per year per source 
• Number of selected papers per year per source. The relevant candidate and selected studies will be selected by a single researcher. The rejected studies will be checked by a nother researcher. We will maintain a list 
candidate papers that were rejected  with reasons for the rejection. Quality Assessment 
 Each SLR will be evaluated using the York University, Centre for Reviews and Dissemination (CDR) Database of Abstracts of Reviews of Effects (DARE) criteria 
(http://www.york.ac.uk/inst/crd/crddatabase .htm#DARE). The criteria are based on 
four questions: 
 
• Are the review’s inclusion and exclusi on criteria describe d and appropriate? • Is the literature search likely to have covered all relevant studies? • Did the reviewers assess the quality/v alidity of the included studies? • Were the basic data/studies adequately described? The questions are scored as follows:  
• Question 1: Y (yes), the inclusion criteria  are explicitly defined in the paper, P 
(Partly), the inclusion criteria are implic it; N (no), the inclusion criteria are not 
defined and cannot be readily inferred. • Question 2: Y, the authors have either s earched 4 or more digital libraries and 
included additional search strategies or identified and referenced all journals 
addressing the topic of interest; P, the authors have searched 3 or 4 digital 
libraries with no extra search strategies, or searched a define d but restricted set 
of journals and conference proceedings;  N, the authors have search up to 2 
digital libraries or an extremel y restricted set of journals. • Question 3: Y, the authors have explicit ly defined quality criteria and extracted 
them from each primary study; P, the re search question involves quality issues 
that are addressed by the study; N no e xplicit quality assessm ent of individual 
papers has been attempted. 

Has its value been demonstrated? Relevance: is the material appropriate to the scope of the journal to which it is submitted? Motivation: does the problem considered have a sound motivation? Does the paper clearly demonstrate the scientiﬁc interest of the results? Referencing: has reference been made to the most recent and most appropriate work? Is the present work set in the context of the previous work? Clarity: is the English clear and well written? Poorly written English may obscure the scientiﬁc merit of your paper. Are the ideas expressed clearly
and concisely?

Characteristicsofhighqualitypapers Table 4shows the most common results from our
coding of the free-text responses to this question. Overall, the most common characteris-
tics reviewers want in a high quality paper relate to aspects of the quality of the research
methodology and how well it is presented in the paper. It is also interesting to note which
characteristics were not among the most commonly given answers, including the availability
of the data (5), the findings themselves (1), and the generalizability of the results (1). Characteristicsleadingtonegativereviews Figure 11shows the ranking of the charac-
teristics that tend to lead to negative reviews. The highest ranked characteristics include (1)
a mismatch of methods with claims raised in the contribution, (2) overly grandiose claims,
(3) a writing style that is hard to follow, (4) a research methodology which is difficultEmpir Software Eng (2021) 26:  103 103Page14of29

Tools that originated from a new technique also described in the 
paper would fall under this category. • “Specific Solution, Prototype, Answer, or Jud gment” covers many 
different types of results, including analysis of existing systems or 
specific analysis of a technique. • •“Empirical Report” covers papers that focus on a set of 
observations, either quantitative or qualitative, and could 
potentially incl ude a general prescription of guidelines based on 
the data in the report. The “Empirical Report” category includes 
the presentation of data or insights typically found in MSR papers. “Empirical Report” is a modification of the original classification scheme, which was called “Report”. The new name emphasizes 
the type of contribution that results of MSR papers.

3   Research Strategies 
Section 2 identifies the three important aspects of an individual research result as 
reported in a typical conference or journal paper. It is clear that the spectrum of good research strategies includes experimental computer science in the sense of [9,10,11, 12]; it is also clear that the spectrum is much broader than just experimental research. Of course, not all the combinations of question, result, and validation make sense.

How can one judgeit? It is very diﬃcult to give a satisfactory and deﬁnitiveanswer to such questions without the consensus of a greatpart of the community, which requires time and greateﬀort. A possible alternative is to summarize and organizethe written information currently available on the matter.And this is precisely the aim of this study. 2. Methodology
The present description of the major types of research
papers published in SE is based exclusively on existing writ-
ten sources, and in particular on the following:(1) Published, relevant papers, such as, for example,
Zelkowitz and Wallace (1998) orSjøberg et al. (2005) .

Use them consistently. The more formal or analytic the paper, the more important 
this is. Here are some questions that the program committee 
may ask about your paper: 
What, precisely, do you claim to contribute? Does your result fiJlly satisfy your claims? Are the 
definitions precise, and are terms used consistently? Authors tend to have trouble in some specific 
situations. Here are some examples, with advice for 
staying out of trouble:" 
• If your result ought to work on large systems, explain 
why you believe it scales. • If you claim your method is "automatic", using it 
should not require human intervention. If it's 
automatic when it's operating but requires manual 
assistance to configure, say so. If it's automatic 
except for certain eases, say so, and say how often 
the exceptions occur. • If you claim your result is "distributed", it probably 
should not have a single central controller or server. If it does, explain what part of it is distributed and 
what part is not. • If you're proposing a new notation for an old 
problem, explain why your notation is clearly 
superior to the old one. • If your paper is an "experience report", relating the 
use of a previously-reported tool or technique in a 
practical software project, be sure that you explain 
what idea the reader can take away from the paper to use in other settings. If that idea is increased 
confidence in the tool or technique, show how your 
experience should increase the reader's confidence 
for applications beyond the example of the paper. What ~ new here? The program committee wants to know what is novel 
or exciting, and why. What, specifically, is the 
contribution? What is the increment over earlier work by 
the same authors? by other authors? Is this a sufficient 
increment, given the usual standards of subdiscipline? Above all, the program committee also wants to know 
what you actually contributed to our store of knowledge 
about software engineering. Sure, you wrote this tool and 
tried it out. But was your contribution the technique that is 
embedded in the tool, or was it making a tool that's more 
effective than other tools that implement the technique, or 
was it showing that the tool you described in a previous 
paper actually worked on a practical large-scale problem? It's better for you as the author to explain than for the 
program committee to guess. Be clear about your claim ... Awful • • I completely and generally solved ... I (unless you actually did!) 
Bad • • I worked on galumphing. (or studied, investigated, sought, 
explored) 
Poor • • I worked on improving galumphing. (or contributed to, participated in, 
helped with) 
Good • • I showed the feasibility of composing 
blitzing with flitzing. i • I significantly improved the accuracy of 
the standard detector. (or proved, demonstrated, created, 
established, found, developed) 
Better • • I automated the production of ritz 
tables from specifications. • With a novel application of the blivet 
transform, I achieved a 10% increase 
in speed and a 15% improvement in 
coverage over the standard method. Use verbs that show results and achievement, not just 
effort and activity.

Examples 
Kitchenham et al. used the following inclusion criteria: 
• any study that compared predictions of cross-company models with within-
company models based on analysis of single company project data. They used the following exclusion criteria: 
• studies where projects were only collect ed from a small number of different sources 
(e.g. 2 or 3 companies),  
• studies where models derived from a with in-company data set were compared with 
predictions from a general cost estimation model. Jørgensen 206H[17] included papers that compare j udgment-based and model-based software 
development effort estimation. He also ex cluded one relevant paper due to “incomplete 
information about how the estimates were derived”. Issues: 
• Medical standards make a point that it is important to avoid, as far as possible, 
exclusions based on the language of th e primary study. This may not be so 
important for Software Engineering. • It is possible that inclus ion decisions could be affected by knowledge of the 
authors, institutions, journals or year of publication. Some medical researchers 
have suggested reviews should be done af ter such information has been removed. However, it takes time to do this and experimental evidence suggests that 
masking the origin of primary studies does not improve reviews 207H[4]. 6.2.2 Study selection process 
Study selection is a multistage process. Initially, selection criteria should be 
interpreted liberally, so that unless a study identifie d by the electronic and hand 
searches can be clearly excluded based on title and abstract, a full copy should be 
obtained. However, Brereton et al. 208H[5] point out that  “The standard of IT and software 
engineering abstracts is too poor to rely  on when selecting primary studies. You 
should also review the conclusions.” 
 
The next step is to apply inclusion/excl usion criteria based on practical issues 209H[11] 
such as: 
• Language 
• Journal 
• Authors 
• Setting 
• Participants or subjects 
• Research Design 
• Sampling method 
• Date of publication. Staples and Niazi point out th at it is sometimes necessary to consider the questions 
that are not being addressed in orde r to refine your exclusion criteria 210H[27]. Example 
Staples and Niazi’s research question was 

Question: What are the criteria for 'specific' results in academic papers?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During the analysis it is important to maintain a chain ofevidence from the findings to the original data. The report should include sufficient dataand examples to allow the reader to understand the chain of evidence. Table 11 Reader ’s checklist items
39. Are the objective, research questions, and hypotheses (if applicable) clear and relevant? 1, 2, 5, 29, 30
40. Are the case and its units of analysis well defined? 1, 5, 28
41. Is the suitability of the case to address the research questions clearly motivated? 8, 9, 1442. Is the case study based on theory or linked to existing literature? 343. Are the data collection procedures sufficient for the purpose of the case study (data sources, collection,
validation)? 11, 13, 16, 18, 21, 31
44. Is sufficient raw data presented to provide understanding of the case and the analysis? 3245. Are the analysis procedures sufficient for the purpose of the case study (repeatable, transparent)? 22, 3346.

Analysis of claim (d) 

Evaluating experiment quality. Even in EBM, it 
is recognised that it is sometimes impossible to perform randomised trials and evidence from other types of 
experiment may need to be considered. The Australian 
National Health and Medical Research Council have published guidelines for evaluating the quality of evidence [2]. They consider:  
•The strength of the evidence . This has three 
elements: Level, Quality, and Statistical Precision. Level relates to the choice of study design and is used as an indicator to which bias has been eliminated by design. Quality refers to the methods used by the investigators to minimize bias within the study design. Statistical Precision refers to the P-value or the confidence interval. •Size of effect . The distance of the estimated treatment 
effect from the null value and the inclusion of 
clinically important effects in the confidence interval. •Relevance of evidence . The usefulness of the 
evidence in clinical practice, particularly the appropriateness of the outcome measures used. These criteria appear to be equally valid for software 
engineering evidence.

 33% (38) 
 8% (9)  65% (74) 
 36% (41) 
 25% (29) 
 12% (14) 
 14% (16) 
 4% (4)  25% (15) 
 7% (4)  74% (45) 
 52% (32) 
 26% (16) 
 13% (8) 
 8% (5) 
 5% (3) Award
NoAward
OtherI carefully calibrate my epistemological
perspective with the authorsI do not validate these artifacts.I do not review these types of papersI demand and/or access data/ scripts and
experimental packageI attempt to run the source code/
container etc. on my machineI check for data consistencyI check for data availability
0% 20% 40% 60%
Number of respondents using that approach
Fig.8 Validate artifacts and claims
look at the chain of evidence from observation to conclusion and consider whether it is clear
and unbroken. I consider different quality criteria and threats to validity depending on the
type of study and its (often implicit) philosophical position.”
Characteristics of impactful papers Figure 9shows the distribution of how respondents
judge the impact of papers. It is interesting to observe that the top three characteristics are
novelty ,relevance ,a n d methodological rigor , which can sometimes be in conflict with each
other.

  24 They also considered the reporting  quality based on 4 questions: 
1. Is it clear what projects were used to construct each model? 2. Is it clear how accuracy was measured? 3. Is it clear what cross-validation method was used? 4.

9. Is there evidence that the 
E/QE can be used by other researchers / practitioners? Do the authors discuss whether or how the findi ngs can be transferred to other populations, or 
consider other ways in whic h the research can be used? To what extent do authors interpret results in the context of other studies / the existing body of 
knowledge?

Has its value been demonstrated? Relevance: is the material appropriate to the scope of the journal to which it is submitted? Motivation: does the problem considered have a sound motivation? Does the paper clearly demonstrate the scientiﬁc interest of the results? Referencing: has reference been made to the most recent and most appropriate work? Is the present work set in the context of the previous work? Clarity: is the English clear and well written? Poorly written English may obscure the scientiﬁc merit of your paper. Are the ideas expressed clearly
and concisely?

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

Figure 6. Distribution of acceptances and rejections 
by type of validation 
Is the validation related to the claim? If you're claiming 
performance improvement, validation should analyze 
performance, not ease of use or generality. And 
conversely. Is this such an interesting, potentially powerful idea 
that it should get exposure despite a shortage of concrete 
evidence? Authors tend to have trouble in some specific 
situations. Here are some examples, with advice for 
staying out of trouble: 
• If you claim to improve on prior art, compare your 
result objectively to the prior art. • If you used an analysis technique, follow the rules of 
that analysis technique. If the technique is not a 
common one in soRware engineering (e.g., meta- 
analysis, decision theory, user studies ° or other 
behavioral analyses), explain the technique and 
standards of proof, and be clear about your 
adherence to the technique. • If you offer practical experience as evidence for your 
result, establish the effect your research has. If at all 
possible, compare similar situations with and without 
your result. 733 


Are the review’s inclusion and exclus ion criteria described and appropriate? 2. Is the literature search likely to ha ve covered all relevant studies? 3. Did the reviewers assess the quality/v alidity of the in cluded studies? 4. Were the basic data/studies adequately described? Examples 
 We applied the DARE criteria bot h to Kitchenham et al.’s study 
186H[21] and to Jørgensen’s study 
187H[17]. We gave Kitchenham et al.’s study a score of 4 and Jørgensen’s study a score of 3.5. Other studies scored using the DARE criteria are listed in Appendix 2. From a more general viewpoint, Greenlaugh 188H[12] suggests the following questions: 
• Can you find an important clinical qu estion, which the review addressed? (Clearly, in software engin eering, this should be adapted to refer to an important 
software engineering question.) 
• Was a thorough search done of the a ppropriate databases and were other 
potentially important sources explored? • Was methodological quality assessed and the trials weighted accordingly? • How sensitive are the results to the way that the review has been done?

Question: How should 'relatedness' between evidence and claims be defined and assessed?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m
[38;5;200m[1;3mcontent='```\nManuscript Text: In the research question, what, precisely, does the author claim to contribute?\nReview: The author claims to contribute a comprehensive taxonomy of 162 types of documentation issues linked to different aspects such as the information it contains, how the information is presented, the documentation process, and documentation tool support. The author also exemplifies each category of documentation issues and discusses their implications in software research and practice, deriving actionable items to address them.\n\nManuscript Text: Why should the reader care about the research question?\nReview: The reader should care about the research question because documentation is a crucial aspect of software development and evolution, yet its creation and maintenance are often neglected, leading to inadequate or nonexistent documentation. By addressing documentation issues through a comprehensive taxonomy and actionable items, the research aims to improve the quality of documentation in software projects, which can ultimately enhance software development practices and outcomes.\n\nManuscript Text: Is the result concrete and specific?\nReview: Yes, the result is concrete and specific as it provides a detailed taxonomy of 162 types of documentation issues, categorizing them based on different aspects related to documentation. The author also exemplifies each category, making the results specific and tangible for understanding and addressing documentation challenges in software development.\n\nManuscript Text: Is the evidence and validation related to the claim?\nReview: The evidence and validation provided in the manuscript are related to the claim. The author qualitatively analyzed artifacts from diverse data sources, including open source software repositories, development emails, programming forum discussions, issues, and pull requests related to software documentation. By examining 878 artifacts, the author identified documentation issues and their implications, thus validating the claim of contributing a comprehensive taxonomy of documentation issues.\n```' response_metadata={'token_usage': {'completion_tokens': 347, 'prompt_tokens': 1502, 'total_tokens': 1849}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-ac284694-5ec2-4fc2-b0d6-2bc94e31d9dc-0'[0m[32;1m[1;3mAction: Fetch Section Content by Title
Action Input: {"section_title": "II. R ELATED W ORK", "path": "/Users/crystalalice/Desktop/ICSHP_Research/SE_paper/Software_Documentation_Issues_Unveiled.pdf"}[0m[33;1m[1;3mSoftware documentation has inspired research mainly on
two fronts: tools & approaches and (empirical) studies.
Tools & Approaches. There has been much work on build-
ing tools to support the automated generation of documentation.
Software summarization has shown progress with different
techniques and tools for generating abstractive and extractive
summaries of diverse software artifacts, such as bug reports
[10]–[12], classes and methods [8], [13]–[19], unit tests [20],
[21], commit messages [22]–[24], release notes [25], [26],
user reviews [27], code examples [28] and user stories [29].
Approaches aimed at supporting developers during coding have
also been developed. Code search engines and recommendation
systems are available for retrieving API and code usage
examples [30]–[33], code fragments implementing speciﬁc
features [34]–[37] and crowd knowledge [38], [39].
Despite these efforts, automated documentation is still
wishful thinking. A ﬁrst research road map to enable automated
on-demand documentation has however been drawn by the
recent proposal of Robillard et al. [9].
1558-1225/19/$31.00 ©2019 IEEE
DOI 10.1109/ICSE.2019.00122
1199
Authorized licensed use limited to: Access paid by The UC Irvine Libraries. Downloaded on February 08,2024 at 23:55:52 UTC from IEEE Xplore.  Restrictions apply.
TABLE I
SUMMARY OF PREVIOUS STUDIES ON SOFTWARE DOCUMENTATION ASPECTS.
Study
Artifacts
Summary of ﬁndings (related to concerns and quality attributes)
Forward and Lethbridge (2002) [1]:
Questionnaire with 48 participants (from sw in-
dustry, research peers, and mail lists members).
Software documentation
regularly used by partic-
ipants
Despite documentation being outdated, practitioners learn how to deal with it. “Software documen-
tation tools should seek to better extract knowledge from core resources. Software professionals
value technologies that improve automation of the documentation process, and its maintenance.”
Kajko-Mattsson (2005) [3]:
Exploratory study with 18 Swedish organizations.
Maintenance-related
documentation artifacts
“Documentation within corrective maintenance is still a very neglected issue.”
Chen and Huang (2009) [6]:
Questionnaire with 137 project managers and
sw engineers of the Chinese Information Service
Industry Association of Taiwan.
Software documentation
regularly used by partic-
ipants
Most
typical problems in software documentation quality for maintenance are that software
documentation is untrustworthy, inadequate, incomplete or does not even exist, lacks traceability,
does not include its changes, and lacks integrity and consistency.
Robillard (2009) [40]:
Personal interviews with 80 professionals at Mi-
crosoft.
API documentation and
source code
The top obstacles for API learning are: resources for learning (documentation, examples, etc.),
API structure, Background, Technical environment, Process. API documentation must include good
examples, be complete, support complex usage scenarios, be organized, and have better design.
Dagenais and Robillard (2010) [41]:
(i) A qualitative study with 12 contributors and
10 users of open source projects, and (ii) an
evolution analysis of 19 documents from 10 open
source projects.
Robillard and Deline (2011) [42]:
(i) An initial questionnaire, (ii) a set of qualitative
in-person interviews and (iii) a questionnaire with
440 developers at Microsoft.
projects
Open
source
documentation
a
repository or wiki (e.g.,
Django,
and
Eclipse)
Firefox
in
In open source projects, knowing the relationships between documentation and decisions of
contributors help to deﬁne better techniques for documentation creation and maintenance. When
a wiki is selected to host documentation, its quality is threatened by erroneous updates, SPAM or
irrelevant content (URLs included). This requires more effort for maintaining wikis.
API documentation reg-
ularly used by partici-
pants
Relevant issues in the documentation that affect the developers learning experience: “documenta-
tion of intent, code examples, cookbooks for mapping usage scenarios to API elements, penetrability
of the API, and format and presentation of the documentation.”
Plösch et al. (2014) [43]:
Online questionnaire with 88 software profession-
als, mainly German speakers.
Software documentation
regularly used by partic-
ipants
The most important attributes are accuracy, clarity, consistency, readability, structuredness and
understandability. “There is a need for automatic analysis of software documentation quality.”
Zhi et al. (2015) [4]:
Mapping study about a set of 69 papers from 1971
to 2011.
N/A
Garousi et al. (2015) [44]:
Industry case study with analysis of documen-
tation (using the taxonomy by Zhi [4]) and a
questionnaire with 25 employees of NovAtel Inc.
Source code and a sam-
ple of
software docu-
mentation (design, tests
and processes)
Documentation quality attributes that appear in most of the papers are completeness, consistency
and accessibility. More empirical evidence is required involving large-scale development projects,
or larger samples of participants from various organizations; more industry-academia collaborations
are also required, and more estimation models or methods to assess documentation.
Technical documentation is preferred during development than during maintenance tasks; the
preferred source of information for maintenance is the source code; other sources of information
have no signiﬁcant impact on developers’ preferences.
Uddin and Robillard (2015) [45]:
A case study and a questionnaire with 230 soft-
ware professionals from IBM.
[0m[32;1m[1;3mAction: Generate Review
Action Input: Software documentation has inspired research mainly on two fronts: tools & approaches and (empirical) studies. Tools & Approaches. There has been much work on building tools to support the automated generation of documentation. Software summarization has shown progress with different techniques and tools for generating abstractive and extractive summaries of diverse software artifacts, such as bug reports, classes and methods, unit tests, commit messages, release notes, user reviews, code examples, and user stories. Approaches aimed at supporting developers during coding have also been developed. Code search engines and recommendation systems are available for retrieving API and code usage examples, code fragments implementing specific features, and crowd knowledge. Despite these efforts, automated documentation is still wishful thinking. A first research road map to enable automated on-demand documentation has however been drawn by the recent proposal of Robillard et al.[0m

[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

This document does  not go to this level of detail. The critical issue in any systematic review is  to ask the right question. In this context, 
the right question is usually one that: 
• Is meaningful and important to practitione rs as well as researchers. For example, 
researchers might be interested in whether a specific analysis technique leads to a 
significantly more accurate estimate of re maining defects after design inspections. However, a practitioner might want to know whether adopting a specific analysis 
technique to predict remaining defects is more effective than expert opinion at 
identifying design documents that require re-inspection. • Will lead either to changes in current software engineering practice or to 
increased confidence in the value of curre nt practice. For example, researchers 
and practitioners would like  to know under what conditi ons a project can safely 
adopt agile technologies and under what conditions it should not. • Will identify discrepancies between commonly held beliefs and reality. Nonetheless, there are systematic reviews that ask questions that are primarily of 
interest to researchers. Such  reviews ask questions that identify and/or scope future 
research activities. For example, a systema tic review in a PhD thesis should identify 
the existing basis for the research stud ent’s work and make it clear where the 
proposed research fits into the current body of knowledge. Examples 
 
Kitchenham et al. 190H[21] had three research questions: 
Question 1: What evidence is there that cross-company estimation models are not 
significantly different from within-company estimation models for predicting effort for 
software/Web projects? Question 2: What characteristics of the study data sets and the data analysis methods used in 
the study affect the outcome of within- a nd cross-company effort estimation accuracy 
studies? Question 3: Which experimental procedure is most appropriate for studies comparing within- 
and cross-company estimation models? Jørgensen 191H[17] had two research questions: 
1. Should we expect more accurate effort estimates when applying expert judgment or 
models? 2. When should software development effort estimates be based on expert judgment, 
when on models, and when on a combination of expert judgment and models? In both cases, the authors were aw are from previous research that  results were mixed, so in 
each case they added a question aimed at invest igating the conditions under which different 
results are obtained. 5.3.2 Question Structure 
Medical guidelines recommend considering a question about the effectiveness of a 
treatment from three viewpoints:  
• The population, i.e. the people affected by the intervention.

3. Impact Project. "Determining the impact of software engineering research upon practice. Panel summary, Proc. 23rd International Conference on Software Engineering (ICSE 2001) , 
2001  
4. William Newman. A preliminary analysis of the products of HCI research, using pro forma 
abstracts. Proc 1994 ACM SIGCHI Human Factors in Computer Systems Conference (CHI 
'94), pp.278-284. 5. Samuel Redwine, et al. DoD Related Software Technology Requirements, Practices, and 
Prospects for the Future . IDA Paper P-1788, June 1984. 6. S. Redwine & W. Riddle. Software technology maturation. Proceedings of the Eighth Inter-
national Conference on Software Engineering,  May 1985, pp. 189-200.

 33% (38) 
 8% (9)  65% (74) 
 36% (41) 
 25% (29) 
 12% (14) 
 14% (16) 
 4% (4)  25% (15) 
 7% (4)  74% (45) 
 52% (32) 
 26% (16) 
 13% (8) 
 8% (5) 
 5% (3) Award
NoAward
OtherI carefully calibrate my epistemological
perspective with the authorsI do not validate these artifacts.I do not review these types of papersI demand and/or access data/ scripts and
experimental packageI attempt to run the source code/
container etc. on my machineI check for data consistencyI check for data availability
0% 20% 40% 60%
Number of respondents using that approach
Fig.8 Validate artifacts and claims
look at the chain of evidence from observation to conclusion and consider whether it is clear
and unbroken. I consider different quality criteria and threats to validity depending on the
type of study and its (often implicit) philosophical position.”
Characteristics of impactful papers Figure 9shows the distribution of how respondents
judge the impact of papers. It is interesting to observe that the top three characteristics are
novelty ,relevance ,a n d methodological rigor , which can sometimes be in conflict with each
other.

2. What, precisely, was your contribution? Before reporting what you did, explain what problem 
you set out to solve or what question you set out to answer 
--and why this is important. 2.1 What kinds of questions do software 
engineers investigate? Generally speaking, software engineering researchers 
seek better ways to develop and evaluate sottware. Devel- 
opment includes all the synthetic activities that involve 
creating and modifying the software, including the code, 
design documents, documentation, etc. Evaluation includes all the analytic activities associated with predict- 
ing, determining, and estimating properties of the software 
systems, including both functionality and extra-functional 
properties such as performance or reliability. Software engineering research answers questions about 
methods of development or analysis, about details of 
designing or evaluating a particular instance, about gener- 
alizations over whole classes of systems or techniques, or 
about exploratory issues concerning existence or feasibil- 
ity. Table 1 lists the types of research questions that are 
asked by software engineering research papers and 
provides specific question templates. Table 1. Types of software engineering research questions 
Type of question Examples 
Method or means of How can we do/create/modify/evolve (or automate doing) X? development What is a better way to do/create/modify/evolve X?

Basics of Software Engi neeri ng Experi ment ation 291 
  
Solution : The effect of experience and of the interaction 
is significant, whereas the effect of the technique is not; 
Technique I used by an experienced developer  
10.7.3. The effect of two modelling techni ques (-A, +A), used by experienced and 
inexperienced people (-B, +B), working in two different domains (-C, +C), 
on small-sized problems (-D, +D) is to be examined. Table 10.33 contains 
a measure of the effort put into gi ven development projects with these 
characteristics. What factors and interactions have significant effects?

Open reviews (e.g., OpenReviews.net) are post-publication reviews in
which the reviewer’s identity is usually known to the author. The International Conference
on Software Engineering conducts post-mortems of each year’s review process. Although
primarily focusing on the mechanism for deciding on paper acceptance, these reports also
explain types of papers accepted and what review information was important in making a
decision16. A recent series by Jacopo Soldani and colleagues in ACM SIGSOFT’s Software
Engineering Notes (Soldani et al. 2020 ) summarizes the “pains and gains” of peer review
and provides discussion about the concerns many scientists have with peer review, such as
workload. 6.2 PeerReviewCriteria
It is common for a peer review request to include some criteria by which the paper is to
be judged. These criteria are particularly important when the paper is a non-standard (i.e.,
non-technical research) format. For example, the Empirical Software Engineering Journal
allows for Technical Papers and Engineering in Practice papers. The Journal of Systems
and Software accepts technical papers, New Ideas and Trends, and In Practice papers. The
International Conference on Software Engineering (ICSE) has numerous tracks, including
Software in Society, Software Engineering in Practice, the main technical track, New Ideas
and Emerging Results, Software Engineering Education and Training, and more. Most conferences have the same or similar reviewing criteria for their research tracks,
typically summarized along the call for papers. The ICSE technical review criteria, for
instance, are available on the conference call for papers. For the 2021 edition, the criteria17
are
– Soundness: The extent to which the paper’s contributions are supported by rigorous
application of appropriate research methods. – Significance: The extent to which the paper’s contributions are important with respect
to open software engineering challenges. – Novelty: The extent to which the contribution is sufficiently original and is clearly
explained with respect to the state-of-the-art. – Verifiability: The extent to which the paper includes sufficient information to support
independent verification or replication of the paper’s claimed contributions. – Presentation: The extent to which the paper’s quality of writing meets the high standards
of ICSE [...]. Common to most review guidelines is a focus on presentation and clarity of the paper,
adherence to the conference or journal topics, and length. Increasingly guidelines are also
emphasizing the importance of constructive criticism (the Reviewer 2 problem (Peterson
16http://www.icse-conferences.org/reports.html
17https://conf.researchr.org/track/icse-2021/icse-2021-papers#Call-for-PapersEmpir Software Eng (2021) 26:  103 103Page26of29

Summary of previous reviews. Description of the software engineering technique being investigated and 
its potential importance. Review questions  Each review question should be 
specified.

56 IEEE SOFTWARE  |  WWW.COMPUTER.ORG/SOFTWARE   |  @IEEESOFTWARE
FOCUS:  SOFTWARE ENGINEERING’S 50TH ANNIVERSARYAs an engineering discipline, SE 
research should strive to impact 
practice. Favoring certain types of 
evidence over others will not suf -
fice. Instead, we require a frame -
work for aggregating the results of multiple pieces of work with differ -
ent types of evidence into action -
able practical feedback. In addition to encouraging technology trans -
fer and true research impact, such a framework can simultaneously open our field to accepting a wider variety of research, including re -
sults that constitute the less rigor -
ous (but still important!) codified knowledge that engineers use in practice. ABOUT THE AUTHORS
CLAIRE LE GOUES  is an assistant 
professor of computer science at Carnegie 
Mellon University’s Institute for Software 
Research. Her research interests lie in 
automatically reasoning about and improv -
ing software quality in real-world, evolving 
systems. Le Goues received a PhD in 
computer science from the University of 
Virginia. Contact her at clegoues@cs.cmu  
.edu, @clegoues. MARY SHAW is the Alan J. Perlis Uni -
versity Professor of Computer Science at Carnegie Mellon University’s Institute for 
Software Research. Her research focuses 
on software engineering and software de
-
sign, particularly software architecture and the design of systems used by real people. Shaw received a PhD in computer science 
from Carnegie Mellon University. She’s a 
Life Fellow of ACM and IEEE. Contact her at 
mary.shaw@cs.cmu.edu. CIERA JASPAN is a senior software engi -
neer at Google. She leads the Engineering Productivity Research team, which aims to 
identify inefficiencies in development tools 
and processes and improve the productiv
-
ity of Google engineers and engineers who use Google products. Jaspan received her 
PhD in software engineering from Carnegie 
Mellon University. Contact her at ciera@
google.com.

[19] R. Kwok, "Research impact: Altmetrics make their mark," 
Nature, vol. 500, pp. 491 -493, 2013. [20] C. Marshall, P. Brereton, and B. Kitchenham, "Tools to 
support systematic reviews in software engineering: a cross -domain 
survey using sem i-structured interviews," presented at the Proceedings 
of the 19th International Conference on Evaluation and Assessment in 
Software Engineering, Nanjing, China, 2015. [21] P. Runeson and M. Höst, "Guidelines for conducting and 
reporting case study research in software engineering," Empirical 
Software Engineering, vol. 14, pp. 131 -164, 2008. [22] A. Sampaio, "Improving Systematic Mapping Reviews," 
SIGSOFT Softw. Eng. Notes, vol. 40, pp. 1 -8, 2015. [23] M. Shaw, "What makes good research in software 
engineering?," J Software Tools for Technology Transfer STTT, vol. 4, 
pp. 1 -7, 2002. [24] M. Shaw, "Writing Good Software Engineering Research 
Papers," Proceedings of 25th Internat ional Conference on Software 
Engineering (ICSE'03), pp. 726 -736, 2003. [25] J. Singer and N. G. Vinson, "Ethical issues in empirical 
studies of software engineering," IEEE Transactions on Software 
Engineering, vol. 28, pp.

• Why should the reader care? • What larger question does this address? • What is your new result? • What new knowledge have you contributed that 
the reader can use elsewhere? • What previous work (yours or someone else's) 
do you build on? What do you provide a superior 
alternative to? • How is your result different from and better than 
this prior work? • What, precisely and in detail, is your new result? • Why should the reader believe your result? • What standard should be used to evaluate your 
claim? • What concrete evidence shows that your result 
satisfies your claim? If you answer these questions clearly, you'll probably 
communicate your result well. If in addition your result 
represents an interesting, sound, and significant contribu- 
tion to our knowledge of software engineering, you'll 
have a good chance of getting it accepted for publication 
in a conference or journal. Other fields of science and engineering have well- 
established research paradigms. For example, the 
experimental model of physics and the double-blind 
studies of medicines are understood, at least in broad 
outline, not only by the research community but also by 
the public at large. In addition to providing guidance for 
the design of research in a discipline, these paradigms 
establish the scope of scientific disciplines through a 
social and political process of "boundary setting" [5]. Software engineering, however, has not yet developed 
this sort of well-understood guidance. I previously [19, 
20] discussed early steps toward such understanding, 
including a model of the way software engineering 
techniques mature [17, 18] and critiques of the lack of 
rigor in experimental software engineering [1, 22, 23, 24, 
25]. Those discussions critique software engineering 
research reports against the standards of classical 
paradigms. The discussion here differs from those in that 
this discussion reports on the types of papers that are 
accepted in practices as good research reports. Another 
current activity, the Impact Project [7] seeks to trace the 
influence of software engineering research on practice. The discussion here focuses on the paradigms rather than 
the content of the research 
This report examines how software engineers answer 
the questions above, with emphasis on the design of the 
research project and the organization of the report. Other 
sources (e.g., [4]) deal with specific issues of technical 
writing. Very concretely, the examples here come from 
the papers submitted to ICSE 2002 and the program 
committee review of those papers. These examples report 
research results in software engineering. Conferences 
often include other kinds of papers, including experience 
reports, materials on software engineering education, and 
opinion essays. 0-7695-1877-X/03 $17.00 © 2003 IEEE 726 


Question: What criteria determine whether a research question is impactful or significant in Software Engineering?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

What is your new result? Explain precisely what you have contributed to the 
store of sot~ware engineering knowledge and how this is 
useful beyond your own project. oOOio -i, i 
80*/0 
60% 
40% 
20% -I 
Iii Dewl Question 
i:G i:~ii i , 
Analy Eval Gener Feas Total 
• Accepted [] Rejected 
Figure 2. Distribution of acceptances and rejections 
by type of research question 
3.1 What kinds of results do software engineers 
produce? The tangible contributions of software engineering 
research may be procedures or techniques for develop- 
ment or analysis; they may be models that generalize from 
specific examples, or they may be specific tools, solutions, 
or results about particular systems. Table 3 lists the types 
of research results that are reported in software engineer- 
ing research papers and provides specific examples. 3.2 Which of these are most common? By far the most common kind of ICSE paper reports a 
new procedure or technique for development or analysis. Models of various degrees of precision and formality were 
also common, with better success rates for quantitative 
than for qualitative models. Tools and notations were well 
represented, usually as auxiliary results in combination 
with a procedure or technique. Table 4 gives the distribu- 
tion of submissions to ICSE 2002, based on reading the 
abstracts (but not the papers), followed by graphs of the 
counts and distributions in Figures 3 and 4. 728 


Of course, one must have the resources to do research this way. 3 Realistic Tasks 
When conducting controlled experiments in software engineering, one should 
consider the realism and representativeness of the tasks regarding the size, complexity and duration of the involved tasks. Specifi cation, implementation and verification 
methods also vary considerably between domains, such as accounting software versus flight-control systems. In our opinion, some experimental tasks bear little resemblance to actual tasks in software engineering; others are very similar to actual tasks [32]. In between there is a continuum. Larger development tasks may take 
months, while many maintenance tasks may take only a couple of hours. 

103–112. Dyba, T., Kitchenham, B.A., Jorgensen, M., 2005. Evidence-based
software engineering for practitioners. IEEE Softw. 22 (1), 58–65. Fenton, N., 1993. How eﬀective are software engineering methods? J. Syst. Softw. 22 (2), 141–146. Fletcher, P., 1995. The role of experiments in computer science. J. Syst. Softw. 30 (1–2), 161–163. Glass, R.L., 1995. Structure-based critique of contemporary computing
research. J. Syst. Softw. 28 (1), 3–7. Glass, R.L., Vessey, I., Ramesh, V., 2002. Research in software engineer-
ing: an analysis of the literature. Information Softw. Technol. 44 (8),491–506. Glass, R.L., Ramesh, V., Vessey, I., 2004. An analysis of research in
computing disciplines. Commun. ACM 47 (6), 89–94. Goldberg, A., 1982. Editorial policy. ACM Comput. Surveys 14 (2), 151–
157. Haddow, G., Klobas, J., 2004. Communication of research to practice in
library and information science: closing the gap. Library Information
Sci.

•They interact with many other development 
techniques and procedures. For example a design method depends on a preceding requirements analysis. It must consider constraints imposed by the software and hardware platform and programming languages, timescales, and budget. It must be integrated with appropriate coding and testing techniques. Thus, it would be difficult to confirm that a design technique had a significant impact on final product reliability. In general, it is difficult to determine a causal link between a particular technique and a desired project outcome when the application of the technique and the final outcome are temporally removed from one another, and there are many other tasks and activities that could also affect the final outcome. •The immediate outcomes of a software engineering 
technique will not necessarily have a strong relationship with final project outcomes. E.g. if you 
are interested in the effect design techniques have on 
application reliability (i.e. probability of failure in a given time period under defined operational conditions), measures of the design product (or 
design process) have no obvious relationship with 
the desired outcome. There are no good surrogate 
measures of product reliability that can be measured 
at the end of the design process. There seem to be two major approaches to this issue: 1.

• What kinds of evidence can demonstrate the validity of a result, and how are 
good results distinguished from bad ones? In this paper I attempt to make generally accepted research strategies in software en-
gineering explicit by examining research in the area to identify what is widely ac-cepted in practice. 1.1   Software Technology Maturation 
Redwine and Riddle [5,6] reviewed a number of software technologies to see how 
they develop and propagate. They found that it typically takes 15-20 years for a tech-nology to evolve from concept formulation to the point where it's ready for populari-zation. They identify six typical phases: 
• Basic research . Investigate basic ideas and concepts, put initial structure on 
the problem, frame critical research questions. • Concept formulation . Circulate ideas informally, develop a research commu-
nity, converge on a compatible set of ideas, publish solutions to specific sub-problems. • Development and extension . Make preliminary use of the technology, clarify 
underlying ideas, generalize the approach. • Internal enhancement and exploration . Extend approach to another domain, 
use technology for real problems, stabilize technology, develop training mate-rials, show value in results. • External enhancement and exploration . Similar to internal, but involving a 
broader community of people who weren’t developers, show substantial evi-dence of value and applicability. • Popularization .

  12 outcomes should be specified. For example, in some cases we require interventions 
that improve some aspect of software  production without affecting another e.g. improved reliability with no increase in cost. A particular problem for soft ware engineering experiments is the widespread use of 
surrogate measures for example, defects found during system testing as a surrogate 
for quality, or coupling measures for de sign quality. Studies that use surrogate 
measures may be misleading and conclusi ons based on such studies may be less 
robust. Context 
For Software Engineering, this is the c ontext in which the co mparison takes place 
(e.g. academia or industry), the participants taking part in  the study (e.g.

have criticized common practice in the field for failing to collect, analyze, and report 
experimental measurements in research reports [9,10,11,12]. In 2001 I [8] presented preliminary sketches of some of the successful paradigms for software engineering research, drawing heavily on examples from software architecture. Scientific and engineering research fields can be characterized by identifying what 
they value: 
• What kinds of questions are "interesting"? • What kinds of results help to answer these questions, and what research 
methods can produce these results?

2   Questions, Results, and Validation in Software Engineering 
Generally speaking, software engineering researchers seek better ways to develop and 
evaluate software. They are motivated by practical problems, and key objectives of the research are often quality, cost, and timeliness of software products. This section presents a model that explains software engineering research papers by 
classifying the types of research questions they ask, the types of results they produce, 
and the character of the validation they provide. This model has evolved over several 
years.

4  Introduct ion 
  Confronted by a confusing array of op tions for producing software, software 
engineers need proof that a particular approach or technique is really better than 
another. They need to know the clear-cut be nefits of one approach versus another. They need reliable evidence that one appr oach clearly works better than another. This need to work with facts rather than assumptions is a property of any 
engineering discipline. Very few ideas in SE are matched with empirical data. Ideas, whose truthfulness has not been tested against reality, are continually assumed as evident. For example, the 
famous software crisis (Naur, 1969) was and still is more a question of a subjective 
customer and/or developer impression than a confirmed phenomenon. In fact, we can find some claims that there is no software crisis (see comments about Turski and 
Hoare opinions given in (Maibaum, 1997)). Another example is the idea that traditional engineering disciplines behave  much better than SE (Pezeé, 1997). For 
example, has any empirical study been pe rformed to compare the failure rate of 
products obtained by SE and those produced by other engineering disciplines? Table 
1.1 presents the percentage of faults in the car industry shown in a study carried out by Lex Vehicle Leasing, one of the major world vehicle hire and leasing companies, 
on a universe of 73,700 cars in England. This fault rate could call the reliability of 
the engineering used in this industry into question; however, this is not the case. Therefore, if we are really to  speak of a software crisis, we  need studies to show that 
SE is less reliable than other branches of engineering, that is, the number of faults in SE should be checked against the number of faults in other branches of engineering. Other examples of computer science theo ries that have not been tested are 
functional programming, object-oriented programming or formal methods.

 SEPTEMBER/OCTOBER 2018  |  IEEE SOFTWARE  51
imagining a dialogue between 
researchers and practitioners (see 
the sidebar). This pattern is common: engi -
neers often rely on their experience 
and a priori beliefs1 or turn to co -
workers for advice. This is better than guessing or giving up. But what 
if incompletely validated research outcomes could be distilled into re -
liable sources, intermediate between 
validated results and folk wisdom? To impact practice, SE research 
results must lead to pragmatic, ac -
tionable advice. This involves syn -
thesizing recommendations from results with different assumptions 
and levels of rigor, assigning appro -
priate levels of confidence to the rec -
ommendations. Here, we examine how these tradeoffs between rigor 
and pragmatism have been handled 
in medicine, where risk is often ac -
ceptable in the face of urgency. We propose an approach to describ -
ing SE research results with varied quality of evidence and synthesizing those results into codified knowledge 
for practitioners. This approach can 
both improve practice and increase the pace of research, especially in ex -
ploratory topics. Software Engineering 
Research Expectations over Time
When the 1968 NATO Conference 
introduced “software engineering” 
to our vocabulary,2 research often 
focused on designing and building programs. There were guidelines for writing programs; the concept of rea -
soning mathematically about a pro -
gram had just been introduced. The 
emphasis was on demonstrated capa -
bility—what we might now call feasi -
bility—rather than rigorous validation. This is visible in a sampling of 
major results of the period. For ex -
ample, Carnegie Mellon University identified a set of canonical papers published between 1968 and 2002. 3 
Several are formal analyses or em -
pirical studies, and a few are case studies. However, the majority are carefully reasoned essays that pro -
pose new approaches based on the authors’ experience and insight. The field has historically built 
on results with varying degrees of certainty. Indeed, Fred Brooks pro -
posed a “certainty-shell” structure RESEARCHERS AND PRACTITIONERS 
VOLLEY ABOUT MAKING RESEARCH USEFUL
Dear Practitioners:
The research community is actually discovering things you might find useful. Please help us organize this knowledge so that 
it’s actually useful to you.

Question: How do we define 'concrete' results in software engineering research?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

objectives, (b) demonstration of novelty and superiority
compared to related work, (c) evidence of eﬀectiveness,reproducibility, scalability, and practical relevance.Reviewers should also ascertain if papers meet quality cri-teria such as conciseness and technical precision, the con-sistent use of terminology, and the deﬁnition of keyconcepts. Focus and limitation of length must often beevaluated together, as the ACM T Database Syst R sug-
gests: ‘‘TODS would like to discourage excessively longpapers (longer than 50 doubled-spaced pages including ﬁg-ures, references, etc.) and unnecessary digressions even inshorter papers. This should help authors to focus on themost important aspects of their submission [ ...]”. For the
rest, practically the same quality criteria as for authorsare brought up, such as clarity, readability and the abilityto target as broad an audience as possible. Studying the abstracts of papers published at ICSE
2002, Shaw (2003) describes how accepted research papers
answered questions such as: What was your contribution?What is your new result? How is your result diﬀerent fromand better than prior work?, etc. According to Shaw, whenit comes to evaluating submitted papers, programme com-mittees look for interesting, novel, and exciting results. Inaddition, as SE knowledge grows incrementally, like in
other areas of science and engineering, programme com-
mittees are very interested in the authors’ interpretationof prior work. 3.2. Extended versions of conference papersA special type of articles are those based on one or
more papers previously presented at a workshop, a sym-
posium or a conference. For instance, van Vliet (2006)
is an extended version of van Vliet (2005) . In the study
previously mentioned ( Montesi and Mackenzie-Owen,
2008), comparing article genres in SE, education and
biology, it was found that extended versions of confer-ence papers played a role only in SE. A distinguishing
feature of extended papers is the double round of
reviews they need to pass before journal publication(i.e. conference and journal reviews). In a survey withauthors of extended versions and journal editors, Mon-
tesi and Mackenzie-Owen (accepted for publication)found that the extension process may change dependingon the type of conference paper to extend (empirical,theoretical, ...). Some journal instructions to authors regulate the pub-
lication of these articles. Among the other conditions, onethird of the content approximately must be new and ori-ginal, and sometimes authors must submit both versions,a description of the diﬀerences between the submittedmanuscript and the preliminary version, and referencethe previous version in the paper (SoSyM). Papers pub-lished as such must be ‘‘important ”(Theor Pract Log
Prog), ‘‘outstanding ”(ACM T Database Syst ), and ‘‘of
particularly high quality ”(J Comput Sci Technol ). New
material takes in explorations of further research, theo-rems, proofs and/or implementation details ( ACM T
Graphic ). If journals address reviewers the general tone doesn’t
change, though they oﬀer some more clues on what to lookfor. In the ACM T Database Syst -R , for instance, extended
versions of conference papers ‘‘should thoroughly consoli-date the material, should extend it to be broader, andshould more carefully cover related research ”. In addition,
the novel approach should be described more deeply, andalternatives should also be considered more comprehen-sively. In IEEE T Vis Comput Gr -R , while no new results
are required, key ideas, examples, and further elaborationare expected. This conﬁrms Shaw’s observation that in con-
ferences time and space limitations do not allow authors to
explore all the possible results of their research ( Shaw,
2003).Table 2
Excerpt from the instructions of IEE Proc-Softw
Originality: is the work scientiﬁcally rigorous, accurate and novel? Does the work contain signiﬁcant additional material to that already published?

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

• Papers not subject to peer-review. When an SLR has been published in more than one journal/conference, the most 
complete version of the survey will be used. Primary study selection process 
 
The results will be tabulated as follows: 
• Number of papers per year per source 
• Number of candidate pape rs per year per source 
• Number of selected papers per year per source. The relevant candidate and selected studies will be selected by a single researcher. The rejected studies will be checked by a nother researcher. We will maintain a list 
candidate papers that were rejected  with reasons for the rejection. Quality Assessment 
 Each SLR will be evaluated using the York University, Centre for Reviews and Dissemination (CDR) Database of Abstracts of Reviews of Effects (DARE) criteria 
(http://www.york.ac.uk/inst/crd/crddatabase .htm#DARE). The criteria are based on 
four questions: 
 
• Are the review’s inclusion and exclusi on criteria describe d and appropriate? • Is the literature search likely to have covered all relevant studies? • Did the reviewers assess the quality/v alidity of the included studies? • Were the basic data/studies adequately described? The questions are scored as follows:  
• Question 1: Y (yes), the inclusion criteria  are explicitly defined in the paper, P 
(Partly), the inclusion criteria are implic it; N (no), the inclusion criteria are not 
defined and cannot be readily inferred. • Question 2: Y, the authors have either s earched 4 or more digital libraries and 
included additional search strategies or identified and referenced all journals 
addressing the topic of interest; P, the authors have searched 3 or 4 digital 
libraries with no extra search strategies, or searched a define d but restricted set 
of journals and conference proceedings;  N, the authors have search up to 2 
digital libraries or an extremel y restricted set of journals. • Question 3: Y, the authors have explicit ly defined quality criteria and extracted 
them from each primary study; P, the re search question involves quality issues 
that are addressed by the study; N no e xplicit quality assessm ent of individual 
papers has been attempted. 

Has its value been demonstrated? Relevance: is the material appropriate to the scope of the journal to which it is submitted? Motivation: does the problem considered have a sound motivation? Does the paper clearly demonstrate the scientiﬁc interest of the results? Referencing: has reference been made to the most recent and most appropriate work? Is the present work set in the context of the previous work? Clarity: is the English clear and well written? Poorly written English may obscure the scientiﬁc merit of your paper. Are the ideas expressed clearly
and concisely?

Characteristicsofhighqualitypapers Table 4shows the most common results from our
coding of the free-text responses to this question. Overall, the most common characteris-
tics reviewers want in a high quality paper relate to aspects of the quality of the research
methodology and how well it is presented in the paper. It is also interesting to note which
characteristics were not among the most commonly given answers, including the availability
of the data (5), the findings themselves (1), and the generalizability of the results (1). Characteristicsleadingtonegativereviews Figure 11shows the ranking of the charac-
teristics that tend to lead to negative reviews. The highest ranked characteristics include (1)
a mismatch of methods with claims raised in the contribution, (2) overly grandiose claims,
(3) a writing style that is hard to follow, (4) a research methodology which is difficultEmpir Software Eng (2021) 26:  103 103Page14of29

Tools that originated from a new technique also described in the 
paper would fall under this category. • “Specific Solution, Prototype, Answer, or Jud gment” covers many 
different types of results, including analysis of existing systems or 
specific analysis of a technique. • •“Empirical Report” covers papers that focus on a set of 
observations, either quantitative or qualitative, and could 
potentially incl ude a general prescription of guidelines based on 
the data in the report. The “Empirical Report” category includes 
the presentation of data or insights typically found in MSR papers. “Empirical Report” is a modification of the original classification scheme, which was called “Report”. The new name emphasizes 
the type of contribution that results of MSR papers.

3   Research Strategies 
Section 2 identifies the three important aspects of an individual research result as 
reported in a typical conference or journal paper. It is clear that the spectrum of good research strategies includes experimental computer science in the sense of [9,10,11, 12]; it is also clear that the spectrum is much broader than just experimental research. Of course, not all the combinations of question, result, and validation make sense.

How can one judgeit? It is very diﬃcult to give a satisfactory and deﬁnitiveanswer to such questions without the consensus of a greatpart of the community, which requires time and greateﬀort. A possible alternative is to summarize and organizethe written information currently available on the matter.And this is precisely the aim of this study. 2. Methodology
The present description of the major types of research
papers published in SE is based exclusively on existing writ-
ten sources, and in particular on the following:(1) Published, relevant papers, such as, for example,
Zelkowitz and Wallace (1998) orSjøberg et al. (2005) .

Use them consistently. The more formal or analytic the paper, the more important 
this is. Here are some questions that the program committee 
may ask about your paper: 
What, precisely, do you claim to contribute? Does your result fiJlly satisfy your claims? Are the 
definitions precise, and are terms used consistently? Authors tend to have trouble in some specific 
situations. Here are some examples, with advice for 
staying out of trouble:" 
• If your result ought to work on large systems, explain 
why you believe it scales. • If you claim your method is "automatic", using it 
should not require human intervention. If it's 
automatic when it's operating but requires manual 
assistance to configure, say so. If it's automatic 
except for certain eases, say so, and say how often 
the exceptions occur. • If you claim your result is "distributed", it probably 
should not have a single central controller or server. If it does, explain what part of it is distributed and 
what part is not. • If you're proposing a new notation for an old 
problem, explain why your notation is clearly 
superior to the old one. • If your paper is an "experience report", relating the 
use of a previously-reported tool or technique in a 
practical software project, be sure that you explain 
what idea the reader can take away from the paper to use in other settings. If that idea is increased 
confidence in the tool or technique, show how your 
experience should increase the reader's confidence 
for applications beyond the example of the paper. What ~ new here? The program committee wants to know what is novel 
or exciting, and why. What, specifically, is the 
contribution? What is the increment over earlier work by 
the same authors? by other authors? Is this a sufficient 
increment, given the usual standards of subdiscipline? Above all, the program committee also wants to know 
what you actually contributed to our store of knowledge 
about software engineering. Sure, you wrote this tool and 
tried it out. But was your contribution the technique that is 
embedded in the tool, or was it making a tool that's more 
effective than other tools that implement the technique, or 
was it showing that the tool you described in a previous 
paper actually worked on a practical large-scale problem? It's better for you as the author to explain than for the 
program committee to guess. Be clear about your claim ... Awful • • I completely and generally solved ... I (unless you actually did!) 
Bad • • I worked on galumphing. (or studied, investigated, sought, 
explored) 
Poor • • I worked on improving galumphing. (or contributed to, participated in, 
helped with) 
Good • • I showed the feasibility of composing 
blitzing with flitzing. i • I significantly improved the accuracy of 
the standard detector. (or proved, demonstrated, created, 
established, found, developed) 
Better • • I automated the production of ritz 
tables from specifications. • With a novel application of the blivet 
transform, I achieved a 10% increase 
in speed and a 15% improvement in 
coverage over the standard method. Use verbs that show results and achievement, not just 
effort and activity.

Examples 
Kitchenham et al. used the following inclusion criteria: 
• any study that compared predictions of cross-company models with within-
company models based on analysis of single company project data. They used the following exclusion criteria: 
• studies where projects were only collect ed from a small number of different sources 
(e.g. 2 or 3 companies),  
• studies where models derived from a with in-company data set were compared with 
predictions from a general cost estimation model. Jørgensen 206H[17] included papers that compare j udgment-based and model-based software 
development effort estimation. He also ex cluded one relevant paper due to “incomplete 
information about how the estimates were derived”. Issues: 
• Medical standards make a point that it is important to avoid, as far as possible, 
exclusions based on the language of th e primary study. This may not be so 
important for Software Engineering. • It is possible that inclus ion decisions could be affected by knowledge of the 
authors, institutions, journals or year of publication. Some medical researchers 
have suggested reviews should be done af ter such information has been removed. However, it takes time to do this and experimental evidence suggests that 
masking the origin of primary studies does not improve reviews 207H[4]. 6.2.2 Study selection process 
Study selection is a multistage process. Initially, selection criteria should be 
interpreted liberally, so that unless a study identifie d by the electronic and hand 
searches can be clearly excluded based on title and abstract, a full copy should be 
obtained. However, Brereton et al. 208H[5] point out that  “The standard of IT and software 
engineering abstracts is too poor to rely  on when selecting primary studies. You 
should also review the conclusions.” 
 
The next step is to apply inclusion/excl usion criteria based on practical issues 209H[11] 
such as: 
• Language 
• Journal 
• Authors 
• Setting 
• Participants or subjects 
• Research Design 
• Sampling method 
• Date of publication. Staples and Niazi point out th at it is sometimes necessary to consider the questions 
that are not being addressed in orde r to refine your exclusion criteria 210H[27]. Example 
Staples and Niazi’s research question was 

Question: What are the criteria for 'specific' results in academic papers?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During the analysis it is important to maintain a chain ofevidence from the findings to the original data. The report should include sufficient dataand examples to allow the reader to understand the chain of evidence. Table 11 Reader ’s checklist items
39. Are the objective, research questions, and hypotheses (if applicable) clear and relevant? 1, 2, 5, 29, 30
40. Are the case and its units of analysis well defined? 1, 5, 28
41. Is the suitability of the case to address the research questions clearly motivated? 8, 9, 1442. Is the case study based on theory or linked to existing literature? 343. Are the data collection procedures sufficient for the purpose of the case study (data sources, collection,
validation)? 11, 13, 16, 18, 21, 31
44. Is sufficient raw data presented to provide understanding of the case and the analysis? 3245. Are the analysis procedures sufficient for the purpose of the case study (repeatable, transparent)? 22, 3346.

Analysis of claim (d) 

Evaluating experiment quality. Even in EBM, it 
is recognised that it is sometimes impossible to perform randomised trials and evidence from other types of 
experiment may need to be considered. The Australian 
National Health and Medical Research Council have published guidelines for evaluating the quality of evidence [2]. They consider:  
•The strength of the evidence . This has three 
elements: Level, Quality, and Statistical Precision. Level relates to the choice of study design and is used as an indicator to which bias has been eliminated by design. Quality refers to the methods used by the investigators to minimize bias within the study design. Statistical Precision refers to the P-value or the confidence interval. •Size of effect . The distance of the estimated treatment 
effect from the null value and the inclusion of 
clinically important effects in the confidence interval. •Relevance of evidence . The usefulness of the 
evidence in clinical practice, particularly the appropriateness of the outcome measures used. These criteria appear to be equally valid for software 
engineering evidence.

 33% (38) 
 8% (9)  65% (74) 
 36% (41) 
 25% (29) 
 12% (14) 
 14% (16) 
 4% (4)  25% (15) 
 7% (4)  74% (45) 
 52% (32) 
 26% (16) 
 13% (8) 
 8% (5) 
 5% (3) Award
NoAward
OtherI carefully calibrate my epistemological
perspective with the authorsI do not validate these artifacts.I do not review these types of papersI demand and/or access data/ scripts and
experimental packageI attempt to run the source code/
container etc. on my machineI check for data consistencyI check for data availability
0% 20% 40% 60%
Number of respondents using that approach
Fig.8 Validate artifacts and claims
look at the chain of evidence from observation to conclusion and consider whether it is clear
and unbroken. I consider different quality criteria and threats to validity depending on the
type of study and its (often implicit) philosophical position.”
Characteristics of impactful papers Figure 9shows the distribution of how respondents
judge the impact of papers. It is interesting to observe that the top three characteristics are
novelty ,relevance ,a n d methodological rigor , which can sometimes be in conflict with each
other.

  24 They also considered the reporting  quality based on 4 questions: 
1. Is it clear what projects were used to construct each model? 2. Is it clear how accuracy was measured? 3. Is it clear what cross-validation method was used? 4.

9. Is there evidence that the 
E/QE can be used by other researchers / practitioners? Do the authors discuss whether or how the findi ngs can be transferred to other populations, or 
consider other ways in whic h the research can be used? To what extent do authors interpret results in the context of other studies / the existing body of 
knowledge?

Has its value been demonstrated? Relevance: is the material appropriate to the scope of the journal to which it is submitted? Motivation: does the problem considered have a sound motivation? Does the paper clearly demonstrate the scientiﬁc interest of the results? Referencing: has reference been made to the most recent and most appropriate work? Is the present work set in the context of the previous work? Clarity: is the English clear and well written? Poorly written English may obscure the scientiﬁc merit of your paper. Are the ideas expressed clearly
and concisely?

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

Figure 6. Distribution of acceptances and rejections 
by type of validation 
Is the validation related to the claim? If you're claiming 
performance improvement, validation should analyze 
performance, not ease of use or generality. And 
conversely. Is this such an interesting, potentially powerful idea 
that it should get exposure despite a shortage of concrete 
evidence? Authors tend to have trouble in some specific 
situations. Here are some examples, with advice for 
staying out of trouble: 
• If you claim to improve on prior art, compare your 
result objectively to the prior art. • If you used an analysis technique, follow the rules of 
that analysis technique. If the technique is not a 
common one in soRware engineering (e.g., meta- 
analysis, decision theory, user studies ° or other 
behavioral analyses), explain the technique and 
standards of proof, and be clear about your 
adherence to the technique. • If you offer practical experience as evidence for your 
result, establish the effect your research has. If at all 
possible, compare similar situations with and without 
your result. 733 


Are the review’s inclusion and exclus ion criteria described and appropriate? 2. Is the literature search likely to ha ve covered all relevant studies? 3. Did the reviewers assess the quality/v alidity of the in cluded studies? 4. Were the basic data/studies adequately described? Examples 
 We applied the DARE criteria bot h to Kitchenham et al.’s study 
186H[21] and to Jørgensen’s study 
187H[17]. We gave Kitchenham et al.’s study a score of 4 and Jørgensen’s study a score of 3.5. Other studies scored using the DARE criteria are listed in Appendix 2. From a more general viewpoint, Greenlaugh 188H[12] suggests the following questions: 
• Can you find an important clinical qu estion, which the review addressed? (Clearly, in software engin eering, this should be adapted to refer to an important 
software engineering question.) 
• Was a thorough search done of the a ppropriate databases and were other 
potentially important sources explored? • Was methodological quality assessed and the trials weighted accordingly? • How sensitive are the results to the way that the review has been done?

Question: How should 'relatedness' between evidence and claims be defined and assessed?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m
[38;5;200m[1;3mcontent="```\n        Manuscript Text: Software documentation has inspired research mainly on two fronts: tools & approaches and (empirical) studies.\n        Review: This sentence provides a good overview of the main areas of research inspired by software documentation, setting the stage for the reader to understand the focus of the subsequent discussion.\n\n        Manuscript Text: Despite these efforts, automated documentation is still wishful thinking.\n        Review: This statement highlights a gap in the current state of automated documentation, which can pique the reader's interest in understanding the challenges and potential solutions in this area.\n\n        Manuscript Text: A first research road map to enable automated on-demand documentation has however been drawn by the recent proposal of Robillard et al.\n        Review: This sentence introduces a specific research roadmap proposed by Robillard et al., indicating a concrete direction in addressing the challenge of automated documentation. It would be beneficial to provide more details on this roadmap to enhance the clarity and specificity of the result.\n```" response_metadata={'token_usage': {'completion_tokens': 195, 'prompt_tokens': 999, 'total_tokens': 1194}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-abd98193-6087-4df2-b2dd-de6a697acbb3-0'[0m[32;1m[1;3mAction: Fetch Section Content by Title
Action Input: {"section_title": "API documentation", "path": "/Users/crystalalice/Desktop/ICSHP_Research/SE_paper/Software_Documentation_Issues_Unveiled.pdf"}[0m[33;1m[1;3mThe top 10 problems in API documentation are (i) incompleteness, (ii) ambiguity, (iii) unexplained
examples, (iv) obsoleteness, (v) inconsistency and (vi) incorrectness; while in presentation are (vii)
bloat, (viii) fragmentation, (ix) excess structural information and (x) tangled information.
Alhindawi et al. (2016) [46]:
Topic-modeling-based study.
KDE/KOfﬁce
source
base and its external
documentation
A novel approach for evaluating documentation quality. Tools that can automatically assess the
software documentation quality in an are highly demanded. Labeling and grouping documentation
would impact its quality positively.
Sohan et al. (2017) [47]:
Controlled study with 26 software engineers.
WordPress REST API
documentation
Developers feel more satisﬁed when having examples. When documentation lacks examples,
developers spend more time on coding, execute more trial attempts, and have lower success rates.
(Empirical) Studies. Software documentation has been
analyzed in diverse empirical studies that (i) report evidence
of its importance and impact in the software life cycle [1], [3],
[6], [41], [42], [44], (ii) describe problems that developers face
when dealing with it [3], [6], [40], [42], [45], (iii) list quality
attributes required in documentation [42], [44], [45], [48], [49],
(iv) provide recommendations for constructing it (including
standards) [1], [3], [40]–[42], [44], [45], [50], [51], and (v)
propose frameworks and tools for evaluating documentation
concerns such as cost, beneﬁt and quality attributes [44], [46]–
[49]. Due to space limitations we summarize the closest ones
to our study in Table I.
The mapping study by Zhi et al. [4] is notable, as it reviews
about 100 documentation-related papers and reports that the
most frequently discussed quality attributes are completeness,
consistency and accessibility. Zhi et al. conclude that software
documentation is an immature area, and that stronger empirical
evidence is needed to gain a deeper understanding of it.
Most of the aforementioned studies gathered information
directly from participants and used practitioner samples re-
stricted to a speciﬁc context (e.g., a company). These studies
are therefore not diverse enough in terms of analyzed artifacts
and programming languages used by developers, and the largest
samples reported in the studies are 440 (Robillard and Deline
[42]) and 230 practitioners (Uddin and Robillard [45]). To avoid
some of the limitations imposed by interviews and surveys,
we opted for an approach that allowed us to study a wider
population in terms of number and types of artifacts by mining
different data sources.
Our results complement previous categorizations of docu-
mentation issues with a taxonomy that considers documentation
content, processes and tools.
Ours is the ﬁrst mining-based study focused on identifying
documentation issues as discussed by practitioners in software
repositories. Previous studies following a mining-based strategy
are more general, identifying topics discussed by developers
[52]–[54] or by apps’ users [55].
[0m[32;1m[1;3mAction: Generate Review
Action Input: The top 10 problems in API documentation are (i) incompleteness, (ii) ambiguity, (iii) unexplained examples, (iv) obsoleteness, (v) inconsistency, and (vi) incorrectness; while in presentation are (vii) bloat, (viii) fragmentation, (ix) excess structural information, and (x) tangled information.[0m

[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

What is your new result? Explain precisely what you have contributed to the 
store of sot~ware engineering knowledge and how this is 
useful beyond your own project. oOOio -i, i 
80*/0 
60% 
40% 
20% -I 
Iii Dewl Question 
i:G i:~ii i , 
Analy Eval Gener Feas Total 
• Accepted [] Rejected 
Figure 2. Distribution of acceptances and rejections 
by type of research question 
3.1 What kinds of results do software engineers 
produce? The tangible contributions of software engineering 
research may be procedures or techniques for develop- 
ment or analysis; they may be models that generalize from 
specific examples, or they may be specific tools, solutions, 
or results about particular systems. Table 3 lists the types 
of research results that are reported in software engineer- 
ing research papers and provides specific examples. 3.2 Which of these are most common? By far the most common kind of ICSE paper reports a 
new procedure or technique for development or analysis. Models of various degrees of precision and formality were 
also common, with better success rates for quantitative 
than for qualitative models. Tools and notations were well 
represented, usually as auxiliary results in combination 
with a procedure or technique. Table 4 gives the distribu- 
tion of submissions to ICSE 2002, based on reading the 
abstracts (but not the papers), followed by graphs of the 
counts and distributions in Figures 3 and 4. 728 


Of course, one must have the resources to do research this way. 3 Realistic Tasks 
When conducting controlled experiments in software engineering, one should 
consider the realism and representativeness of the tasks regarding the size, complexity and duration of the involved tasks. Specifi cation, implementation and verification 
methods also vary considerably between domains, such as accounting software versus flight-control systems. In our opinion, some experimental tasks bear little resemblance to actual tasks in software engineering; others are very similar to actual tasks [32]. In between there is a continuum. Larger development tasks may take 
months, while many maintenance tasks may take only a couple of hours. 

103–112. Dyba, T., Kitchenham, B.A., Jorgensen, M., 2005. Evidence-based
software engineering for practitioners. IEEE Softw. 22 (1), 58–65. Fenton, N., 1993. How eﬀective are software engineering methods? J. Syst. Softw. 22 (2), 141–146. Fletcher, P., 1995. The role of experiments in computer science. J. Syst. Softw. 30 (1–2), 161–163. Glass, R.L., 1995. Structure-based critique of contemporary computing
research. J. Syst. Softw. 28 (1), 3–7. Glass, R.L., Vessey, I., Ramesh, V., 2002. Research in software engineer-
ing: an analysis of the literature. Information Softw. Technol. 44 (8),491–506. Glass, R.L., Ramesh, V., Vessey, I., 2004. An analysis of research in
computing disciplines. Commun. ACM 47 (6), 89–94. Goldberg, A., 1982. Editorial policy. ACM Comput. Surveys 14 (2), 151–
157. Haddow, G., Klobas, J., 2004. Communication of research to practice in
library and information science: closing the gap. Library Information
Sci.

•They interact with many other development 
techniques and procedures. For example a design method depends on a preceding requirements analysis. It must consider constraints imposed by the software and hardware platform and programming languages, timescales, and budget. It must be integrated with appropriate coding and testing techniques. Thus, it would be difficult to confirm that a design technique had a significant impact on final product reliability. In general, it is difficult to determine a causal link between a particular technique and a desired project outcome when the application of the technique and the final outcome are temporally removed from one another, and there are many other tasks and activities that could also affect the final outcome. •The immediate outcomes of a software engineering 
technique will not necessarily have a strong relationship with final project outcomes. E.g. if you 
are interested in the effect design techniques have on 
application reliability (i.e. probability of failure in a given time period under defined operational conditions), measures of the design product (or 
design process) have no obvious relationship with 
the desired outcome. There are no good surrogate 
measures of product reliability that can be measured 
at the end of the design process. There seem to be two major approaches to this issue: 1.

• What kinds of evidence can demonstrate the validity of a result, and how are 
good results distinguished from bad ones? In this paper I attempt to make generally accepted research strategies in software en-
gineering explicit by examining research in the area to identify what is widely ac-cepted in practice. 1.1   Software Technology Maturation 
Redwine and Riddle [5,6] reviewed a number of software technologies to see how 
they develop and propagate. They found that it typically takes 15-20 years for a tech-nology to evolve from concept formulation to the point where it's ready for populari-zation. They identify six typical phases: 
• Basic research . Investigate basic ideas and concepts, put initial structure on 
the problem, frame critical research questions. • Concept formulation . Circulate ideas informally, develop a research commu-
nity, converge on a compatible set of ideas, publish solutions to specific sub-problems. • Development and extension . Make preliminary use of the technology, clarify 
underlying ideas, generalize the approach. • Internal enhancement and exploration . Extend approach to another domain, 
use technology for real problems, stabilize technology, develop training mate-rials, show value in results. • External enhancement and exploration . Similar to internal, but involving a 
broader community of people who weren’t developers, show substantial evi-dence of value and applicability. • Popularization .

  12 outcomes should be specified. For example, in some cases we require interventions 
that improve some aspect of software  production without affecting another e.g. improved reliability with no increase in cost. A particular problem for soft ware engineering experiments is the widespread use of 
surrogate measures for example, defects found during system testing as a surrogate 
for quality, or coupling measures for de sign quality. Studies that use surrogate 
measures may be misleading and conclusi ons based on such studies may be less 
robust. Context 
For Software Engineering, this is the c ontext in which the co mparison takes place 
(e.g. academia or industry), the participants taking part in  the study (e.g.

have criticized common practice in the field for failing to collect, analyze, and report 
experimental measurements in research reports [9,10,11,12]. In 2001 I [8] presented preliminary sketches of some of the successful paradigms for software engineering research, drawing heavily on examples from software architecture. Scientific and engineering research fields can be characterized by identifying what 
they value: 
• What kinds of questions are "interesting"? • What kinds of results help to answer these questions, and what research 
methods can produce these results?

2   Questions, Results, and Validation in Software Engineering 
Generally speaking, software engineering researchers seek better ways to develop and 
evaluate software. They are motivated by practical problems, and key objectives of the research are often quality, cost, and timeliness of software products. This section presents a model that explains software engineering research papers by 
classifying the types of research questions they ask, the types of results they produce, 
and the character of the validation they provide. This model has evolved over several 
years.

4  Introduct ion 
  Confronted by a confusing array of op tions for producing software, software 
engineers need proof that a particular approach or technique is really better than 
another. They need to know the clear-cut be nefits of one approach versus another. They need reliable evidence that one appr oach clearly works better than another. This need to work with facts rather than assumptions is a property of any 
engineering discipline. Very few ideas in SE are matched with empirical data. Ideas, whose truthfulness has not been tested against reality, are continually assumed as evident. For example, the 
famous software crisis (Naur, 1969) was and still is more a question of a subjective 
customer and/or developer impression than a confirmed phenomenon. In fact, we can find some claims that there is no software crisis (see comments about Turski and 
Hoare opinions given in (Maibaum, 1997)). Another example is the idea that traditional engineering disciplines behave  much better than SE (Pezeé, 1997). For 
example, has any empirical study been pe rformed to compare the failure rate of 
products obtained by SE and those produced by other engineering disciplines? Table 
1.1 presents the percentage of faults in the car industry shown in a study carried out by Lex Vehicle Leasing, one of the major world vehicle hire and leasing companies, 
on a universe of 73,700 cars in England. This fault rate could call the reliability of 
the engineering used in this industry into question; however, this is not the case. Therefore, if we are really to  speak of a software crisis, we  need studies to show that 
SE is less reliable than other branches of engineering, that is, the number of faults in SE should be checked against the number of faults in other branches of engineering. Other examples of computer science theo ries that have not been tested are 
functional programming, object-oriented programming or formal methods.

 SEPTEMBER/OCTOBER 2018  |  IEEE SOFTWARE  51
imagining a dialogue between 
researchers and practitioners (see 
the sidebar). This pattern is common: engi -
neers often rely on their experience 
and a priori beliefs1 or turn to co -
workers for advice. This is better than guessing or giving up. But what 
if incompletely validated research outcomes could be distilled into re -
liable sources, intermediate between 
validated results and folk wisdom? To impact practice, SE research 
results must lead to pragmatic, ac -
tionable advice. This involves syn -
thesizing recommendations from results with different assumptions 
and levels of rigor, assigning appro -
priate levels of confidence to the rec -
ommendations. Here, we examine how these tradeoffs between rigor 
and pragmatism have been handled 
in medicine, where risk is often ac -
ceptable in the face of urgency. We propose an approach to describ -
ing SE research results with varied quality of evidence and synthesizing those results into codified knowledge 
for practitioners. This approach can 
both improve practice and increase the pace of research, especially in ex -
ploratory topics. Software Engineering 
Research Expectations over Time
When the 1968 NATO Conference 
introduced “software engineering” 
to our vocabulary,2 research often 
focused on designing and building programs. There were guidelines for writing programs; the concept of rea -
soning mathematically about a pro -
gram had just been introduced. The 
emphasis was on demonstrated capa -
bility—what we might now call feasi -
bility—rather than rigorous validation. This is visible in a sampling of 
major results of the period. For ex -
ample, Carnegie Mellon University identified a set of canonical papers published between 1968 and 2002. 3 
Several are formal analyses or em -
pirical studies, and a few are case studies. However, the majority are carefully reasoned essays that pro -
pose new approaches based on the authors’ experience and insight. The field has historically built 
on results with varying degrees of certainty. Indeed, Fred Brooks pro -
posed a “certainty-shell” structure RESEARCHERS AND PRACTITIONERS 
VOLLEY ABOUT MAKING RESEARCH USEFUL
Dear Practitioners:
The research community is actually discovering things you might find useful. Please help us organize this knowledge so that 
it’s actually useful to you.

Question: How do we define 'concrete' results in software engineering research?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

objectives, (b) demonstration of novelty and superiority
compared to related work, (c) evidence of eﬀectiveness,reproducibility, scalability, and practical relevance.Reviewers should also ascertain if papers meet quality cri-teria such as conciseness and technical precision, the con-sistent use of terminology, and the deﬁnition of keyconcepts. Focus and limitation of length must often beevaluated together, as the ACM T Database Syst R sug-
gests: ‘‘TODS would like to discourage excessively longpapers (longer than 50 doubled-spaced pages including ﬁg-ures, references, etc.) and unnecessary digressions even inshorter papers. This should help authors to focus on themost important aspects of their submission [ ...]”. For the
rest, practically the same quality criteria as for authorsare brought up, such as clarity, readability and the abilityto target as broad an audience as possible. Studying the abstracts of papers published at ICSE
2002, Shaw (2003) describes how accepted research papers
answered questions such as: What was your contribution?What is your new result? How is your result diﬀerent fromand better than prior work?, etc. According to Shaw, whenit comes to evaluating submitted papers, programme com-mittees look for interesting, novel, and exciting results. Inaddition, as SE knowledge grows incrementally, like in
other areas of science and engineering, programme com-
mittees are very interested in the authors’ interpretationof prior work. 3.2. Extended versions of conference papersA special type of articles are those based on one or
more papers previously presented at a workshop, a sym-
posium or a conference. For instance, van Vliet (2006)
is an extended version of van Vliet (2005) . In the study
previously mentioned ( Montesi and Mackenzie-Owen,
2008), comparing article genres in SE, education and
biology, it was found that extended versions of confer-ence papers played a role only in SE. A distinguishing
feature of extended papers is the double round of
reviews they need to pass before journal publication(i.e. conference and journal reviews). In a survey withauthors of extended versions and journal editors, Mon-
tesi and Mackenzie-Owen (accepted for publication)found that the extension process may change dependingon the type of conference paper to extend (empirical,theoretical, ...). Some journal instructions to authors regulate the pub-
lication of these articles. Among the other conditions, onethird of the content approximately must be new and ori-ginal, and sometimes authors must submit both versions,a description of the diﬀerences between the submittedmanuscript and the preliminary version, and referencethe previous version in the paper (SoSyM). Papers pub-lished as such must be ‘‘important ”(Theor Pract Log
Prog), ‘‘outstanding ”(ACM T Database Syst ), and ‘‘of
particularly high quality ”(J Comput Sci Technol ). New
material takes in explorations of further research, theo-rems, proofs and/or implementation details ( ACM T
Graphic ). If journals address reviewers the general tone doesn’t
change, though they oﬀer some more clues on what to lookfor. In the ACM T Database Syst -R , for instance, extended
versions of conference papers ‘‘should thoroughly consoli-date the material, should extend it to be broader, andshould more carefully cover related research ”. In addition,
the novel approach should be described more deeply, andalternatives should also be considered more comprehen-sively. In IEEE T Vis Comput Gr -R , while no new results
are required, key ideas, examples, and further elaborationare expected. This conﬁrms Shaw’s observation that in con-
ferences time and space limitations do not allow authors to
explore all the possible results of their research ( Shaw,
2003).Table 2
Excerpt from the instructions of IEE Proc-Softw
Originality: is the work scientiﬁcally rigorous, accurate and novel? Does the work contain signiﬁcant additional material to that already published?

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

• Papers not subject to peer-review. When an SLR has been published in more than one journal/conference, the most 
complete version of the survey will be used. Primary study selection process 
 
The results will be tabulated as follows: 
• Number of papers per year per source 
• Number of candidate pape rs per year per source 
• Number of selected papers per year per source. The relevant candidate and selected studies will be selected by a single researcher. The rejected studies will be checked by a nother researcher. We will maintain a list 
candidate papers that were rejected  with reasons for the rejection. Quality Assessment 
 Each SLR will be evaluated using the York University, Centre for Reviews and Dissemination (CDR) Database of Abstracts of Reviews of Effects (DARE) criteria 
(http://www.york.ac.uk/inst/crd/crddatabase .htm#DARE). The criteria are based on 
four questions: 
 
• Are the review’s inclusion and exclusi on criteria describe d and appropriate? • Is the literature search likely to have covered all relevant studies? • Did the reviewers assess the quality/v alidity of the included studies? • Were the basic data/studies adequately described? The questions are scored as follows:  
• Question 1: Y (yes), the inclusion criteria  are explicitly defined in the paper, P 
(Partly), the inclusion criteria are implic it; N (no), the inclusion criteria are not 
defined and cannot be readily inferred. • Question 2: Y, the authors have either s earched 4 or more digital libraries and 
included additional search strategies or identified and referenced all journals 
addressing the topic of interest; P, the authors have searched 3 or 4 digital 
libraries with no extra search strategies, or searched a define d but restricted set 
of journals and conference proceedings;  N, the authors have search up to 2 
digital libraries or an extremel y restricted set of journals. • Question 3: Y, the authors have explicit ly defined quality criteria and extracted 
them from each primary study; P, the re search question involves quality issues 
that are addressed by the study; N no e xplicit quality assessm ent of individual 
papers has been attempted. 

Has its value been demonstrated? Relevance: is the material appropriate to the scope of the journal to which it is submitted? Motivation: does the problem considered have a sound motivation? Does the paper clearly demonstrate the scientiﬁc interest of the results? Referencing: has reference been made to the most recent and most appropriate work? Is the present work set in the context of the previous work? Clarity: is the English clear and well written? Poorly written English may obscure the scientiﬁc merit of your paper. Are the ideas expressed clearly
and concisely?

Characteristicsofhighqualitypapers Table 4shows the most common results from our
coding of the free-text responses to this question. Overall, the most common characteris-
tics reviewers want in a high quality paper relate to aspects of the quality of the research
methodology and how well it is presented in the paper. It is also interesting to note which
characteristics were not among the most commonly given answers, including the availability
of the data (5), the findings themselves (1), and the generalizability of the results (1). Characteristicsleadingtonegativereviews Figure 11shows the ranking of the charac-
teristics that tend to lead to negative reviews. The highest ranked characteristics include (1)
a mismatch of methods with claims raised in the contribution, (2) overly grandiose claims,
(3) a writing style that is hard to follow, (4) a research methodology which is difficultEmpir Software Eng (2021) 26:  103 103Page14of29

Tools that originated from a new technique also described in the 
paper would fall under this category. • “Specific Solution, Prototype, Answer, or Jud gment” covers many 
different types of results, including analysis of existing systems or 
specific analysis of a technique. • •“Empirical Report” covers papers that focus on a set of 
observations, either quantitative or qualitative, and could 
potentially incl ude a general prescription of guidelines based on 
the data in the report. The “Empirical Report” category includes 
the presentation of data or insights typically found in MSR papers. “Empirical Report” is a modification of the original classification scheme, which was called “Report”. The new name emphasizes 
the type of contribution that results of MSR papers.

3   Research Strategies 
Section 2 identifies the three important aspects of an individual research result as 
reported in a typical conference or journal paper. It is clear that the spectrum of good research strategies includes experimental computer science in the sense of [9,10,11, 12]; it is also clear that the spectrum is much broader than just experimental research. Of course, not all the combinations of question, result, and validation make sense.

How can one judgeit? It is very diﬃcult to give a satisfactory and deﬁnitiveanswer to such questions without the consensus of a greatpart of the community, which requires time and greateﬀort. A possible alternative is to summarize and organizethe written information currently available on the matter.And this is precisely the aim of this study. 2. Methodology
The present description of the major types of research
papers published in SE is based exclusively on existing writ-
ten sources, and in particular on the following:(1) Published, relevant papers, such as, for example,
Zelkowitz and Wallace (1998) orSjøberg et al. (2005) .

Use them consistently. The more formal or analytic the paper, the more important 
this is. Here are some questions that the program committee 
may ask about your paper: 
What, precisely, do you claim to contribute? Does your result fiJlly satisfy your claims? Are the 
definitions precise, and are terms used consistently? Authors tend to have trouble in some specific 
situations. Here are some examples, with advice for 
staying out of trouble:" 
• If your result ought to work on large systems, explain 
why you believe it scales. • If you claim your method is "automatic", using it 
should not require human intervention. If it's 
automatic when it's operating but requires manual 
assistance to configure, say so. If it's automatic 
except for certain eases, say so, and say how often 
the exceptions occur. • If you claim your result is "distributed", it probably 
should not have a single central controller or server. If it does, explain what part of it is distributed and 
what part is not. • If you're proposing a new notation for an old 
problem, explain why your notation is clearly 
superior to the old one. • If your paper is an "experience report", relating the 
use of a previously-reported tool or technique in a 
practical software project, be sure that you explain 
what idea the reader can take away from the paper to use in other settings. If that idea is increased 
confidence in the tool or technique, show how your 
experience should increase the reader's confidence 
for applications beyond the example of the paper. What ~ new here? The program committee wants to know what is novel 
or exciting, and why. What, specifically, is the 
contribution? What is the increment over earlier work by 
the same authors? by other authors? Is this a sufficient 
increment, given the usual standards of subdiscipline? Above all, the program committee also wants to know 
what you actually contributed to our store of knowledge 
about software engineering. Sure, you wrote this tool and 
tried it out. But was your contribution the technique that is 
embedded in the tool, or was it making a tool that's more 
effective than other tools that implement the technique, or 
was it showing that the tool you described in a previous 
paper actually worked on a practical large-scale problem? It's better for you as the author to explain than for the 
program committee to guess. Be clear about your claim ... Awful • • I completely and generally solved ... I (unless you actually did!) 
Bad • • I worked on galumphing. (or studied, investigated, sought, 
explored) 
Poor • • I worked on improving galumphing. (or contributed to, participated in, 
helped with) 
Good • • I showed the feasibility of composing 
blitzing with flitzing. i • I significantly improved the accuracy of 
the standard detector. (or proved, demonstrated, created, 
established, found, developed) 
Better • • I automated the production of ritz 
tables from specifications. • With a novel application of the blivet 
transform, I achieved a 10% increase 
in speed and a 15% improvement in 
coverage over the standard method. Use verbs that show results and achievement, not just 
effort and activity.

Examples 
Kitchenham et al. used the following inclusion criteria: 
• any study that compared predictions of cross-company models with within-
company models based on analysis of single company project data. They used the following exclusion criteria: 
• studies where projects were only collect ed from a small number of different sources 
(e.g. 2 or 3 companies),  
• studies where models derived from a with in-company data set were compared with 
predictions from a general cost estimation model. Jørgensen 206H[17] included papers that compare j udgment-based and model-based software 
development effort estimation. He also ex cluded one relevant paper due to “incomplete 
information about how the estimates were derived”. Issues: 
• Medical standards make a point that it is important to avoid, as far as possible, 
exclusions based on the language of th e primary study. This may not be so 
important for Software Engineering. • It is possible that inclus ion decisions could be affected by knowledge of the 
authors, institutions, journals or year of publication. Some medical researchers 
have suggested reviews should be done af ter such information has been removed. However, it takes time to do this and experimental evidence suggests that 
masking the origin of primary studies does not improve reviews 207H[4]. 6.2.2 Study selection process 
Study selection is a multistage process. Initially, selection criteria should be 
interpreted liberally, so that unless a study identifie d by the electronic and hand 
searches can be clearly excluded based on title and abstract, a full copy should be 
obtained. However, Brereton et al. 208H[5] point out that  “The standard of IT and software 
engineering abstracts is too poor to rely  on when selecting primary studies. You 
should also review the conclusions.” 
 
The next step is to apply inclusion/excl usion criteria based on practical issues 209H[11] 
such as: 
• Language 
• Journal 
• Authors 
• Setting 
• Participants or subjects 
• Research Design 
• Sampling method 
• Date of publication. Staples and Niazi point out th at it is sometimes necessary to consider the questions 
that are not being addressed in orde r to refine your exclusion criteria 210H[27]. Example 
Staples and Niazi’s research question was 

Question: What are the criteria for 'specific' results in academic papers?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m
[38;5;200m[1;3mcontent='```\n        Manuscript Text: The top 10 problems in API documentation are (i) incompleteness, (ii) ambiguity, (iii) unexplained examples, (iv) obsoleteness, (v) inconsistency, and (vi) incorrectness; while in presentation are (vii) bloat, (viii) fragmentation, (ix) excess structural information, and (x) tangled information.\n        Review: The list of the top 10 problems in API documentation provided in the section is specific and concrete. It clearly outlines the specific issues faced in API documentation related to both content and presentation, making it a concrete result in the context of software engineering research.\n```' response_metadata={'token_usage': {'completion_tokens': 140, 'prompt_tokens': 618, 'total_tokens': 758}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-fd0cff99-c125-46e3-838b-36ebc2aba688-0'[0m[32;1m[1;3mAction: Fetch Section Content by Title
Action Input: {"section_title": "III. E MPIRICAL S TUDY D ESIGN", "path": "/Users/crystalalice/Desktop/ICSHP_Research/SE_paper/Software_Documentation_Issues_Unveiled.pdf"}[0m[33;1m[1;3mOur goal is to answer the following research question (RQ):
What are the documentation issues faced by developers?
Authorized licensed use limited to: Access paid by The UC Irvine Libraries. Downloaded on February 08,2024 at 23:55:52 UTC from IEEE Xplore.  Restrictions apply.
1200
[0m[32;1m[1;3mAction: Generate Review
Action Input: Our goal is to answer the following research question (RQ): What are the documentation issues faced by developers?[0m

[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

5. WHAT TYPE OF CONTRIBUTION? In this section, we describe the types of contributions that researchers 
claim in their papers, and provide the reasoning for the classification 
scheme and how abstracts might fall into each code. While some of the 
categories remain t he same compared to the Shaw study, we made some 
modifications to account for changes in the research community over 
the intervening years. This subsection will also explain the classification 
procedure and provide the quantitative and qualitative results for the 
analysis of contributions to the research community. 5.1 What kinds of results do researchers 
generate? The types of new results/contributions that are derived from software 
engineering research can take on many forms. Shaw provided seven 
types of cont ributions, a s shown in Table 3 [24]. Table 1. Types of software engineering research questions, adapted from [24] (Changes in Bold)  
Type of questio n Examples  
Method or means of development  How can we do/create/modify/evolve (or automate doing) X? What is a better way to do/create/modify/evolve X? Method for analysis or evaluation  How can I evaluate the quality/correctness of X? How do I choose between X and Y? Design, evaluation, or analysis of a particular 
instance  How do we evaluate Y, and when is it determined to be “good”? What is  a property X of 
artifact/method Y? What is a (better) design, implementation, maintenance, or adaptation for application X? How does X compare to Y? What is the current state of X / practice of Y? Generalization or characterization  Given X, what will Y (necessarily) be? What, exactly, do we mean by X? What are its important characteristics? What is a good formal/empirical model for X? What are the varieties of X, how are they related? Feasibility study or exploration  Does X even exist, and if so what is it like? Is it possible to accomplish X at all?

The study emphasizes the need for further experimentation in the area. 

Of these, only 5 
papers were accepted. We used a logistic regression model to analyze 
whether collaboration impacts the acceptance of papers by controlling 
for the number of authors, number of unique affiliations, number of 
unique countries, and the number of previously published ICSE papers 
of the authors. The only statistically significant factor is the number of 
previously published ICSE papers by the authors (p -value = 0.0008). From the results presented in this section, we argue that  ICSE draws in 
research from a diverse community with sub missions coming from 44 
unique countries and 465 unique organizations. Furthermore, the 2016 
acceptance data shows no significant favorable bias towards PC and PB 
members. The only factor that increases a paper’s chances of acceptance 
is the number of pape rs published by the authors at ICSE previously. We 
hypothesize that senior researchers submit higher quality papers or 
papers that fit better at ICSE. Table 7. Submissions and acceptance rates for research question/contribution combinations  
Research Question  Procedure 
or 
technique  Qualitativ
e or 
descriptive 
model  Empirical 
model  Analytic 
model  Tool or 
notation  Specific 
solution  
/prototype  
/answer/  
judgment  Empirical 
Report  
Method for means of 
development  128 (16%)  10 (20%)  12 (25%)  29 (10%)  42 (18%)  2 (0%)  3 (33%)  
Method for analysis or 
evaluation  48 (23%)  3 (0%)  10 (0%)  11 (18%)  10 (50%)  3 (33%)  5 (40%)  
Evaluation or Analysis of 
instance  11 (18%)  2 (0%)  4 (50%)  9 (33%)  15 (7%)  7 (14%)  25 (44%)  
Generalization or 
characterization  2 (0%)  7 (29%)  2 (0%)  2 (0%)  5 (20%)  0 (0%)  69 (25%)  
Feasibility study or 
exploration  6 (17%)  0 (0%)  1 (0%)  3 (0%)  9 (11%)  2 (0%)  1 (0%)  
Table 8. Submissions and acceptance rates for research Contribution/validation combinations  
Submitted (% 
Accepted)  Analysis  Evaluation  Experience  Example  Underspecified  Persuasion  Blatant 
Assertion  
Procedure or technique  31 (26%)  70 (31%)  3 (33%)  32 (9%)  45 (9%)  8 (0%)  6 (0%)  
Qualitative or descriptive 
model  1 (0%)  6 (50%)  2 (0%)  2 (0%)  5 (0%)  3 (33%)  3 (0%)  
Empirical model  12 (25%)  10 (20%)  0 (0%)  2 (0%)  1 (0%)  2 (0%)  2 (0%)  
Analytic model  10 (30%)  8 (25%)  1 (0%)  7 (14%)  9 (22%)  11 (0%)  8 (0%)  
Tool or notation  6 (17%)  27 (30%)  4 (50%)  10 (0%)  25 (20%)  3 (0%)  8 (0%)  
Specific 
solution/prototype/  
answer/judgment  2 (50%)  5 (20%)  1 (0%)  2 (0%)  3 (0%)  0 (0%)  1 (0%)  
Report  10 (30%)  62 (44%)  8 (13%)  6 (0%)  6 (0%)  8 (0%)  3 (0%)  

and in development and extension (still more time is spent in basic research, but it is 
very difficult to identify the beginning of this phase). As a result, full understanding of research strategy must account for the accumulation of evidence over time as well as for the form and content of individual projects and papers. The IMPACT project [3] is tracing the path from research into practice. The objec-
tives of the project include identifying the kinds of contributions that have substantial impact and the types of research that are successful.

The number of research questions also plays a major role; it is important to restrict them to few. Another important influence factor is the number of stakeholders involved. The more stakeholders are involved, the more potential interest conflicts about research questions will arise, which increase the amount of communication involved. Thus, the number of stakeholders is a factor that may contribute exponentially to the total effort. Design : Time/effort increases basically with (a) the complexity of the conceptual 
model; (b) the number of contacts based on validity goals and anticipated response 
rate; and (c) the length of the questionnaire/interview (i.e., number and complexity of 
questions). The main influence factor for design is the complexity of the conceptual 
model, which is, in turn, influenced by the number and complexity of research 
questions. In our experience, the research questions contribute exponentially to the 
conceptual model; that is, even one additional question can increase the complexity of the conceptual model significantly. In our case, the conceptual model was quite 
complex, which resulted in a large and complex questionnaire (many conditional 
questions). This increased the design effort significantly. Therefore, the complexity of the conceptual model has an exponential influence on design effort. 

9. Is there evidence that the 
E/QE can be used by other researchers / practitioners? Do the authors discuss whether or how the findi ngs can be transferred to other populations, or 
consider other ways in whic h the research can be used? To what extent do authors interpret results in the context of other studies / the existing body of 
knowledge?

What was the result? • If the concept of similarity ha s been used in the internal re plications, for example, with 
regard to the subjects or the experimental un its, have the possible differences been tested 
for statistical significance? What was the result? Findings of the experiment • How can the result of the experiment be explained? • Does the result obtained contradict or support the results of earlier experiments? • What other experiments could be run on the ba sis of this one to further investigate the 
results yielded? 

In what way is it better? What real problem 
does it solve? Does it scale? If your contribution is principally the synthesis or 
integration of other results or components, be clear about 
why the synthesis is itself a contribution. What is novel, 
exciting, or nonobvious about the integration? Did you 
generalize prior results? Did you find a better 
representation? Did your research improve the individual 
results or components as well as integrating them? A 
paper that simply reports on using numerous elements 
together is not enough, even if it's well-engineered. There 
must be an idea or lesson or model that the reader can take 
from the paper and apply to some other situation. If your paper is chiefly a report on experience 
applying research results to a practical problem, say what 
the reader can learn from the experience. Are your 
conclusions strong and well-supported? Do you show 
comparative data and/or statistics? An anecdotal report on 
a single project is usually not enough. Also, if your report 
mixes additional innovation with validation through 
experience, avoid confusing your discussion of the 
innovation with your report on experience. After all, if 
you changed the result before you applied it, you're 
evaluating the changed result. And if you changed the 
result while you were applying it, you may have 
confounded the experiences with the two versions. If a tool plays a featured role in your paper, what is 
the role of the tool? Does it simply support the main 
contribution, or is the tool itself a principal contribution, 
or is some aspect of the tool's use or implementation the 
main point? Can a reader apply the idea without the tool? If the tool is a central part of result, what is the technical 
innovation embedded in the tool or its implementation? If a system implementation plays a featured role in 
your paper, what is the role of the implementation? Is the 
system sound? Does it do what you claim it does? What 
ideas does the system demonstrate? • If the implementation illustrates an architecture or 
design strategy, what does it reveal about the 
architecture? What was the design rationale? What 
were the design tradeoffs? What can the reader apply 
to a different implementation? • If the implementation demonstrates an 
implementation technique, how does it help the 
reader use the technique in another setting? • If the implementation demonstrates a capability or 
performance improvement, what concrete evidence 
does it offer to support the claim? • If the system is itself the result, in what way is it a 
contribution to knowledge? Does it, for example, 
show you can do something that no one has done 
before (especially if people doubted that this could 
be done)? 731 


It may be a complementary approach. However, it is important to be aware of the difference and also to consider how this affects the statistical analysis, si nce most statistical methods have developed 

Our thinking was that higher numbers of defects would correspond to students 
paying more attention to performing a serious review. However, instead it seemed 
to motivate students to disregard the process we wanted to study (or at least augment it with additional activities) in order to report as many potential defects as possible. Thus results were unrepresentative of what would have been observed in a normal application of the review technique. We have rectified this mistake on later experiments by grading based on process conformance (as assessed by the creation of intermediate artifacts) as well as on the soundness of the evaluation of the technology done by the subjects after the experiment. 6 Conclusions and Future Work 
In this paper, we have reported on a few empirical studies that we have carried out with students. Even though most of these studies were initially conceived for research reasons, we believe that it is important to keep in mind and study the impact of these studies on the students’ education. This paper is a preliminary report in which we have described our experiences and 
lessons learned, but a lot more work needs to be done. Here, we outline some possible 
research directions. Impact of empirical studies on other actors . In this paper, we have focused on the 
researchers’ and the students’ viewpoint s. However, other actors may play an 
important role in the process and be affected by it. For one thing, we have assumed throughout the paper that the researcher and the instructor are the same person. This is 
not necessarily the case, but even so, the same person is playing two different roles, 
with two different sets of goals and responsibilities. As a researcher, one needs to 
provide scientifically sound results to the research community and the industry; as an 
instructor, one needs to provide his or he r students with the best education possible 
for their future activities. The instructor’s goals may conflict with the researcher’s 
goals, so a sensible trade-off should be sought. By the same token, when carrying out 
an empirical study in an industrial envi ronment, one should always try to minimize 
the risk that the experimental activities disrupt the normal course of business. This 
minimization of risk can be seen in the examples presented in Sections 2.4 and 2.2. By running and debugging the experiment in the university classroom, researchers 
were able to have a positive experience when they moved it to and industrial setting. Costs vs.

Question: What does 'contribution' mean in the context of this research?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2   After 
developing a research question, you will need to consider whether it can be answered through the existing published literature or whether new data must be collected. Reviewing the litera-ture is only manageable if the question has clear boundaries. In research, we are often contribut-
ing a tiny step to the existing knowledge, rather than making huge leaps. Small contributions are better than no changes at all. So a long, speciﬁ  c question is 
likely to be preferable to a short vague question. To illustrate these points 
Table  1  provides some speciﬁ  c 
examples, and in the next section we discuss some key considerations. CONSIDERATIONS: 
RELEVANCE, ORIGINALITY AND RIGOUR 
 Here, we highlight some things 
to think about as you develop your research question, building on the points raised above. These analyses are not intended to be undertaken in any speciﬁ  c order. The considerations fall into three main categories (Figure  1 ), which 
can be thought of as broadly con-cerning the relevance, originality and rigour of the research ques-tion, and which are interrelated and partly inspired by the UK Research Excellence Framework. 3   
You may start with any one of the categories and revisit each one multiple times. In doing so, you may change your research question slightly or completely.

The study emphasizes the need for further experimentation in the area. 

We provide an overview of these perspectives
in Section 6. Typically, for technical scientific results, peer reviewers are often expected to
comment on the novelty of the work, the soundness and reliability of the work, and the pre-
sentation of the work. These expectations are summarized by the maxim “is it new, and is it
true?” (MacAuley 2012 ).

and in development and extension (still more time is spent in basic research, but it is 
very difficult to identify the beginning of this phase). As a result, full understanding of research strategy must account for the accumulation of evidence over time as well as for the form and content of individual projects and papers. The IMPACT project [3] is tracing the path from research into practice. The objec-
tives of the project include identifying the kinds of contributions that have substantial impact and the types of research that are successful.

The rest of this 
section addresses  scope and then tackles two frequently  asked questions  
about this work. Section  2 explores  historical  work on writing  ‘good’ ar- 
ticles and methodological  guidance,  coupled  with how other data-driven  
disciplines  have addressed  experimental  reliability  concerns. Next, in 
Section  3 we describe  our approach  to identifying  “bad smells ”. This is 
followed  by Section  4 that lists some key symptoms  or ‘smells’  in ap- 
proximate  order of importance. Finally,  in Section  5 we consider  the 
implications  for the future and how we, the research  community,  might 
collectively  take ownership  of these “smells  ”, re-prioritise,  re-order,  add 
and subtract  “smells  ”. 1.1. Scope 
Software  analytics  research  is relatively  new and growing  fast, so 
much of our advice relates to articles  discussing  induction  from soft- 
ware project data. We discuss problems  with empirical  data analysis  
via experiments,  quasi-experiments  and correlation  studies [91] rather 
than issues relating  to e.g., qualitative  studies. Other articles  should be 
consulted  for tips and traps relating  to qualitative  research  and experi- 
mentation  involving  human participants,  e.g., [25,60,84,102]  . Finally,  in order to delineate  the scope of this article we feel it helpful 
to add some remarks  concerning  the role of theory in software  analyt- 
ics type research. Naturally  much analytics  research  will be data-driven  
or inductive  in nature. In such circumstances  the role of theory or de- 
ductive  reasoning  will be secondary. This of course begs the question  of 
36 

&Selection strategy —where to seek data? The objective of the study may be, for example, exploratory, descriptive, explanatory, or
improving. The objective is naturally more generally formulated and less precise than infixed research designs. The objective is initially more like a focus point which evolvesduring the study. The research questions state what is needed to know in order to fulfill theobjective of the study. Similar to the objective, the research questions evolve during thestudy and are narrowed to specific research questions during the study iterations(Andersson and Runeson 2007b ). The case may in general be virtually anything which is a “contemporary phenomenon in
its real-life context ”(Yin 2003 ). In software engineering, the case may be a software
development project, which is the most straightforward choice. It may alternatively be an
individual, a group of people, a process, a product, a policy, a role in the organization, an
event, a technology, etc. The project, individual, group etc. may also constitute a unit ofanalysis within a case. In the information systems field, the case may be “individuals,
groups …or an entire organization. Alternatively, the unit of analysis may be a specific
project or decision ”(Benbasat et al. 1987 ). Studies on “toy programs ”or similarly are of
course excluded due to its lack of real-life context. Yin ( 2003 ) distinguishes between
holistic case studies , where the case is studied as a whole, and embedded case studies
where multiple units of analysis are studied within a case, see Fig. 1. Whether to define a
study consisting of two cases as holistic or embedded depends on what we define as thecontext and research goals. In our XP example, two projects are studied in two differentcompanies in two different application domains, both using agile practices (Karlström and
Runeson 2006 ). The projects may be considered two units of analysis in an embedded case
study if the context is software companies in general and the research goal is to study agilepractices. On the contrary, if the context is considered being the specific company orapplication domain, they have to be seen as two separate holistic cases. Benbasat et al.comment on a specific case study, “Even though this study appeared to be a single-case,
embedded unit analysis, it could be considered a multiple-case design, due to thecentralized nature of the sites. ”(Benbasat et al.

24 The Role of Experimentation in Scientific and Technological Research 
and so on. A technological investigation is, in this sense, more prescribed than a 
scientific investigation. It is also more limited, in that it may end when it has led 
to an adequate solution of a technical problem”. Anyone working in scientific and t echnical research accepts, as a working 
hypothesis, that the world is a cosmos not a chaos, that is, th ere are natural laws 
that can be comprehended and thought out. In the case of SE, researchers can be 
said to assume that precepts that desc ribe and prescribe the optimum means of 
building software can be discovered  and established. As Pfleeger (1999) said, 
the basis of all empirical software engineering is “if we look long enough and hard enough, we will find rational rules that  show us the best ways to build the 
best software”. For a body of knowledge to be considered scientific, its truth and validity must be proven.

How can one judgeit? It is very diﬃcult to give a satisfactory and deﬁnitiveanswer to such questions without the consensus of a greatpart of the community, which requires time and greateﬀort. A possible alternative is to summarize and organizethe written information currently available on the matter.And this is precisely the aim of this study. 2. Methodology
The present description of the major types of research
papers published in SE is based exclusively on existing writ-
ten sources, and in particular on the following:(1) Published, relevant papers, such as, for example,
Zelkowitz and Wallace (1998) orSjøberg et al. (2005) .

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

Are the case and its units of analysis well defined? 1, 5, 2841. Is the suitability of the case to address the research questions clearly motivated? 8, 9, 1442. Is the case study based on theory or linked to existing literature?

Question: How is 'precisely' interpreted in terms of research scope and focus?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

This document does  not go to this level of detail. The critical issue in any systematic review is  to ask the right question. In this context, 
the right question is usually one that: 
• Is meaningful and important to practitione rs as well as researchers. For example, 
researchers might be interested in whether a specific analysis technique leads to a 
significantly more accurate estimate of re maining defects after design inspections. However, a practitioner might want to know whether adopting a specific analysis 
technique to predict remaining defects is more effective than expert opinion at 
identifying design documents that require re-inspection. • Will lead either to changes in current software engineering practice or to 
increased confidence in the value of curre nt practice. For example, researchers 
and practitioners would like  to know under what conditi ons a project can safely 
adopt agile technologies and under what conditions it should not. • Will identify discrepancies between commonly held beliefs and reality. Nonetheless, there are systematic reviews that ask questions that are primarily of 
interest to researchers. Such  reviews ask questions that identify and/or scope future 
research activities. For example, a systema tic review in a PhD thesis should identify 
the existing basis for the research stud ent’s work and make it clear where the 
proposed research fits into the current body of knowledge. Examples 
 
Kitchenham et al. 190H[21] had three research questions: 
Question 1: What evidence is there that cross-company estimation models are not 
significantly different from within-company estimation models for predicting effort for 
software/Web projects? Question 2: What characteristics of the study data sets and the data analysis methods used in 
the study affect the outcome of within- a nd cross-company effort estimation accuracy 
studies? Question 3: Which experimental procedure is most appropriate for studies comparing within- 
and cross-company estimation models? Jørgensen 191H[17] had two research questions: 
1. Should we expect more accurate effort estimates when applying expert judgment or 
models? 2. When should software development effort estimates be based on expert judgment, 
when on models, and when on a combination of expert judgment and models? In both cases, the authors were aw are from previous research that  results were mixed, so in 
each case they added a question aimed at invest igating the conditions under which different 
results are obtained. 5.3.2 Question Structure 
Medical guidelines recommend considering a question about the effectiveness of a 
treatment from three viewpoints:  
• The population, i.e. the people affected by the intervention.

3. Impact Project. "Determining the impact of software engineering research upon practice. Panel summary, Proc. 23rd International Conference on Software Engineering (ICSE 2001) , 
2001  
4. William Newman. A preliminary analysis of the products of HCI research, using pro forma 
abstracts. Proc 1994 ACM SIGCHI Human Factors in Computer Systems Conference (CHI 
'94), pp.278-284. 5. Samuel Redwine, et al. DoD Related Software Technology Requirements, Practices, and 
Prospects for the Future . IDA Paper P-1788, June 1984. 6. S. Redwine & W. Riddle. Software technology maturation. Proceedings of the Eighth Inter-
national Conference on Software Engineering,  May 1985, pp. 189-200.

 33% (38) 
 8% (9)  65% (74) 
 36% (41) 
 25% (29) 
 12% (14) 
 14% (16) 
 4% (4)  25% (15) 
 7% (4)  74% (45) 
 52% (32) 
 26% (16) 
 13% (8) 
 8% (5) 
 5% (3) Award
NoAward
OtherI carefully calibrate my epistemological
perspective with the authorsI do not validate these artifacts.I do not review these types of papersI demand and/or access data/ scripts and
experimental packageI attempt to run the source code/
container etc. on my machineI check for data consistencyI check for data availability
0% 20% 40% 60%
Number of respondents using that approach
Fig.8 Validate artifacts and claims
look at the chain of evidence from observation to conclusion and consider whether it is clear
and unbroken. I consider different quality criteria and threats to validity depending on the
type of study and its (often implicit) philosophical position.”
Characteristics of impactful papers Figure 9shows the distribution of how respondents
judge the impact of papers. It is interesting to observe that the top three characteristics are
novelty ,relevance ,a n d methodological rigor , which can sometimes be in conflict with each
other.

2. What, precisely, was your contribution? Before reporting what you did, explain what problem 
you set out to solve or what question you set out to answer 
--and why this is important. 2.1 What kinds of questions do software 
engineers investigate? Generally speaking, software engineering researchers 
seek better ways to develop and evaluate sottware. Devel- 
opment includes all the synthetic activities that involve 
creating and modifying the software, including the code, 
design documents, documentation, etc. Evaluation includes all the analytic activities associated with predict- 
ing, determining, and estimating properties of the software 
systems, including both functionality and extra-functional 
properties such as performance or reliability. Software engineering research answers questions about 
methods of development or analysis, about details of 
designing or evaluating a particular instance, about gener- 
alizations over whole classes of systems or techniques, or 
about exploratory issues concerning existence or feasibil- 
ity. Table 1 lists the types of research questions that are 
asked by software engineering research papers and 
provides specific question templates. Table 1. Types of software engineering research questions 
Type of question Examples 
Method or means of How can we do/create/modify/evolve (or automate doing) X? development What is a better way to do/create/modify/evolve X?

Basics of Software Engi neeri ng Experi ment ation 291 
  
Solution : The effect of experience and of the interaction 
is significant, whereas the effect of the technique is not; 
Technique I used by an experienced developer  
10.7.3. The effect of two modelling techni ques (-A, +A), used by experienced and 
inexperienced people (-B, +B), working in two different domains (-C, +C), 
on small-sized problems (-D, +D) is to be examined. Table 10.33 contains 
a measure of the effort put into gi ven development projects with these 
characteristics. What factors and interactions have significant effects?

Open reviews (e.g., OpenReviews.net) are post-publication reviews in
which the reviewer’s identity is usually known to the author. The International Conference
on Software Engineering conducts post-mortems of each year’s review process. Although
primarily focusing on the mechanism for deciding on paper acceptance, these reports also
explain types of papers accepted and what review information was important in making a
decision16. A recent series by Jacopo Soldani and colleagues in ACM SIGSOFT’s Software
Engineering Notes (Soldani et al. 2020 ) summarizes the “pains and gains” of peer review
and provides discussion about the concerns many scientists have with peer review, such as
workload. 6.2 PeerReviewCriteria
It is common for a peer review request to include some criteria by which the paper is to
be judged. These criteria are particularly important when the paper is a non-standard (i.e.,
non-technical research) format. For example, the Empirical Software Engineering Journal
allows for Technical Papers and Engineering in Practice papers. The Journal of Systems
and Software accepts technical papers, New Ideas and Trends, and In Practice papers. The
International Conference on Software Engineering (ICSE) has numerous tracks, including
Software in Society, Software Engineering in Practice, the main technical track, New Ideas
and Emerging Results, Software Engineering Education and Training, and more. Most conferences have the same or similar reviewing criteria for their research tracks,
typically summarized along the call for papers. The ICSE technical review criteria, for
instance, are available on the conference call for papers. For the 2021 edition, the criteria17
are
– Soundness: The extent to which the paper’s contributions are supported by rigorous
application of appropriate research methods. – Significance: The extent to which the paper’s contributions are important with respect
to open software engineering challenges. – Novelty: The extent to which the contribution is sufficiently original and is clearly
explained with respect to the state-of-the-art. – Verifiability: The extent to which the paper includes sufficient information to support
independent verification or replication of the paper’s claimed contributions. – Presentation: The extent to which the paper’s quality of writing meets the high standards
of ICSE [...]. Common to most review guidelines is a focus on presentation and clarity of the paper,
adherence to the conference or journal topics, and length. Increasingly guidelines are also
emphasizing the importance of constructive criticism (the Reviewer 2 problem (Peterson
16http://www.icse-conferences.org/reports.html
17https://conf.researchr.org/track/icse-2021/icse-2021-papers#Call-for-PapersEmpir Software Eng (2021) 26:  103 103Page26of29

Summary of previous reviews. Description of the software engineering technique being investigated and 
its potential importance. Review questions  Each review question should be 
specified.

56 IEEE SOFTWARE  |  WWW.COMPUTER.ORG/SOFTWARE   |  @IEEESOFTWARE
FOCUS:  SOFTWARE ENGINEERING’S 50TH ANNIVERSARYAs an engineering discipline, SE 
research should strive to impact 
practice. Favoring certain types of 
evidence over others will not suf -
fice. Instead, we require a frame -
work for aggregating the results of multiple pieces of work with differ -
ent types of evidence into action -
able practical feedback. In addition to encouraging technology trans -
fer and true research impact, such a framework can simultaneously open our field to accepting a wider variety of research, including re -
sults that constitute the less rigor -
ous (but still important!) codified knowledge that engineers use in practice. ABOUT THE AUTHORS
CLAIRE LE GOUES  is an assistant 
professor of computer science at Carnegie 
Mellon University’s Institute for Software 
Research. Her research interests lie in 
automatically reasoning about and improv -
ing software quality in real-world, evolving 
systems. Le Goues received a PhD in 
computer science from the University of 
Virginia. Contact her at clegoues@cs.cmu  
.edu, @clegoues. MARY SHAW is the Alan J. Perlis Uni -
versity Professor of Computer Science at Carnegie Mellon University’s Institute for 
Software Research. Her research focuses 
on software engineering and software de
-
sign, particularly software architecture and the design of systems used by real people. Shaw received a PhD in computer science 
from Carnegie Mellon University. She’s a 
Life Fellow of ACM and IEEE. Contact her at 
mary.shaw@cs.cmu.edu. CIERA JASPAN is a senior software engi -
neer at Google. She leads the Engineering Productivity Research team, which aims to 
identify inefficiencies in development tools 
and processes and improve the productiv
-
ity of Google engineers and engineers who use Google products. Jaspan received her 
PhD in software engineering from Carnegie 
Mellon University. Contact her at ciera@
google.com.

[19] R. Kwok, "Research impact: Altmetrics make their mark," 
Nature, vol. 500, pp. 491 -493, 2013. [20] C. Marshall, P. Brereton, and B. Kitchenham, "Tools to 
support systematic reviews in software engineering: a cross -domain 
survey using sem i-structured interviews," presented at the Proceedings 
of the 19th International Conference on Evaluation and Assessment in 
Software Engineering, Nanjing, China, 2015. [21] P. Runeson and M. Höst, "Guidelines for conducting and 
reporting case study research in software engineering," Empirical 
Software Engineering, vol. 14, pp. 131 -164, 2008. [22] A. Sampaio, "Improving Systematic Mapping Reviews," 
SIGSOFT Softw. Eng. Notes, vol. 40, pp. 1 -8, 2015. [23] M. Shaw, "What makes good research in software 
engineering?," J Software Tools for Technology Transfer STTT, vol. 4, 
pp. 1 -7, 2002. [24] M. Shaw, "Writing Good Software Engineering Research 
Papers," Proceedings of 25th Internat ional Conference on Software 
Engineering (ICSE'03), pp. 726 -736, 2003. [25] J. Singer and N. G. Vinson, "Ethical issues in empirical 
studies of software engineering," IEEE Transactions on Software 
Engineering, vol. 28, pp.

• Why should the reader care? • What larger question does this address? • What is your new result? • What new knowledge have you contributed that 
the reader can use elsewhere? • What previous work (yours or someone else's) 
do you build on? What do you provide a superior 
alternative to? • How is your result different from and better than 
this prior work? • What, precisely and in detail, is your new result? • Why should the reader believe your result? • What standard should be used to evaluate your 
claim? • What concrete evidence shows that your result 
satisfies your claim? If you answer these questions clearly, you'll probably 
communicate your result well. If in addition your result 
represents an interesting, sound, and significant contribu- 
tion to our knowledge of software engineering, you'll 
have a good chance of getting it accepted for publication 
in a conference or journal. Other fields of science and engineering have well- 
established research paradigms. For example, the 
experimental model of physics and the double-blind 
studies of medicines are understood, at least in broad 
outline, not only by the research community but also by 
the public at large. In addition to providing guidance for 
the design of research in a discipline, these paradigms 
establish the scope of scientific disciplines through a 
social and political process of "boundary setting" [5]. Software engineering, however, has not yet developed 
this sort of well-understood guidance. I previously [19, 
20] discussed early steps toward such understanding, 
including a model of the way software engineering 
techniques mature [17, 18] and critiques of the lack of 
rigor in experimental software engineering [1, 22, 23, 24, 
25]. Those discussions critique software engineering 
research reports against the standards of classical 
paradigms. The discussion here differs from those in that 
this discussion reports on the types of papers that are 
accepted in practices as good research reports. Another 
current activity, the Impact Project [7] seeks to trace the 
influence of software engineering research on practice. The discussion here focuses on the paradigms rather than 
the content of the research 
This report examines how software engineers answer 
the questions above, with emphasis on the design of the 
research project and the organization of the report. Other 
sources (e.g., [4]) deal with specific issues of technical 
writing. Very concretely, the examples here come from 
the papers submitted to ICSE 2002 and the program 
committee review of those papers. These examples report 
research results in software engineering. Conferences 
often include other kinds of papers, including experience 
reports, materials on software engineering education, and 
opinion essays. 0-7695-1877-X/03 $17.00 © 2003 IEEE 726 


Question: What criteria determine whether a research question is impactful or significant in Software Engineering?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

What is your new result? Explain precisely what you have contributed to the 
store of sot~ware engineering knowledge and how this is 
useful beyond your own project. oOOio -i, i 
80*/0 
60% 
40% 
20% -I 
Iii Dewl Question 
i:G i:~ii i , 
Analy Eval Gener Feas Total 
• Accepted [] Rejected 
Figure 2. Distribution of acceptances and rejections 
by type of research question 
3.1 What kinds of results do software engineers 
produce? The tangible contributions of software engineering 
research may be procedures or techniques for develop- 
ment or analysis; they may be models that generalize from 
specific examples, or they may be specific tools, solutions, 
or results about particular systems. Table 3 lists the types 
of research results that are reported in software engineer- 
ing research papers and provides specific examples. 3.2 Which of these are most common? By far the most common kind of ICSE paper reports a 
new procedure or technique for development or analysis. Models of various degrees of precision and formality were 
also common, with better success rates for quantitative 
than for qualitative models. Tools and notations were well 
represented, usually as auxiliary results in combination 
with a procedure or technique. Table 4 gives the distribu- 
tion of submissions to ICSE 2002, based on reading the 
abstracts (but not the papers), followed by graphs of the 
counts and distributions in Figures 3 and 4. 728 


Of course, one must have the resources to do research this way. 3 Realistic Tasks 
When conducting controlled experiments in software engineering, one should 
consider the realism and representativeness of the tasks regarding the size, complexity and duration of the involved tasks. Specifi cation, implementation and verification 
methods also vary considerably between domains, such as accounting software versus flight-control systems. In our opinion, some experimental tasks bear little resemblance to actual tasks in software engineering; others are very similar to actual tasks [32]. In between there is a continuum. Larger development tasks may take 
months, while many maintenance tasks may take only a couple of hours. 

103–112. Dyba, T., Kitchenham, B.A., Jorgensen, M., 2005. Evidence-based
software engineering for practitioners. IEEE Softw. 22 (1), 58–65. Fenton, N., 1993. How eﬀective are software engineering methods? J. Syst. Softw. 22 (2), 141–146. Fletcher, P., 1995. The role of experiments in computer science. J. Syst. Softw. 30 (1–2), 161–163. Glass, R.L., 1995. Structure-based critique of contemporary computing
research. J. Syst. Softw. 28 (1), 3–7. Glass, R.L., Vessey, I., Ramesh, V., 2002. Research in software engineer-
ing: an analysis of the literature. Information Softw. Technol. 44 (8),491–506. Glass, R.L., Ramesh, V., Vessey, I., 2004. An analysis of research in
computing disciplines. Commun. ACM 47 (6), 89–94. Goldberg, A., 1982. Editorial policy. ACM Comput. Surveys 14 (2), 151–
157. Haddow, G., Klobas, J., 2004. Communication of research to practice in
library and information science: closing the gap. Library Information
Sci.

•They interact with many other development 
techniques and procedures. For example a design method depends on a preceding requirements analysis. It must consider constraints imposed by the software and hardware platform and programming languages, timescales, and budget. It must be integrated with appropriate coding and testing techniques. Thus, it would be difficult to confirm that a design technique had a significant impact on final product reliability. In general, it is difficult to determine a causal link between a particular technique and a desired project outcome when the application of the technique and the final outcome are temporally removed from one another, and there are many other tasks and activities that could also affect the final outcome. •The immediate outcomes of a software engineering 
technique will not necessarily have a strong relationship with final project outcomes. E.g. if you 
are interested in the effect design techniques have on 
application reliability (i.e. probability of failure in a given time period under defined operational conditions), measures of the design product (or 
design process) have no obvious relationship with 
the desired outcome. There are no good surrogate 
measures of product reliability that can be measured 
at the end of the design process. There seem to be two major approaches to this issue: 1.

• What kinds of evidence can demonstrate the validity of a result, and how are 
good results distinguished from bad ones? In this paper I attempt to make generally accepted research strategies in software en-
gineering explicit by examining research in the area to identify what is widely ac-cepted in practice. 1.1   Software Technology Maturation 
Redwine and Riddle [5,6] reviewed a number of software technologies to see how 
they develop and propagate. They found that it typically takes 15-20 years for a tech-nology to evolve from concept formulation to the point where it's ready for populari-zation. They identify six typical phases: 
• Basic research . Investigate basic ideas and concepts, put initial structure on 
the problem, frame critical research questions. • Concept formulation . Circulate ideas informally, develop a research commu-
nity, converge on a compatible set of ideas, publish solutions to specific sub-problems. • Development and extension . Make preliminary use of the technology, clarify 
underlying ideas, generalize the approach. • Internal enhancement and exploration . Extend approach to another domain, 
use technology for real problems, stabilize technology, develop training mate-rials, show value in results. • External enhancement and exploration . Similar to internal, but involving a 
broader community of people who weren’t developers, show substantial evi-dence of value and applicability. • Popularization .

  12 outcomes should be specified. For example, in some cases we require interventions 
that improve some aspect of software  production without affecting another e.g. improved reliability with no increase in cost. A particular problem for soft ware engineering experiments is the widespread use of 
surrogate measures for example, defects found during system testing as a surrogate 
for quality, or coupling measures for de sign quality. Studies that use surrogate 
measures may be misleading and conclusi ons based on such studies may be less 
robust. Context 
For Software Engineering, this is the c ontext in which the co mparison takes place 
(e.g. academia or industry), the participants taking part in  the study (e.g.

have criticized common practice in the field for failing to collect, analyze, and report 
experimental measurements in research reports [9,10,11,12]. In 2001 I [8] presented preliminary sketches of some of the successful paradigms for software engineering research, drawing heavily on examples from software architecture. Scientific and engineering research fields can be characterized by identifying what 
they value: 
• What kinds of questions are "interesting"? • What kinds of results help to answer these questions, and what research 
methods can produce these results?

2   Questions, Results, and Validation in Software Engineering 
Generally speaking, software engineering researchers seek better ways to develop and 
evaluate software. They are motivated by practical problems, and key objectives of the research are often quality, cost, and timeliness of software products. This section presents a model that explains software engineering research papers by 
classifying the types of research questions they ask, the types of results they produce, 
and the character of the validation they provide. This model has evolved over several 
years.

4  Introduct ion 
  Confronted by a confusing array of op tions for producing software, software 
engineers need proof that a particular approach or technique is really better than 
another. They need to know the clear-cut be nefits of one approach versus another. They need reliable evidence that one appr oach clearly works better than another. This need to work with facts rather than assumptions is a property of any 
engineering discipline. Very few ideas in SE are matched with empirical data. Ideas, whose truthfulness has not been tested against reality, are continually assumed as evident. For example, the 
famous software crisis (Naur, 1969) was and still is more a question of a subjective 
customer and/or developer impression than a confirmed phenomenon. In fact, we can find some claims that there is no software crisis (see comments about Turski and 
Hoare opinions given in (Maibaum, 1997)). Another example is the idea that traditional engineering disciplines behave  much better than SE (Pezeé, 1997). For 
example, has any empirical study been pe rformed to compare the failure rate of 
products obtained by SE and those produced by other engineering disciplines? Table 
1.1 presents the percentage of faults in the car industry shown in a study carried out by Lex Vehicle Leasing, one of the major world vehicle hire and leasing companies, 
on a universe of 73,700 cars in England. This fault rate could call the reliability of 
the engineering used in this industry into question; however, this is not the case. Therefore, if we are really to  speak of a software crisis, we  need studies to show that 
SE is less reliable than other branches of engineering, that is, the number of faults in SE should be checked against the number of faults in other branches of engineering. Other examples of computer science theo ries that have not been tested are 
functional programming, object-oriented programming or formal methods.

 SEPTEMBER/OCTOBER 2018  |  IEEE SOFTWARE  51
imagining a dialogue between 
researchers and practitioners (see 
the sidebar). This pattern is common: engi -
neers often rely on their experience 
and a priori beliefs1 or turn to co -
workers for advice. This is better than guessing or giving up. But what 
if incompletely validated research outcomes could be distilled into re -
liable sources, intermediate between 
validated results and folk wisdom? To impact practice, SE research 
results must lead to pragmatic, ac -
tionable advice. This involves syn -
thesizing recommendations from results with different assumptions 
and levels of rigor, assigning appro -
priate levels of confidence to the rec -
ommendations. Here, we examine how these tradeoffs between rigor 
and pragmatism have been handled 
in medicine, where risk is often ac -
ceptable in the face of urgency. We propose an approach to describ -
ing SE research results with varied quality of evidence and synthesizing those results into codified knowledge 
for practitioners. This approach can 
both improve practice and increase the pace of research, especially in ex -
ploratory topics. Software Engineering 
Research Expectations over Time
When the 1968 NATO Conference 
introduced “software engineering” 
to our vocabulary,2 research often 
focused on designing and building programs. There were guidelines for writing programs; the concept of rea -
soning mathematically about a pro -
gram had just been introduced. The 
emphasis was on demonstrated capa -
bility—what we might now call feasi -
bility—rather than rigorous validation. This is visible in a sampling of 
major results of the period. For ex -
ample, Carnegie Mellon University identified a set of canonical papers published between 1968 and 2002. 3 
Several are formal analyses or em -
pirical studies, and a few are case studies. However, the majority are carefully reasoned essays that pro -
pose new approaches based on the authors’ experience and insight. The field has historically built 
on results with varying degrees of certainty. Indeed, Fred Brooks pro -
posed a “certainty-shell” structure RESEARCHERS AND PRACTITIONERS 
VOLLEY ABOUT MAKING RESEARCH USEFUL
Dear Practitioners:
The research community is actually discovering things you might find useful. Please help us organize this knowledge so that 
it’s actually useful to you.

Question: How do we define 'concrete' results in software engineering research?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

objectives, (b) demonstration of novelty and superiority
compared to related work, (c) evidence of eﬀectiveness,reproducibility, scalability, and practical relevance.Reviewers should also ascertain if papers meet quality cri-teria such as conciseness and technical precision, the con-sistent use of terminology, and the deﬁnition of keyconcepts. Focus and limitation of length must often beevaluated together, as the ACM T Database Syst R sug-
gests: ‘‘TODS would like to discourage excessively longpapers (longer than 50 doubled-spaced pages including ﬁg-ures, references, etc.) and unnecessary digressions even inshorter papers. This should help authors to focus on themost important aspects of their submission [ ...]”. For the
rest, practically the same quality criteria as for authorsare brought up, such as clarity, readability and the abilityto target as broad an audience as possible. Studying the abstracts of papers published at ICSE
2002, Shaw (2003) describes how accepted research papers
answered questions such as: What was your contribution?What is your new result? How is your result diﬀerent fromand better than prior work?, etc. According to Shaw, whenit comes to evaluating submitted papers, programme com-mittees look for interesting, novel, and exciting results. Inaddition, as SE knowledge grows incrementally, like in
other areas of science and engineering, programme com-
mittees are very interested in the authors’ interpretationof prior work. 3.2. Extended versions of conference papersA special type of articles are those based on one or
more papers previously presented at a workshop, a sym-
posium or a conference. For instance, van Vliet (2006)
is an extended version of van Vliet (2005) . In the study
previously mentioned ( Montesi and Mackenzie-Owen,
2008), comparing article genres in SE, education and
biology, it was found that extended versions of confer-ence papers played a role only in SE. A distinguishing
feature of extended papers is the double round of
reviews they need to pass before journal publication(i.e. conference and journal reviews). In a survey withauthors of extended versions and journal editors, Mon-
tesi and Mackenzie-Owen (accepted for publication)found that the extension process may change dependingon the type of conference paper to extend (empirical,theoretical, ...). Some journal instructions to authors regulate the pub-
lication of these articles. Among the other conditions, onethird of the content approximately must be new and ori-ginal, and sometimes authors must submit both versions,a description of the diﬀerences between the submittedmanuscript and the preliminary version, and referencethe previous version in the paper (SoSyM). Papers pub-lished as such must be ‘‘important ”(Theor Pract Log
Prog), ‘‘outstanding ”(ACM T Database Syst ), and ‘‘of
particularly high quality ”(J Comput Sci Technol ). New
material takes in explorations of further research, theo-rems, proofs and/or implementation details ( ACM T
Graphic ). If journals address reviewers the general tone doesn’t
change, though they oﬀer some more clues on what to lookfor. In the ACM T Database Syst -R , for instance, extended
versions of conference papers ‘‘should thoroughly consoli-date the material, should extend it to be broader, andshould more carefully cover related research ”. In addition,
the novel approach should be described more deeply, andalternatives should also be considered more comprehen-sively. In IEEE T Vis Comput Gr -R , while no new results
are required, key ideas, examples, and further elaborationare expected. This conﬁrms Shaw’s observation that in con-
ferences time and space limitations do not allow authors to
explore all the possible results of their research ( Shaw,
2003).Table 2
Excerpt from the instructions of IEE Proc-Softw
Originality: is the work scientiﬁcally rigorous, accurate and novel? Does the work contain signiﬁcant additional material to that already published?

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

• Papers not subject to peer-review. When an SLR has been published in more than one journal/conference, the most 
complete version of the survey will be used. Primary study selection process 
 
The results will be tabulated as follows: 
• Number of papers per year per source 
• Number of candidate pape rs per year per source 
• Number of selected papers per year per source. The relevant candidate and selected studies will be selected by a single researcher. The rejected studies will be checked by a nother researcher. We will maintain a list 
candidate papers that were rejected  with reasons for the rejection. Quality Assessment 
 Each SLR will be evaluated using the York University, Centre for Reviews and Dissemination (CDR) Database of Abstracts of Reviews of Effects (DARE) criteria 
(http://www.york.ac.uk/inst/crd/crddatabase .htm#DARE). The criteria are based on 
four questions: 
 
• Are the review’s inclusion and exclusi on criteria describe d and appropriate? • Is the literature search likely to have covered all relevant studies? • Did the reviewers assess the quality/v alidity of the included studies? • Were the basic data/studies adequately described? The questions are scored as follows:  
• Question 1: Y (yes), the inclusion criteria  are explicitly defined in the paper, P 
(Partly), the inclusion criteria are implic it; N (no), the inclusion criteria are not 
defined and cannot be readily inferred. • Question 2: Y, the authors have either s earched 4 or more digital libraries and 
included additional search strategies or identified and referenced all journals 
addressing the topic of interest; P, the authors have searched 3 or 4 digital 
libraries with no extra search strategies, or searched a define d but restricted set 
of journals and conference proceedings;  N, the authors have search up to 2 
digital libraries or an extremel y restricted set of journals. • Question 3: Y, the authors have explicit ly defined quality criteria and extracted 
them from each primary study; P, the re search question involves quality issues 
that are addressed by the study; N no e xplicit quality assessm ent of individual 
papers has been attempted. 

Has its value been demonstrated? Relevance: is the material appropriate to the scope of the journal to which it is submitted? Motivation: does the problem considered have a sound motivation? Does the paper clearly demonstrate the scientiﬁc interest of the results? Referencing: has reference been made to the most recent and most appropriate work? Is the present work set in the context of the previous work? Clarity: is the English clear and well written? Poorly written English may obscure the scientiﬁc merit of your paper. Are the ideas expressed clearly
and concisely?

Characteristicsofhighqualitypapers Table 4shows the most common results from our
coding of the free-text responses to this question. Overall, the most common characteris-
tics reviewers want in a high quality paper relate to aspects of the quality of the research
methodology and how well it is presented in the paper. It is also interesting to note which
characteristics were not among the most commonly given answers, including the availability
of the data (5), the findings themselves (1), and the generalizability of the results (1). Characteristicsleadingtonegativereviews Figure 11shows the ranking of the charac-
teristics that tend to lead to negative reviews. The highest ranked characteristics include (1)
a mismatch of methods with claims raised in the contribution, (2) overly grandiose claims,
(3) a writing style that is hard to follow, (4) a research methodology which is difficultEmpir Software Eng (2021) 26:  103 103Page14of29

Tools that originated from a new technique also described in the 
paper would fall under this category. • “Specific Solution, Prototype, Answer, or Jud gment” covers many 
different types of results, including analysis of existing systems or 
specific analysis of a technique. • •“Empirical Report” covers papers that focus on a set of 
observations, either quantitative or qualitative, and could 
potentially incl ude a general prescription of guidelines based on 
the data in the report. The “Empirical Report” category includes 
the presentation of data or insights typically found in MSR papers. “Empirical Report” is a modification of the original classification scheme, which was called “Report”. The new name emphasizes 
the type of contribution that results of MSR papers.

3   Research Strategies 
Section 2 identifies the three important aspects of an individual research result as 
reported in a typical conference or journal paper. It is clear that the spectrum of good research strategies includes experimental computer science in the sense of [9,10,11, 12]; it is also clear that the spectrum is much broader than just experimental research. Of course, not all the combinations of question, result, and validation make sense.

How can one judgeit? It is very diﬃcult to give a satisfactory and deﬁnitiveanswer to such questions without the consensus of a greatpart of the community, which requires time and greateﬀort. A possible alternative is to summarize and organizethe written information currently available on the matter.And this is precisely the aim of this study. 2. Methodology
The present description of the major types of research
papers published in SE is based exclusively on existing writ-
ten sources, and in particular on the following:(1) Published, relevant papers, such as, for example,
Zelkowitz and Wallace (1998) orSjøberg et al. (2005) .

Use them consistently. The more formal or analytic the paper, the more important 
this is. Here are some questions that the program committee 
may ask about your paper: 
What, precisely, do you claim to contribute? Does your result fiJlly satisfy your claims? Are the 
definitions precise, and are terms used consistently? Authors tend to have trouble in some specific 
situations. Here are some examples, with advice for 
staying out of trouble:" 
• If your result ought to work on large systems, explain 
why you believe it scales. • If you claim your method is "automatic", using it 
should not require human intervention. If it's 
automatic when it's operating but requires manual 
assistance to configure, say so. If it's automatic 
except for certain eases, say so, and say how often 
the exceptions occur. • If you claim your result is "distributed", it probably 
should not have a single central controller or server. If it does, explain what part of it is distributed and 
what part is not. • If you're proposing a new notation for an old 
problem, explain why your notation is clearly 
superior to the old one. • If your paper is an "experience report", relating the 
use of a previously-reported tool or technique in a 
practical software project, be sure that you explain 
what idea the reader can take away from the paper to use in other settings. If that idea is increased 
confidence in the tool or technique, show how your 
experience should increase the reader's confidence 
for applications beyond the example of the paper. What ~ new here? The program committee wants to know what is novel 
or exciting, and why. What, specifically, is the 
contribution? What is the increment over earlier work by 
the same authors? by other authors? Is this a sufficient 
increment, given the usual standards of subdiscipline? Above all, the program committee also wants to know 
what you actually contributed to our store of knowledge 
about software engineering. Sure, you wrote this tool and 
tried it out. But was your contribution the technique that is 
embedded in the tool, or was it making a tool that's more 
effective than other tools that implement the technique, or 
was it showing that the tool you described in a previous 
paper actually worked on a practical large-scale problem? It's better for you as the author to explain than for the 
program committee to guess. Be clear about your claim ... Awful • • I completely and generally solved ... I (unless you actually did!) 
Bad • • I worked on galumphing. (or studied, investigated, sought, 
explored) 
Poor • • I worked on improving galumphing. (or contributed to, participated in, 
helped with) 
Good • • I showed the feasibility of composing 
blitzing with flitzing. i • I significantly improved the accuracy of 
the standard detector. (or proved, demonstrated, created, 
established, found, developed) 
Better • • I automated the production of ritz 
tables from specifications. • With a novel application of the blivet 
transform, I achieved a 10% increase 
in speed and a 15% improvement in 
coverage over the standard method. Use verbs that show results and achievement, not just 
effort and activity.

Examples 
Kitchenham et al. used the following inclusion criteria: 
• any study that compared predictions of cross-company models with within-
company models based on analysis of single company project data. They used the following exclusion criteria: 
• studies where projects were only collect ed from a small number of different sources 
(e.g. 2 or 3 companies),  
• studies where models derived from a with in-company data set were compared with 
predictions from a general cost estimation model. Jørgensen 206H[17] included papers that compare j udgment-based and model-based software 
development effort estimation. He also ex cluded one relevant paper due to “incomplete 
information about how the estimates were derived”. Issues: 
• Medical standards make a point that it is important to avoid, as far as possible, 
exclusions based on the language of th e primary study. This may not be so 
important for Software Engineering. • It is possible that inclus ion decisions could be affected by knowledge of the 
authors, institutions, journals or year of publication. Some medical researchers 
have suggested reviews should be done af ter such information has been removed. However, it takes time to do this and experimental evidence suggests that 
masking the origin of primary studies does not improve reviews 207H[4]. 6.2.2 Study selection process 
Study selection is a multistage process. Initially, selection criteria should be 
interpreted liberally, so that unless a study identifie d by the electronic and hand 
searches can be clearly excluded based on title and abstract, a full copy should be 
obtained. However, Brereton et al. 208H[5] point out that  “The standard of IT and software 
engineering abstracts is too poor to rely  on when selecting primary studies. You 
should also review the conclusions.” 
 
The next step is to apply inclusion/excl usion criteria based on practical issues 209H[11] 
such as: 
• Language 
• Journal 
• Authors 
• Setting 
• Participants or subjects 
• Research Design 
• Sampling method 
• Date of publication. Staples and Niazi point out th at it is sometimes necessary to consider the questions 
that are not being addressed in orde r to refine your exclusion criteria 210H[27]. Example 
Staples and Niazi’s research question was 

Question: What are the criteria for 'specific' results in academic papers?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m
[38;5;200m[1;3mcontent='```\nManuscript Text: Our goal is to answer the following research question (RQ): What are the documentation issues faced by developers?\nReview: \n1. In the research question, the author claims to contribute to identifying and analyzing the documentation issues faced by developers in software engineering.\n2. The reader should care about this research question because understanding the documentation issues faced by developers is crucial for improving software development processes, enhancing collaboration, and ultimately delivering high-quality software products.\n3. The result is specific as it focuses on a particular aspect of software development (documentation issues) and aims to provide insights and potential solutions in this area.\n```' response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 920, 'total_tokens': 1048}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-5719bc1a-4b87-4673-ac4a-9adf0d76c962-0'[0m[32;1m[1;3mAction: Fetch Section Content by Title
Action Input: {"section_title": "A. Data Collection", "path": "/Users/crystalalice/Desktop/ICSHP_Research/SE_paper/Software_Documentation_Issues_Unveiled.pdf"}[0m[33;1m[1;3mOur data collection consists of two steps. First, we adopt
an automatic process based on keyword matching to mine
candidate artifacts related to documentation from the four
analyzed sources (i.e., emails, issues and pull requests of
open source projects, and Stack Overﬂow threads). Then, we
manually analyze a statistically signiﬁcant sample of artifacts to
categorize them based on the issues they discuss, the solutions
they propose, and the type of documentation they involve.
1) Identiﬁcation of Candidate Artifacts Related to Documen-
tation Issues: Table II summarizes the artifacts automatically
collected from the four sources (see column “candidate
artifacts”). We discuss the process adopted in each case.
TABLE II
STUDY DATASET
Source
Issues
Mailing Lists
Pull Requests
Stack Overﬂow
Overall
Candidate Manually
Analyzed
345
101
332
100
878
Artifacts
394,504
6,898
375,745
28,792
805,939
False
Posit.
19
5
21
4
49
Valid
Artifacts
324
95
310
95
824
Labeled
Sentences
562
220
581
185
1,548
Stack Overﬂow (SO). We mined from the ofﬁcial SO
dump of June 2018 all discussions having a question la-
beled with a documentation-related tag. To determine these
tags, we searched for all
tags related to documentation
and documentation tools in the SO tag page by using the
keywords doc, documentation and documentor. The latter term
is known to be part of the name of tools supporting software
documentation. One author then inspected all the tags resulting
from these three searches to identify the ones actually related to
software documentation and/or documentation tools. During the
inspection, the author read the tag name, the tag description and
some of the questions in which the tag was used. This process
resulted in the selection of 23 tags (e.g., code-documentation,
phpdocumentor, design-documents) that were used to search for
the related discussions in SO. The ﬁrst 30 results (discussions)
returned by the 23 searches were manually inspected to look for
additional documentation-related tags missed in the ﬁrst step.
The process was iterated with the newly founded tags until no
new tags were found in the top 30 results of the tag searches.
This resulted in a total of 78 (23+55) documentation-related
tags (available in our replication package [56]).
Next, we queried the SO dump to extract all discussions
having a question with a non-negative score and tagged with
one or more of the relevant 78 tags. We removed questions
with a negative score to ﬁlter out irrelevant discussions. This
process resulted in the selection of 28,792 discussions. For
each of them, we kept the question, the two top-scored answers
and the accepted answer (if any).
GitHub Issues and Pull Requests. We downloaded the
GitHub Archive [57] containing every public GitHub event
occurring between January 2015 and April 2018. While older
data is available, we excluded it since some of the information
needed for our study was only archived starting from 2015. We
extracted all events of type IssuesEvent, IssueCommentEvent,
PullRequestEvent and PullRequestReviewCommentEvent.
These events capture the opening/closing of issues and
pull requests as well as all the discussion held for them
through comments. A detailed description of these event types
is available online [58]. Then, we selected issues and pull
requests from projects having at least ten forks and/or stars
to exclude “toy” projects. Finally, we adopted a keyword-
matching approach to extract issues and pull requests related
to documentation. We started from the 78 SO tags previously
mentioned and converted them into 56 “keywords”. This
means, for example, that the SO tag design-documents was
converted into design doc (to match “design document”, “design
documents” and “design doc”), while tags including the word
“documentation” (e.g., xml-documentation) were replaced with
the keyword documentation, since matching this keyword will
also match the more speciﬁc ones. We also added 11 keywords
that we considered relevant but were not derived from any of
the 78 SO tags. For example, while the keyword api doc was
derived from the SO tag api-doc, we also added api manual.
In total, we deﬁned 66 documentation-related keywords [56].
We extracted all the issues/pull requests having at least one
of the 66 keywords in their title and/or in their ﬁrst post (i.e.,
the one opening the issue or the pull request). This resulted in
the selection of 394,504 issues and 375,745 pull requests.
Mailing Lists. We built a crawler to mine the mail archives
of the Apache Software Foundation (ASF) [59]. The ASF
archives all emails exchanged in the mailing lists of the projects
it runs. Each of its 295 projects has several mailing lists focused
on different topics. We mined all mailing lists named docs
(discussions related to documentation), dev (discussions among
developers) and users (discussions involving both users and
developers), for a total of 480 mailing lists. For the threads
extracted from the docs mailing lists, we did not apply any
ﬁlter. For the threads extracted from the dev and the users
mailing lists, we only selected those containing in the subject at
least one of 66 documentation-related keywords we previously
deﬁned. This resulted in the extraction of 6,898 email threads,
each one composed by one or more messages.
2) Manual Classiﬁcation of Documentation Issues: Once
we collected the candidate artifacts, we manually analyzed a
statistically signiﬁcant sample ensuring a 99% conﬁdence level
± 5%. This resulted in the selection of 665 artifacts for our
manual analysis, out of the 805,939 artifacts collected from
the four sources. Since the number of collected artifacts is
substantially different between the four sources (Table II), we
decided to randomly select the 665 artifacts by considering
these proportions. A simple proportional selection would
basically discard SO and mailing lists from our study, since
issues and pull requests account for over 90% of our dataset.
Indeed, this would result in the selection of 311 pull requests,
326 issues, 24 SO discussions and 6 mailing list threads. For
this reason, we adopted the following sampling procedure: for
SO and mailing lists, we targeted the analysis of 96 artifacts
each, ensuring a 95% conﬁdence level ± 10% within those
two sources. For issues and pull requests, we adopted the
proportional selection as explained above. This resulted in 829
artifacts to be manually analyzed (99% conﬁdence ± 4.5%).
Authorized licensed use limited to: Access paid by The UC Irvine Libraries. Downloaded on February 08,2024 at 23:55:52 UTC from IEEE Xplore.  Restrictions apply.
1201
The selected artifacts were manually analyzed by six of
the authors with the goal of classifying them as false positive
(i.e., unrelated to documentation issues) or assigning a set of
labels describing (i) the documentation issue discussed, (ii) the
solution proposed/adopted, (iii) the type of the documentation
and (iv) the documentation tools discussed. Each of these labels
was optional. For example, it is possible that only the issue
type and the solution were labeled for an artifact.
For two of the four categories, namely issue type and
documentation type, we started from a predeﬁned list of labels.
For the issue types, we used the 13 quality attributes deﬁned
by Zhi et al. [4]. For the type of documentation, we had 11
predeﬁned labels that we selected based on our experience (e.g.,
code comments). See [56] for the list of predeﬁned labels.
The labeling was supported by a Web app that we developed
for this task and for conﬂict resolution. Each author indepen-
dently labeled artifacts randomly assigned to her by the Web
app, selecting a proper label among the predeﬁned ones or
deﬁning a new label when needed. To assign a label, the author
inspected the whole artifact and, in the case of issues and pull
requests, also the related commits. Every time an author had
to label an artifact, the Web app also showed the list of labels
created by all taggers so far. The labeling was performed at
sentence level: The Web app allowed the author to select one
sentence from the artifact at a time and assign labels to it.
This means that multiple sentences could be labeled for each
artifact and hence multiple labels could be assigned to it. This
allowed us to create a database (publicly available [56]) of
labeled sentences related to documentation artifacts.
Each artifact was assigned to two authors by the Web app.
In case both of them classiﬁed the artifact as a false positive,
the artifact was replaced with another one randomly selected
from the same source (e.g., a false positive email thread was
replaced with another email thread). For each artifact X in
which there was a conﬂict in the assigned labels, a third
author (not previously involved in the labeling of X) was
assigned to solve it. A conﬂict in this scenario can happen
for many reasons. First, the two authors could label different
sentences in the artifact. Second, assuming the same sentences
are selected, different “categories” of labels could be assigned
to the sentences (e.g., one author labels a sentence as discussing
the issue, one as presenting a solution). Third, assuming the
same sentences and the same categories of labels are selected,
the label values differ (e.g., different solutions indicated for the
same sentence). Fourth, one author could classify the artifact as
a false positive, while the other could label it. For these reasons,
we had a high number of conﬂicted artifacts (765 out of 829
— 92.27%). We solved some speciﬁc cases automatically. In
particular, if two authors (i) labeled for the same artifact two
different sentences Si and Sj where Si is a substring of Sj
(or vice versa), and (ii) had no conﬂicts between the label
values, we automatically solved the conﬂict by selecting the
longest sentence as the valid one. This reduced the number of
conﬂicted artifacts to 592, which were manually reviewed by
a third author who could accept a conﬂicting sentence (and
apply minor modiﬁcations if necessary) or discard it.
In this ﬁnal process, 5 artifacts were discarded as false
positives. The ﬁnal number of sentences labeled for each type
of artifact is reported in Table II.
[0m[32;1m[1;3mAction: Generate Review
Action Input: Our data collection consists of two steps. First, we adopt an automatic process based on keyword matching to mine candidate artifacts related to documentation from the four analyzed sources (i.e., emails, issues and pull requests of open source projects, and Stack Overflow threads). Then, we manually analyze a statistically significant sample of artifacts to categorize them based on the issues they discuss, the solutions they propose, and the type of documentation they involve. 1) Identification of Candidate Artifacts Related to Documentation Issues: Table II summarizes the artifacts automatically collected from the four sources (see column “candidate artifacts”). We discuss the process adopted in each case.[0m

[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

methodology in software engineering has a strong focus on experimental research; the
earliest by Moher and Schneider ( 1981 ), Basili et al. ( 1986 ), the first methodology
handbook by Wohlin et al. ( 2000 ), and promoted by Tichy ( 1998 ). All have a tendency
towards quantitative approaches, although also qualitative approaches are discussed duringthe later years, e.g. by Seaman ( 1999 ). There exist guidelines for experiments ’conduct
(Kitchenham et al. 2002 ; Wohlin et al. 2000 ) and reporting (Jedlitschka and Pfahl 2005 ),
measurements (Basili and Weiss 1984 ; Fenton and Pfleeger 1996 ; van Solingen and
Berghout 1999 ), and systematic reviews (Kitchenham 2007 ), while only little is written on
case studies in software engineering (Höst and Runeson 2007 ; Kitchenham et al. 1995 ;
Wohlin et al.

2. What, precisely, was your contribution? Before reporting what you did, explain what problem 
you set out to solve or what question you set out to answer 
--and why this is important. 2.1 What kinds of questions do software 
engineers investigate? Generally speaking, software engineering researchers 
seek better ways to develop and evaluate sottware. Devel- 
opment includes all the synthetic activities that involve 
creating and modifying the software, including the code, 
design documents, documentation, etc. Evaluation includes all the analytic activities associated with predict- 
ing, determining, and estimating properties of the software 
systems, including both functionality and extra-functional 
properties such as performance or reliability. Software engineering research answers questions about 
methods of development or analysis, about details of 
designing or evaluating a particular instance, about gener- 
alizations over whole classes of systems or techniques, or 
about exploratory issues concerning existence or feasibil- 
ity. Table 1 lists the types of research questions that are 
asked by software engineering research papers and 
provides specific question templates. Table 1. Types of software engineering research questions 
Type of question Examples 
Method or means of How can we do/create/modify/evolve (or automate doing) X? development What is a better way to do/create/modify/evolve X?

have criticized common practice in the field for failing to collect, analyze, and report 
experimental measurements in research reports [9,10,11,12]. In 2001 I [8] presented preliminary sketches of some of the successful paradigms for software engineering research, drawing heavily on examples from software architecture. Scientific and engineering research fields can be characterized by identifying what 
they value: 
• What kinds of questions are "interesting"? • What kinds of results help to answer these questions, and what research 
methods can produce these results?

Chapter 11
Selecting Empirical Methods 
for Software Engineering Research
Steve Easterbrook, Janice Singer, Margaret-Anne Storey, 
and Daniela Damian
Abstract  Selecting a research method for empirical software engineering research 
is problematic because the benefits and challenges to using each method are not yet 
well catalogued. Therefore, this chapter describes a number of empirical methods 
available. It examines the goals of each and analyzes the types of questions each 
best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understand-ing and selecting from the variety of methods applicable to empirical software 
engineering. 1. Introduction
Despite widespread interest in empirical software engineering, there is little guid-
ance on which research methods are suitable to which research problems, and how to choose amongst them. Many researchers select inappropriate methods because 
they do not understand the goals underlying a method or possess little knowledge 
about alternatives. As a first step in helping researchers select an appropriate method, this chapter discusses key questions to consider in selecting a method, from philosophical considerations about the nature of knowledge to practical con-siderations in the application of the method. We characterize key empirical methods 
applicable to empirical software engineering, and explain the strengths and weak nesses 
of each. Software engineering is a multi-disciplinary field, crossing many social and 
technological boundaries. To understand how software engineers construct and maintain complex, evolving software systems, we need to investigate not just the tools and processes they use, but also the social and cognitive processes surround-
ing them. This requires the study of human activities. We need to understand how 
individual software engineers develop software, as well as how teams and organiza tions 
coordinate their efforts. 285
F.

• What kinds of evidence can demonstrate the validity of a result, and how are 
good results distinguished from bad ones? In this paper I attempt to make generally accepted research strategies in software en-
gineering explicit by examining research in the area to identify what is widely ac-cepted in practice. 1.1   Software Technology Maturation 
Redwine and Riddle [5,6] reviewed a number of software technologies to see how 
they develop and propagate. They found that it typically takes 15-20 years for a tech-nology to evolve from concept formulation to the point where it's ready for populari-zation. They identify six typical phases: 
• Basic research . Investigate basic ideas and concepts, put initial structure on 
the problem, frame critical research questions. • Concept formulation . Circulate ideas informally, develop a research commu-
nity, converge on a compatible set of ideas, publish solutions to specific sub-problems. • Development and extension . Make preliminary use of the technology, clarify 
underlying ideas, generalize the approach. • Internal enhancement and exploration . Extend approach to another domain, 
use technology for real problems, stabilize technology, develop training mate-rials, show value in results. • External enhancement and exploration . Similar to internal, but involving a 
broader community of people who weren’t developers, show substantial evi-dence of value and applicability. • Popularization .

What Makes Good Research in Software Engineering? Mary Shaw 
School of Computer Science, Carnegie Mellon University, 
Pittsburgh PA 15213 USA 
mary.shaw@cs.cmu.edu
http://www.cs.cmu.edu/~shaw/
Abstract. Physics, biology, and medicine have well-refined public explanations 
of their research processes. Even in simplified form, these provide guidance 
about what counts as "good research" both inside and outside the field. Soft-
ware engineering has not yet explicitly identified and explained either our re-
search processes or the ways we recognize excellent work. Science and engineering research fields can be characterized in terms of the kinds of questions they find worth investigating, the research methods they 
adopt, and the criteria by which they evaluate their results. I will present such a 
characterization for software engineering, showing the diversity of research strategies and the way they shift as ideas mature. Understanding these strategies 
should help software engineers design research plans and report the results 
clearly; it should also help explain the character of software engineering re-search to computer science at large and to other scientists. 1   Introduction 
Many sciences have good explanations of their research strategies. These explanations 
include not only detailed guidance for researchers but also simplified views for the 
public and other observers. Acceptance of their results relies on the process of obtain-ing the results as well as analysis of the results themselves. Schoolchildren learn the experimental model of physics: hypothesis, controlled experiment, analysis, and pos-sible refutation. The public understands large-scale double-blind medical studies well enough to discuss the risks of experimental treatment, the ethics of withholding prom-ising treatment from the control group, and the conflicts of interest that are addressed by the blinding process. Software engineering does not have this sort of well-understood guidance. Software 
engineering researchers rarely write explicitly about their paradigms of research and their standards for judging quality of results. A number of attempts to characterize software engineering research have contributed elements of the answer, but they do not yet paint a comprehensive picture. In 1980, I [7] examined the relation of engi-neering disciplines to their underlying craft and technology and laid out expectations 
for an engineering discipline for software. In 1984-5, Redwine, Riddle, and others [5,6] proposed a model for the way software engineering technology evolves from 
research ideas to widespread practice. More recently, software engineering researchers 
International Journal of Software Tools for Technology Transfer, 2002, vol. 4, no.

Empirical Research Methods in Software Engineering         23 
 References 
[1] D. A. Garvin, “Building a Learning Organization”, in Harward Business Review on 
Knowledge Management, pp. 47–80, Harward Business School Press, Boston, USA, 1998. [2] V. R. Basili, G. Caldiera, and H. D. Rombach, ”E xperience Factory” in Encyclopaedia of 
Software Engineering, editor John J. Marciniak, John Wiley & Sons, Inc., Hoboken, N.J., USA, 2002. [3] N. K. Denzin and Y. S. Lincoln, Handbook of Qualitative Research, Sage Publications, 
London, UK, 1994. [4] J. W. Creswell, Research Design, Qua litative and Quantitative Approaches, Sage 
Publications, 1994. [5] N. Fenton, and S. L. Pfleeger, Software Metrics: A Rigorous & Practical Approach, 2nd 
edition, International Thomson Computer Press, 1996. [6] B.

Software measurement forms the basis, but it is not sufficient. Empirical methods 
such as controlled experiments, case studies, surveys and post-mortem analyses are 
needed to help us evaluate and validate the research results. These methods are needed so that it is possible to scientifically state whether something is better than 
something else. Thus, empirical methods provide one important scientific basis for 
software engineering. For some type of pr oblems other methods, for example the use 
of mathematical models for predicting software reliability, is better suited, but in most cases the best method is applying empirici sm. The main reason being that software 

2003 ) and qualitative methods (Dittrich 2007 ; Seaman 1999 ; Sim et al. 2001 ). Recently, a comprehensive view of empirical research issues for software engineering hasbeen presented, edited by Shull et al. ( 2008 ). The term “case study ”appears every now and then in the title of software engineering
research papers. However, the presented studies range from very ambitious and wellorganized studies in the field, to small toy examples that claim to be case studies.Additionally, there are different taxonomies used to classify research. The term case study is
used in parallel with terms like field study and observational study, each focusing on a
particular aspect of the research methodology. For example, Lethbridge et al. use field
studies as the most general term (Lethbridge et al. 2005 ), while Easterbrook et al. ( 2008 )
callcase studies one of five “classes of research methods ”. Zelkowitz and Wallace propose
a terminology that is somewhat different from what is used in other fields, and categorizeproject monitoring, case study and field study as observational methods (Zelkowitz and
Wallace 1998 ). This plethora of terms causes confusion and problems when trying to
aggregate multiple empirical studies. The case study methodology is well suited for many kinds of software engineering
research, as the objects of study are contemporary phenomena, which are hard to study inisolation. Case studies do not generate the same results on e.g.

Easterbrook et al. Because of the importance of human activities in software development, many 
of the research methods that are appropriate to software engineering are drawn 
from disciplines that study human behaviour, both at the individual level (e.g. psy-
chology) and at the team and organizational levels (e.g. sociology).These methods 
all have known flaws, and each can only provide limited, qualified evidence about the phenomena being studied. However, each method is flawed differently (McGrath, 1995) and viable research strategies use multiple methods, chosen in such a way that the weaknesses of each method are addressed by use of comple-
mentary methods (Creswell, 2002). Describing in detail the wide variety of possible empirical methods and how to 
apply them is beyond the scope of the chapter. Instead, we identify and compare five classes of research method that we believe are most relevant to software engineering:
● Controlled Experiments  (including Quasi-Experiments )
● Case Studies  (both exploratory  and confirmatory )
● Survey Research
● Ethnographies
● Action Research
We describe the tradeoffs involved in choosing between these methods, but do not 
provide a recipe for building research strategies, as we doubt that such recipes exist. The selection of methods for a given research project depends on many local con-tingencies, including available resources, access to subjects, opportunity to control the variables of interest, and, of course, the skills of the researcher. To illustrate the steps involved in deciding which method or methods to use, we 
present two guiding examples. Two fictional software engineering researchers, Joe and 
Jane, will explore how the various research methods can be applied to their work:
● Jane is a new PhD student interested in the effectiveness of a novel fisheye-view 
file navigator. Her research is motivated by the fact that navigation is a primary 
activity of software developers requiring a lot of scrolling and many clicks to 
find files. “Fisheye-views” use a distortion technique that, if applied correctly, 
display information in a compact format that could potentially reduce the amount of scrolling required. Jane’s intuition is that the fisheye-view file navi-gator is more efficient for file navigation, but critics argue that the more compact information is difficult to read and that developers will not adopt it over the tra-
ditional file navigator. Her research goal, therefore, is to find evidence that sup-ports or refutes her intuition that fisheye-view file navigators are more efficient than traditional file navigators for navigation. ● Joe is a researcher in an industrial lab. His current interests are in understanding 
how developers in industry use (or not) UML diagrams during software design. This is because, as a student, his professors recommended UML diagrams be used 
during software design, but his recent exposure to industrial practices indi cates 
that UML is rarely used. His research goal is to explore how widely UML 

Question: What kinds of 'methodology' are there in software engineering research?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

What kind of Research Question are You Asking? One of the first steps in choosing an appropriate research method is to clarify the 
research question. While Jane and Joe have identified the problems they wish to work on, neither has pinned down a precise question. In each case, they could focus 
on a number of different research questions, each of which leads to a different 
direction in developing research strategies. The classification of research questions we use in this section is adapted from Meltzoff (1998). Often, the most obvious question is not the best choice for a starting point. Jane’s first attempt to formulate her research question is “Is a fisheye-view file 
navigator more efficient than the traditional view for file navigation?” , while Joe 
asks “how widely are UML diagrams used as collaborative shared artifacts during 
design?” . Both questions are vague, because they make assumptions about the 
phenomena to be studied, and kinds of situation in which these phenomena occur. For example, Jane’s question only makes sense if we already know that some people (who?) need to do file navigation (whatever that is?), under some circum-
stances (which are?), and that efficiency (measured how?) is a relevant goal for 
these people (how would we know that?). Joe’s question presupposes that we know what a “collaborative shared artifact” is, and can reliably identify one, and even reliably say which things are UML diagrams. Defining the precise meaning of terms is a crucial part of empirical research, and is closely tied with the idea of developing (or selecting) an appropriate theory . In the early stages of a research program, we usually need to ask exploratory  
questions, as we attempt to understand the phenomena, and identify useful distinctions that clarify our understanding. Suitable research methods for explor-atory questions tend to be those that offer rich, qualitative data, which help us to build tentative theories. Unless they are building on existing work that already offers clear definitions, both Jane and Joe need to formulate exploratory ques-tions, such as:

There seems to be a balance between the need for relevance and scientific rigor, which
has also been a perennial discussion in conferences with industry tracks or sessions. It is
Table 2 How Reviewers assess the quality and/or validity of a paper’s research method (showing only
selected codes with at least five responses)
Response Code Explanation Count
Rigor Assess rigor, validity of methodology relative to published literature or
standard practice57
Appropriateness Is the method appropriate 27
Detail Present appropriate level of detail; Transparent process 20
Validity Discussion of threats to validity 18
Conclusions Do the conclusions follow from the evidence 10
Reproducible Determine if the study is reproducible / replicable 10
Support material Includes support materials (e.g. tools, instruments, scripts, ...) 8
Knowledge Use own knowledge to judge validity 8
Data availability Availability of data 5
Sample Valid sample 5Empir Software Eng (2021) 26:  103 Page 11 of 29     103

It may be a complementary approach. However, it is important to be aware of the difference and also to consider how this affects the statistical analysis, si nce most statistical methods have developed 

12. Are multiple data sources and collection methods planned (triangulation)? 13. Are measurement instruments and procedures well defined (measurement definitions, interview
questions)? 14. Are the planned methods and measurements sufficient to fulfill the objective of the study?15. Is the study design approved by a review board, and has informed consent obtained from individuals and
organizations?Empir Software Eng (2009) 14:131 –164 149

How can one judgeit? It is very diﬃcult to give a satisfactory and deﬁnitiveanswer to such questions without the consensus of a greatpart of the community, which requires time and greateﬀort. A possible alternative is to summarize and organizethe written information currently available on the matter.And this is precisely the aim of this study. 2. Methodology
The present description of the major types of research
papers published in SE is based exclusively on existing writ-
ten sources, and in particular on the following:(1) Published, relevant papers, such as, for example,
Zelkowitz and Wallace (1998) orSjøberg et al. (2005) .

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

B. Conclusion
Existing guidelines offer various and limited sets of different
research approaches, which are of varying levels of granularity
(case study v. exploratory/evaluative case study; experiment v. controlled/quasi experiment, etc.), and thus represent a “mixed
bag” of methods. We have adopted a framework from the
social sciences, which provides a holistic overview of different
research strategies. While existing overviews provide overviews
of different methods, they do not systematically explain how
these methods relate to one another. The framework in Fig. 1
positions a set of eight different research strategies along two
dimensions: the level of ‘obtrusiveness’ (i.e., control by the
researcher), and the level of generalizability. We illustrate
each of these strategies with examples from the software
engineering literature which have been published in the ﬁeld’s
top outlets—an indication that these strategies are already
prominent, merely lacking a common terminology. These
strategies can be considered to be the architectures of research
designs, which have associated weaknesses and strengths due
to their relation with the level of the researcher’s ‘control’
(the obtrusiveness dimension) and the level of generalizability

8. Practical Considerations
In addition to the question of how well the methods fit a given type of research 
question and philosophical stance, the choice of methods also depends on prac-tical considerations. Often these practical considerations force the researcher to 
change the original research design in terms of the choice of method, data 
collection and analysis procedures. It is important to document the original planned research protocol, and all subsequent deviations to it, to allow other researchers to understand the study design, interpret the research results, and replicate the study. Most of the practical challenges relate to time, budget and personnel resources, 
and access to data. Rather than describe the challenges for each method individu-
ally, we summarize the challenges related to groups of methods, according to the type of data they deal with:
Methods that are primarily qualitative  include ethnography, case study, and action 
research. These methods rely on fieldwork, using techniques such as participant 
observation and interviews. Key challenges include preparing good questions for 

This document does  not go to this level of detail. The critical issue in any systematic review is  to ask the right question. In this context, 
the right question is usually one that: 
• Is meaningful and important to practitione rs as well as researchers. For example, 
researchers might be interested in whether a specific analysis technique leads to a 
significantly more accurate estimate of re maining defects after design inspections. However, a practitioner might want to know whether adopting a specific analysis 
technique to predict remaining defects is more effective than expert opinion at 
identifying design documents that require re-inspection. • Will lead either to changes in current software engineering practice or to 
increased confidence in the value of curre nt practice. For example, researchers 
and practitioners would like  to know under what conditi ons a project can safely 
adopt agile technologies and under what conditions it should not. • Will identify discrepancies between commonly held beliefs and reality. Nonetheless, there are systematic reviews that ask questions that are primarily of 
interest to researchers. Such  reviews ask questions that identify and/or scope future 
research activities. For example, a systema tic review in a PhD thesis should identify 
the existing basis for the research stud ent’s work and make it clear where the 
proposed research fits into the current body of knowledge. Examples 
 
Kitchenham et al. 190H[21] had three research questions: 
Question 1: What evidence is there that cross-company estimation models are not 
significantly different from within-company estimation models for predicting effort for 
software/Web projects? Question 2: What characteristics of the study data sets and the data analysis methods used in 
the study affect the outcome of within- a nd cross-company effort estimation accuracy 
studies? Question 3: Which experimental procedure is most appropriate for studies comparing within- 
and cross-company estimation models? Jørgensen 191H[17] had two research questions: 
1. Should we expect more accurate effort estimates when applying expert judgment or 
models? 2. When should software development effort estimates be based on expert judgment, 
when on models, and when on a combination of expert judgment and models? In both cases, the authors were aw are from previous research that  results were mixed, so in 
each case they added a question aimed at invest igating the conditions under which different 
results are obtained. 5.3.2 Question Structure 
Medical guidelines recommend considering a question about the effectiveness of a 
treatment from three viewpoints:  
• The population, i.e. the people affected by the intervention.

Easterbrook et al. program involves a mix of design questions and knowledge questions as the 
researchers investigate specific problems, how best to solve them, and which solu-tions work best (Wieringa and Heerkens, 2006). 3. What will You Accept as an Empirical Truth? Having specified the research question(s), it is worth considering what to accept as valid  answers.

Question: How is a methodology viewed as appropriate for a specific research question?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

What is your new result? Explain precisely what you have contributed to the 
store of sot~ware engineering knowledge and how this is 
useful beyond your own project. oOOio -i, i 
80*/0 
60% 
40% 
20% -I 
Iii Dewl Question 
i:G i:~ii i , 
Analy Eval Gener Feas Total 
• Accepted [] Rejected 
Figure 2. Distribution of acceptances and rejections 
by type of research question 
3.1 What kinds of results do software engineers 
produce? The tangible contributions of software engineering 
research may be procedures or techniques for develop- 
ment or analysis; they may be models that generalize from 
specific examples, or they may be specific tools, solutions, 
or results about particular systems. Table 3 lists the types 
of research results that are reported in software engineer- 
ing research papers and provides specific examples. 3.2 Which of these are most common? By far the most common kind of ICSE paper reports a 
new procedure or technique for development or analysis. Models of various degrees of precision and formality were 
also common, with better success rates for quantitative 
than for qualitative models. Tools and notations were well 
represented, usually as auxiliary results in combination 
with a procedure or technique. Table 4 gives the distribu- 
tion of submissions to ICSE 2002, based on reading the 
abstracts (but not the papers), followed by graphs of the 
counts and distributions in Figures 3 and 4. 728 


Of course, one must have the resources to do research this way. 3 Realistic Tasks 
When conducting controlled experiments in software engineering, one should 
consider the realism and representativeness of the tasks regarding the size, complexity and duration of the involved tasks. Specifi cation, implementation and verification 
methods also vary considerably between domains, such as accounting software versus flight-control systems. In our opinion, some experimental tasks bear little resemblance to actual tasks in software engineering; others are very similar to actual tasks [32]. In between there is a continuum. Larger development tasks may take 
months, while many maintenance tasks may take only a couple of hours. 

103–112. Dyba, T., Kitchenham, B.A., Jorgensen, M., 2005. Evidence-based
software engineering for practitioners. IEEE Softw. 22 (1), 58–65. Fenton, N., 1993. How eﬀective are software engineering methods? J. Syst. Softw. 22 (2), 141–146. Fletcher, P., 1995. The role of experiments in computer science. J. Syst. Softw. 30 (1–2), 161–163. Glass, R.L., 1995. Structure-based critique of contemporary computing
research. J. Syst. Softw. 28 (1), 3–7. Glass, R.L., Vessey, I., Ramesh, V., 2002. Research in software engineer-
ing: an analysis of the literature. Information Softw. Technol. 44 (8),491–506. Glass, R.L., Ramesh, V., Vessey, I., 2004. An analysis of research in
computing disciplines. Commun. ACM 47 (6), 89–94. Goldberg, A., 1982. Editorial policy. ACM Comput. Surveys 14 (2), 151–
157. Haddow, G., Klobas, J., 2004. Communication of research to practice in
library and information science: closing the gap. Library Information
Sci.

•They interact with many other development 
techniques and procedures. For example a design method depends on a preceding requirements analysis. It must consider constraints imposed by the software and hardware platform and programming languages, timescales, and budget. It must be integrated with appropriate coding and testing techniques. Thus, it would be difficult to confirm that a design technique had a significant impact on final product reliability. In general, it is difficult to determine a causal link between a particular technique and a desired project outcome when the application of the technique and the final outcome are temporally removed from one another, and there are many other tasks and activities that could also affect the final outcome. •The immediate outcomes of a software engineering 
technique will not necessarily have a strong relationship with final project outcomes. E.g. if you 
are interested in the effect design techniques have on 
application reliability (i.e. probability of failure in a given time period under defined operational conditions), measures of the design product (or 
design process) have no obvious relationship with 
the desired outcome. There are no good surrogate 
measures of product reliability that can be measured 
at the end of the design process. There seem to be two major approaches to this issue: 1.

• What kinds of evidence can demonstrate the validity of a result, and how are 
good results distinguished from bad ones? In this paper I attempt to make generally accepted research strategies in software en-
gineering explicit by examining research in the area to identify what is widely ac-cepted in practice. 1.1   Software Technology Maturation 
Redwine and Riddle [5,6] reviewed a number of software technologies to see how 
they develop and propagate. They found that it typically takes 15-20 years for a tech-nology to evolve from concept formulation to the point where it's ready for populari-zation. They identify six typical phases: 
• Basic research . Investigate basic ideas and concepts, put initial structure on 
the problem, frame critical research questions. • Concept formulation . Circulate ideas informally, develop a research commu-
nity, converge on a compatible set of ideas, publish solutions to specific sub-problems. • Development and extension . Make preliminary use of the technology, clarify 
underlying ideas, generalize the approach. • Internal enhancement and exploration . Extend approach to another domain, 
use technology for real problems, stabilize technology, develop training mate-rials, show value in results. • External enhancement and exploration . Similar to internal, but involving a 
broader community of people who weren’t developers, show substantial evi-dence of value and applicability. • Popularization .

  12 outcomes should be specified. For example, in some cases we require interventions 
that improve some aspect of software  production without affecting another e.g. improved reliability with no increase in cost. A particular problem for soft ware engineering experiments is the widespread use of 
surrogate measures for example, defects found during system testing as a surrogate 
for quality, or coupling measures for de sign quality. Studies that use surrogate 
measures may be misleading and conclusi ons based on such studies may be less 
robust. Context 
For Software Engineering, this is the c ontext in which the co mparison takes place 
(e.g. academia or industry), the participants taking part in  the study (e.g.

have criticized common practice in the field for failing to collect, analyze, and report 
experimental measurements in research reports [9,10,11,12]. In 2001 I [8] presented preliminary sketches of some of the successful paradigms for software engineering research, drawing heavily on examples from software architecture. Scientific and engineering research fields can be characterized by identifying what 
they value: 
• What kinds of questions are "interesting"? • What kinds of results help to answer these questions, and what research 
methods can produce these results?

2   Questions, Results, and Validation in Software Engineering 
Generally speaking, software engineering researchers seek better ways to develop and 
evaluate software. They are motivated by practical problems, and key objectives of the research are often quality, cost, and timeliness of software products. This section presents a model that explains software engineering research papers by 
classifying the types of research questions they ask, the types of results they produce, 
and the character of the validation they provide. This model has evolved over several 
years.

4  Introduct ion 
  Confronted by a confusing array of op tions for producing software, software 
engineers need proof that a particular approach or technique is really better than 
another. They need to know the clear-cut be nefits of one approach versus another. They need reliable evidence that one appr oach clearly works better than another. This need to work with facts rather than assumptions is a property of any 
engineering discipline. Very few ideas in SE are matched with empirical data. Ideas, whose truthfulness has not been tested against reality, are continually assumed as evident. For example, the 
famous software crisis (Naur, 1969) was and still is more a question of a subjective 
customer and/or developer impression than a confirmed phenomenon. In fact, we can find some claims that there is no software crisis (see comments about Turski and 
Hoare opinions given in (Maibaum, 1997)). Another example is the idea that traditional engineering disciplines behave  much better than SE (Pezeé, 1997). For 
example, has any empirical study been pe rformed to compare the failure rate of 
products obtained by SE and those produced by other engineering disciplines? Table 
1.1 presents the percentage of faults in the car industry shown in a study carried out by Lex Vehicle Leasing, one of the major world vehicle hire and leasing companies, 
on a universe of 73,700 cars in England. This fault rate could call the reliability of 
the engineering used in this industry into question; however, this is not the case. Therefore, if we are really to  speak of a software crisis, we  need studies to show that 
SE is less reliable than other branches of engineering, that is, the number of faults in SE should be checked against the number of faults in other branches of engineering. Other examples of computer science theo ries that have not been tested are 
functional programming, object-oriented programming or formal methods.

 SEPTEMBER/OCTOBER 2018  |  IEEE SOFTWARE  51
imagining a dialogue between 
researchers and practitioners (see 
the sidebar). This pattern is common: engi -
neers often rely on their experience 
and a priori beliefs1 or turn to co -
workers for advice. This is better than guessing or giving up. But what 
if incompletely validated research outcomes could be distilled into re -
liable sources, intermediate between 
validated results and folk wisdom? To impact practice, SE research 
results must lead to pragmatic, ac -
tionable advice. This involves syn -
thesizing recommendations from results with different assumptions 
and levels of rigor, assigning appro -
priate levels of confidence to the rec -
ommendations. Here, we examine how these tradeoffs between rigor 
and pragmatism have been handled 
in medicine, where risk is often ac -
ceptable in the face of urgency. We propose an approach to describ -
ing SE research results with varied quality of evidence and synthesizing those results into codified knowledge 
for practitioners. This approach can 
both improve practice and increase the pace of research, especially in ex -
ploratory topics. Software Engineering 
Research Expectations over Time
When the 1968 NATO Conference 
introduced “software engineering” 
to our vocabulary,2 research often 
focused on designing and building programs. There were guidelines for writing programs; the concept of rea -
soning mathematically about a pro -
gram had just been introduced. The 
emphasis was on demonstrated capa -
bility—what we might now call feasi -
bility—rather than rigorous validation. This is visible in a sampling of 
major results of the period. For ex -
ample, Carnegie Mellon University identified a set of canonical papers published between 1968 and 2002. 3 
Several are formal analyses or em -
pirical studies, and a few are case studies. However, the majority are carefully reasoned essays that pro -
pose new approaches based on the authors’ experience and insight. The field has historically built 
on results with varying degrees of certainty. Indeed, Fred Brooks pro -
posed a “certainty-shell” structure RESEARCHERS AND PRACTITIONERS 
VOLLEY ABOUT MAKING RESEARCH USEFUL
Dear Practitioners:
The research community is actually discovering things you might find useful. Please help us organize this knowledge so that 
it’s actually useful to you.

Question: How do we define 'concrete' results in software engineering research?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

objectives, (b) demonstration of novelty and superiority
compared to related work, (c) evidence of eﬀectiveness,reproducibility, scalability, and practical relevance.Reviewers should also ascertain if papers meet quality cri-teria such as conciseness and technical precision, the con-sistent use of terminology, and the deﬁnition of keyconcepts. Focus and limitation of length must often beevaluated together, as the ACM T Database Syst R sug-
gests: ‘‘TODS would like to discourage excessively longpapers (longer than 50 doubled-spaced pages including ﬁg-ures, references, etc.) and unnecessary digressions even inshorter papers. This should help authors to focus on themost important aspects of their submission [ ...]”. For the
rest, practically the same quality criteria as for authorsare brought up, such as clarity, readability and the abilityto target as broad an audience as possible. Studying the abstracts of papers published at ICSE
2002, Shaw (2003) describes how accepted research papers
answered questions such as: What was your contribution?What is your new result? How is your result diﬀerent fromand better than prior work?, etc. According to Shaw, whenit comes to evaluating submitted papers, programme com-mittees look for interesting, novel, and exciting results. Inaddition, as SE knowledge grows incrementally, like in
other areas of science and engineering, programme com-
mittees are very interested in the authors’ interpretationof prior work. 3.2. Extended versions of conference papersA special type of articles are those based on one or
more papers previously presented at a workshop, a sym-
posium or a conference. For instance, van Vliet (2006)
is an extended version of van Vliet (2005) . In the study
previously mentioned ( Montesi and Mackenzie-Owen,
2008), comparing article genres in SE, education and
biology, it was found that extended versions of confer-ence papers played a role only in SE. A distinguishing
feature of extended papers is the double round of
reviews they need to pass before journal publication(i.e. conference and journal reviews). In a survey withauthors of extended versions and journal editors, Mon-
tesi and Mackenzie-Owen (accepted for publication)found that the extension process may change dependingon the type of conference paper to extend (empirical,theoretical, ...). Some journal instructions to authors regulate the pub-
lication of these articles. Among the other conditions, onethird of the content approximately must be new and ori-ginal, and sometimes authors must submit both versions,a description of the diﬀerences between the submittedmanuscript and the preliminary version, and referencethe previous version in the paper (SoSyM). Papers pub-lished as such must be ‘‘important ”(Theor Pract Log
Prog), ‘‘outstanding ”(ACM T Database Syst ), and ‘‘of
particularly high quality ”(J Comput Sci Technol ). New
material takes in explorations of further research, theo-rems, proofs and/or implementation details ( ACM T
Graphic ). If journals address reviewers the general tone doesn’t
change, though they oﬀer some more clues on what to lookfor. In the ACM T Database Syst -R , for instance, extended
versions of conference papers ‘‘should thoroughly consoli-date the material, should extend it to be broader, andshould more carefully cover related research ”. In addition,
the novel approach should be described more deeply, andalternatives should also be considered more comprehen-sively. In IEEE T Vis Comput Gr -R , while no new results
are required, key ideas, examples, and further elaborationare expected. This conﬁrms Shaw’s observation that in con-
ferences time and space limitations do not allow authors to
explore all the possible results of their research ( Shaw,
2003).Table 2
Excerpt from the instructions of IEE Proc-Softw
Originality: is the work scientiﬁcally rigorous, accurate and novel? Does the work contain signiﬁcant additional material to that already published?

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

• Papers not subject to peer-review. When an SLR has been published in more than one journal/conference, the most 
complete version of the survey will be used. Primary study selection process 
 
The results will be tabulated as follows: 
• Number of papers per year per source 
• Number of candidate pape rs per year per source 
• Number of selected papers per year per source. The relevant candidate and selected studies will be selected by a single researcher. The rejected studies will be checked by a nother researcher. We will maintain a list 
candidate papers that were rejected  with reasons for the rejection. Quality Assessment 
 Each SLR will be evaluated using the York University, Centre for Reviews and Dissemination (CDR) Database of Abstracts of Reviews of Effects (DARE) criteria 
(http://www.york.ac.uk/inst/crd/crddatabase .htm#DARE). The criteria are based on 
four questions: 
 
• Are the review’s inclusion and exclusi on criteria describe d and appropriate? • Is the literature search likely to have covered all relevant studies? • Did the reviewers assess the quality/v alidity of the included studies? • Were the basic data/studies adequately described? The questions are scored as follows:  
• Question 1: Y (yes), the inclusion criteria  are explicitly defined in the paper, P 
(Partly), the inclusion criteria are implic it; N (no), the inclusion criteria are not 
defined and cannot be readily inferred. • Question 2: Y, the authors have either s earched 4 or more digital libraries and 
included additional search strategies or identified and referenced all journals 
addressing the topic of interest; P, the authors have searched 3 or 4 digital 
libraries with no extra search strategies, or searched a define d but restricted set 
of journals and conference proceedings;  N, the authors have search up to 2 
digital libraries or an extremel y restricted set of journals. • Question 3: Y, the authors have explicit ly defined quality criteria and extracted 
them from each primary study; P, the re search question involves quality issues 
that are addressed by the study; N no e xplicit quality assessm ent of individual 
papers has been attempted. 

Has its value been demonstrated? Relevance: is the material appropriate to the scope of the journal to which it is submitted? Motivation: does the problem considered have a sound motivation? Does the paper clearly demonstrate the scientiﬁc interest of the results? Referencing: has reference been made to the most recent and most appropriate work? Is the present work set in the context of the previous work? Clarity: is the English clear and well written? Poorly written English may obscure the scientiﬁc merit of your paper. Are the ideas expressed clearly
and concisely?

Characteristicsofhighqualitypapers Table 4shows the most common results from our
coding of the free-text responses to this question. Overall, the most common characteris-
tics reviewers want in a high quality paper relate to aspects of the quality of the research
methodology and how well it is presented in the paper. It is also interesting to note which
characteristics were not among the most commonly given answers, including the availability
of the data (5), the findings themselves (1), and the generalizability of the results (1). Characteristicsleadingtonegativereviews Figure 11shows the ranking of the charac-
teristics that tend to lead to negative reviews. The highest ranked characteristics include (1)
a mismatch of methods with claims raised in the contribution, (2) overly grandiose claims,
(3) a writing style that is hard to follow, (4) a research methodology which is difficultEmpir Software Eng (2021) 26:  103 103Page14of29

Tools that originated from a new technique also described in the 
paper would fall under this category. • “Specific Solution, Prototype, Answer, or Jud gment” covers many 
different types of results, including analysis of existing systems or 
specific analysis of a technique. • •“Empirical Report” covers papers that focus on a set of 
observations, either quantitative or qualitative, and could 
potentially incl ude a general prescription of guidelines based on 
the data in the report. The “Empirical Report” category includes 
the presentation of data or insights typically found in MSR papers. “Empirical Report” is a modification of the original classification scheme, which was called “Report”. The new name emphasizes 
the type of contribution that results of MSR papers.

3   Research Strategies 
Section 2 identifies the three important aspects of an individual research result as 
reported in a typical conference or journal paper. It is clear that the spectrum of good research strategies includes experimental computer science in the sense of [9,10,11, 12]; it is also clear that the spectrum is much broader than just experimental research. Of course, not all the combinations of question, result, and validation make sense.

How can one judgeit? It is very diﬃcult to give a satisfactory and deﬁnitiveanswer to such questions without the consensus of a greatpart of the community, which requires time and greateﬀort. A possible alternative is to summarize and organizethe written information currently available on the matter.And this is precisely the aim of this study. 2. Methodology
The present description of the major types of research
papers published in SE is based exclusively on existing writ-
ten sources, and in particular on the following:(1) Published, relevant papers, such as, for example,
Zelkowitz and Wallace (1998) orSjøberg et al. (2005) .

Use them consistently. The more formal or analytic the paper, the more important 
this is. Here are some questions that the program committee 
may ask about your paper: 
What, precisely, do you claim to contribute? Does your result fiJlly satisfy your claims? Are the 
definitions precise, and are terms used consistently? Authors tend to have trouble in some specific 
situations. Here are some examples, with advice for 
staying out of trouble:" 
• If your result ought to work on large systems, explain 
why you believe it scales. • If you claim your method is "automatic", using it 
should not require human intervention. If it's 
automatic when it's operating but requires manual 
assistance to configure, say so. If it's automatic 
except for certain eases, say so, and say how often 
the exceptions occur. • If you claim your result is "distributed", it probably 
should not have a single central controller or server. If it does, explain what part of it is distributed and 
what part is not. • If you're proposing a new notation for an old 
problem, explain why your notation is clearly 
superior to the old one. • If your paper is an "experience report", relating the 
use of a previously-reported tool or technique in a 
practical software project, be sure that you explain 
what idea the reader can take away from the paper to use in other settings. If that idea is increased 
confidence in the tool or technique, show how your 
experience should increase the reader's confidence 
for applications beyond the example of the paper. What ~ new here? The program committee wants to know what is novel 
or exciting, and why. What, specifically, is the 
contribution? What is the increment over earlier work by 
the same authors? by other authors? Is this a sufficient 
increment, given the usual standards of subdiscipline? Above all, the program committee also wants to know 
what you actually contributed to our store of knowledge 
about software engineering. Sure, you wrote this tool and 
tried it out. But was your contribution the technique that is 
embedded in the tool, or was it making a tool that's more 
effective than other tools that implement the technique, or 
was it showing that the tool you described in a previous 
paper actually worked on a practical large-scale problem? It's better for you as the author to explain than for the 
program committee to guess. Be clear about your claim ... Awful • • I completely and generally solved ... I (unless you actually did!) 
Bad • • I worked on galumphing. (or studied, investigated, sought, 
explored) 
Poor • • I worked on improving galumphing. (or contributed to, participated in, 
helped with) 
Good • • I showed the feasibility of composing 
blitzing with flitzing. i • I significantly improved the accuracy of 
the standard detector. (or proved, demonstrated, created, 
established, found, developed) 
Better • • I automated the production of ritz 
tables from specifications. • With a novel application of the blivet 
transform, I achieved a 10% increase 
in speed and a 15% improvement in 
coverage over the standard method. Use verbs that show results and achievement, not just 
effort and activity.

Examples 
Kitchenham et al. used the following inclusion criteria: 
• any study that compared predictions of cross-company models with within-
company models based on analysis of single company project data. They used the following exclusion criteria: 
• studies where projects were only collect ed from a small number of different sources 
(e.g. 2 or 3 companies),  
• studies where models derived from a with in-company data set were compared with 
predictions from a general cost estimation model. Jørgensen 206H[17] included papers that compare j udgment-based and model-based software 
development effort estimation. He also ex cluded one relevant paper due to “incomplete 
information about how the estimates were derived”. Issues: 
• Medical standards make a point that it is important to avoid, as far as possible, 
exclusions based on the language of th e primary study. This may not be so 
important for Software Engineering. • It is possible that inclus ion decisions could be affected by knowledge of the 
authors, institutions, journals or year of publication. Some medical researchers 
have suggested reviews should be done af ter such information has been removed. However, it takes time to do this and experimental evidence suggests that 
masking the origin of primary studies does not improve reviews 207H[4]. 6.2.2 Study selection process 
Study selection is a multistage process. Initially, selection criteria should be 
interpreted liberally, so that unless a study identifie d by the electronic and hand 
searches can be clearly excluded based on title and abstract, a full copy should be 
obtained. However, Brereton et al. 208H[5] point out that  “The standard of IT and software 
engineering abstracts is too poor to rely  on when selecting primary studies. You 
should also review the conclusions.” 
 
The next step is to apply inclusion/excl usion criteria based on practical issues 209H[11] 
such as: 
• Language 
• Journal 
• Authors 
• Setting 
• Participants or subjects 
• Research Design 
• Sampling method 
• Date of publication. Staples and Niazi point out th at it is sometimes necessary to consider the questions 
that are not being addressed in orde r to refine your exclusion criteria 210H[27]. Example 
Staples and Niazi’s research question was 

Question: What are the criteria for 'specific' results in academic papers?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m
[38;5;200m[1;3mcontent='```\n        Manuscript Text: Our data collection consists of two steps. First, we adopt an automatic process based on keyword matching to mine candidate artifacts related to documentation from the four analyzed sources (i.e., emails, issues and pull requests of open source projects, and Stack Overflow threads). Then, we manually analyze a statistically significant sample of artifacts to categorize them based on the issues they discuss, the solutions they propose, and the type of documentation they involve.\n        Review: The methodology used in the research involves a combination of automatic data collection through keyword matching and manual analysis of a statistically significant sample of artifacts. This combination of methods aligns with the research goals of categorizing artifacts related to documentation from various sources.\n```\n\n```\n        Manuscript Text: Identification of Candidate Artifacts Related to Documentation Issues: Table II summarizes the artifacts automatically collected from the four sources (see column “candidate artifacts”). We discuss the process adopted in each case.\n        Review: The results presented in Table II provide concrete information about the artifacts automatically collected from the four sources, demonstrating a specific aspect of the data collection process in the research.\n```' response_metadata={'token_usage': {'completion_tokens': 226, 'prompt_tokens': 855, 'total_tokens': 1081}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-a9c42ed1-f0ba-48f4-92a2-7f0fc302dce9-0'[0m[32;1m[1;3mAction: Fetch Section Content by Title
Action Input: {"section_title": "B. Data Analysis", "path": "/Users/crystalalice/Desktop/ICSHP_Research/SE_paper/Software_Documentation_Issues_Unveiled.pdf"}[0m[33;1m[1;3mWe answer our RQ by presenting a taxonomy of the types
of documentation issues found in our analysis.
Such a taxonomy was deﬁned in an open discussion involving
all the authors and aimed at merging similar labels and hierarchi-
cally organizing them (see Figure 1). We focus our qualitative
analysis on speciﬁc categories of issue types. For each category,
we present interesting examples and common solutions, and
discuss implications for researchers and practitioners.
[0m[32;1m[1;3mAction: Generate Review
Action Input: We answer our RQ by presenting a taxonomy of the types of documentation issues found in our analysis. Such a taxonomy was defined in an open discussion involving all the authors and aimed at merging similar labels and hierarchically organizing them. We focus our qualitative analysis on specific categories of issue types. For each category, we present interesting examples and common solutions, and discuss implications for researchers and practitioners.[0m

[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

5. WHAT TYPE OF CONTRIBUTION? In this section, we describe the types of contributions that researchers 
claim in their papers, and provide the reasoning for the classification 
scheme and how abstracts might fall into each code. While some of the 
categories remain t he same compared to the Shaw study, we made some 
modifications to account for changes in the research community over 
the intervening years. This subsection will also explain the classification 
procedure and provide the quantitative and qualitative results for the 
analysis of contributions to the research community. 5.1 What kinds of results do researchers 
generate? The types of new results/contributions that are derived from software 
engineering research can take on many forms. Shaw provided seven 
types of cont ributions, a s shown in Table 3 [24]. Table 1. Types of software engineering research questions, adapted from [24] (Changes in Bold)  
Type of questio n Examples  
Method or means of development  How can we do/create/modify/evolve (or automate doing) X? What is a better way to do/create/modify/evolve X? Method for analysis or evaluation  How can I evaluate the quality/correctness of X? How do I choose between X and Y? Design, evaluation, or analysis of a particular 
instance  How do we evaluate Y, and when is it determined to be “good”? What is  a property X of 
artifact/method Y? What is a (better) design, implementation, maintenance, or adaptation for application X? How does X compare to Y? What is the current state of X / practice of Y? Generalization or characterization  Given X, what will Y (necessarily) be? What, exactly, do we mean by X? What are its important characteristics? What is a good formal/empirical model for X? What are the varieties of X, how are they related? Feasibility study or exploration  Does X even exist, and if so what is it like? Is it possible to accomplish X at all?

The study emphasizes the need for further experimentation in the area. 

Of these, only 5 
papers were accepted. We used a logistic regression model to analyze 
whether collaboration impacts the acceptance of papers by controlling 
for the number of authors, number of unique affiliations, number of 
unique countries, and the number of previously published ICSE papers 
of the authors. The only statistically significant factor is the number of 
previously published ICSE papers by the authors (p -value = 0.0008). From the results presented in this section, we argue that  ICSE draws in 
research from a diverse community with sub missions coming from 44 
unique countries and 465 unique organizations. Furthermore, the 2016 
acceptance data shows no significant favorable bias towards PC and PB 
members. The only factor that increases a paper’s chances of acceptance 
is the number of pape rs published by the authors at ICSE previously. We 
hypothesize that senior researchers submit higher quality papers or 
papers that fit better at ICSE. Table 7. Submissions and acceptance rates for research question/contribution combinations  
Research Question  Procedure 
or 
technique  Qualitativ
e or 
descriptive 
model  Empirical 
model  Analytic 
model  Tool or 
notation  Specific 
solution  
/prototype  
/answer/  
judgment  Empirical 
Report  
Method for means of 
development  128 (16%)  10 (20%)  12 (25%)  29 (10%)  42 (18%)  2 (0%)  3 (33%)  
Method for analysis or 
evaluation  48 (23%)  3 (0%)  10 (0%)  11 (18%)  10 (50%)  3 (33%)  5 (40%)  
Evaluation or Analysis of 
instance  11 (18%)  2 (0%)  4 (50%)  9 (33%)  15 (7%)  7 (14%)  25 (44%)  
Generalization or 
characterization  2 (0%)  7 (29%)  2 (0%)  2 (0%)  5 (20%)  0 (0%)  69 (25%)  
Feasibility study or 
exploration  6 (17%)  0 (0%)  1 (0%)  3 (0%)  9 (11%)  2 (0%)  1 (0%)  
Table 8. Submissions and acceptance rates for research Contribution/validation combinations  
Submitted (% 
Accepted)  Analysis  Evaluation  Experience  Example  Underspecified  Persuasion  Blatant 
Assertion  
Procedure or technique  31 (26%)  70 (31%)  3 (33%)  32 (9%)  45 (9%)  8 (0%)  6 (0%)  
Qualitative or descriptive 
model  1 (0%)  6 (50%)  2 (0%)  2 (0%)  5 (0%)  3 (33%)  3 (0%)  
Empirical model  12 (25%)  10 (20%)  0 (0%)  2 (0%)  1 (0%)  2 (0%)  2 (0%)  
Analytic model  10 (30%)  8 (25%)  1 (0%)  7 (14%)  9 (22%)  11 (0%)  8 (0%)  
Tool or notation  6 (17%)  27 (30%)  4 (50%)  10 (0%)  25 (20%)  3 (0%)  8 (0%)  
Specific 
solution/prototype/  
answer/judgment  2 (50%)  5 (20%)  1 (0%)  2 (0%)  3 (0%)  0 (0%)  1 (0%)  
Report  10 (30%)  62 (44%)  8 (13%)  6 (0%)  6 (0%)  8 (0%)  3 (0%)  

and in development and extension (still more time is spent in basic research, but it is 
very difficult to identify the beginning of this phase). As a result, full understanding of research strategy must account for the accumulation of evidence over time as well as for the form and content of individual projects and papers. The IMPACT project [3] is tracing the path from research into practice. The objec-
tives of the project include identifying the kinds of contributions that have substantial impact and the types of research that are successful.

The number of research questions also plays a major role; it is important to restrict them to few. Another important influence factor is the number of stakeholders involved. The more stakeholders are involved, the more potential interest conflicts about research questions will arise, which increase the amount of communication involved. Thus, the number of stakeholders is a factor that may contribute exponentially to the total effort. Design : Time/effort increases basically with (a) the complexity of the conceptual 
model; (b) the number of contacts based on validity goals and anticipated response 
rate; and (c) the length of the questionnaire/interview (i.e., number and complexity of 
questions). The main influence factor for design is the complexity of the conceptual 
model, which is, in turn, influenced by the number and complexity of research 
questions. In our experience, the research questions contribute exponentially to the 
conceptual model; that is, even one additional question can increase the complexity of the conceptual model significantly. In our case, the conceptual model was quite 
complex, which resulted in a large and complex questionnaire (many conditional 
questions). This increased the design effort significantly. Therefore, the complexity of the conceptual model has an exponential influence on design effort. 

9. Is there evidence that the 
E/QE can be used by other researchers / practitioners? Do the authors discuss whether or how the findi ngs can be transferred to other populations, or 
consider other ways in whic h the research can be used? To what extent do authors interpret results in the context of other studies / the existing body of 
knowledge?

What was the result? • If the concept of similarity ha s been used in the internal re plications, for example, with 
regard to the subjects or the experimental un its, have the possible differences been tested 
for statistical significance? What was the result? Findings of the experiment • How can the result of the experiment be explained? • Does the result obtained contradict or support the results of earlier experiments? • What other experiments could be run on the ba sis of this one to further investigate the 
results yielded? 

In what way is it better? What real problem 
does it solve? Does it scale? If your contribution is principally the synthesis or 
integration of other results or components, be clear about 
why the synthesis is itself a contribution. What is novel, 
exciting, or nonobvious about the integration? Did you 
generalize prior results? Did you find a better 
representation? Did your research improve the individual 
results or components as well as integrating them? A 
paper that simply reports on using numerous elements 
together is not enough, even if it's well-engineered. There 
must be an idea or lesson or model that the reader can take 
from the paper and apply to some other situation. If your paper is chiefly a report on experience 
applying research results to a practical problem, say what 
the reader can learn from the experience. Are your 
conclusions strong and well-supported? Do you show 
comparative data and/or statistics? An anecdotal report on 
a single project is usually not enough. Also, if your report 
mixes additional innovation with validation through 
experience, avoid confusing your discussion of the 
innovation with your report on experience. After all, if 
you changed the result before you applied it, you're 
evaluating the changed result. And if you changed the 
result while you were applying it, you may have 
confounded the experiences with the two versions. If a tool plays a featured role in your paper, what is 
the role of the tool? Does it simply support the main 
contribution, or is the tool itself a principal contribution, 
or is some aspect of the tool's use or implementation the 
main point? Can a reader apply the idea without the tool? If the tool is a central part of result, what is the technical 
innovation embedded in the tool or its implementation? If a system implementation plays a featured role in 
your paper, what is the role of the implementation? Is the 
system sound? Does it do what you claim it does? What 
ideas does the system demonstrate? • If the implementation illustrates an architecture or 
design strategy, what does it reveal about the 
architecture? What was the design rationale? What 
were the design tradeoffs? What can the reader apply 
to a different implementation? • If the implementation demonstrates an 
implementation technique, how does it help the 
reader use the technique in another setting? • If the implementation demonstrates a capability or 
performance improvement, what concrete evidence 
does it offer to support the claim? • If the system is itself the result, in what way is it a 
contribution to knowledge? Does it, for example, 
show you can do something that no one has done 
before (especially if people doubted that this could 
be done)? 731 


It may be a complementary approach. However, it is important to be aware of the difference and also to consider how this affects the statistical analysis, si nce most statistical methods have developed 

Our thinking was that higher numbers of defects would correspond to students 
paying more attention to performing a serious review. However, instead it seemed 
to motivate students to disregard the process we wanted to study (or at least augment it with additional activities) in order to report as many potential defects as possible. Thus results were unrepresentative of what would have been observed in a normal application of the review technique. We have rectified this mistake on later experiments by grading based on process conformance (as assessed by the creation of intermediate artifacts) as well as on the soundness of the evaluation of the technology done by the subjects after the experiment. 6 Conclusions and Future Work 
In this paper, we have reported on a few empirical studies that we have carried out with students. Even though most of these studies were initially conceived for research reasons, we believe that it is important to keep in mind and study the impact of these studies on the students’ education. This paper is a preliminary report in which we have described our experiences and 
lessons learned, but a lot more work needs to be done. Here, we outline some possible 
research directions. Impact of empirical studies on other actors . In this paper, we have focused on the 
researchers’ and the students’ viewpoint s. However, other actors may play an 
important role in the process and be affected by it. For one thing, we have assumed throughout the paper that the researcher and the instructor are the same person. This is 
not necessarily the case, but even so, the same person is playing two different roles, 
with two different sets of goals and responsibilities. As a researcher, one needs to 
provide scientifically sound results to the research community and the industry; as an 
instructor, one needs to provide his or he r students with the best education possible 
for their future activities. The instructor’s goals may conflict with the researcher’s 
goals, so a sensible trade-off should be sought. By the same token, when carrying out 
an empirical study in an industrial envi ronment, one should always try to minimize 
the risk that the experimental activities disrupt the normal course of business. This 
minimization of risk can be seen in the examples presented in Sections 2.4 and 2.2. By running and debugging the experiment in the university classroom, researchers 
were able to have a positive experience when they moved it to and industrial setting. Costs vs.

Question: What does 'contribution' mean in the context of this research?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2   After 
developing a research question, you will need to consider whether it can be answered through the existing published literature or whether new data must be collected. Reviewing the litera-ture is only manageable if the question has clear boundaries. In research, we are often contribut-
ing a tiny step to the existing knowledge, rather than making huge leaps. Small contributions are better than no changes at all. So a long, speciﬁ  c question is 
likely to be preferable to a short vague question. To illustrate these points 
Table  1  provides some speciﬁ  c 
examples, and in the next section we discuss some key considerations. CONSIDERATIONS: 
RELEVANCE, ORIGINALITY AND RIGOUR 
 Here, we highlight some things 
to think about as you develop your research question, building on the points raised above. These analyses are not intended to be undertaken in any speciﬁ  c order. The considerations fall into three main categories (Figure  1 ), which 
can be thought of as broadly con-cerning the relevance, originality and rigour of the research ques-tion, and which are interrelated and partly inspired by the UK Research Excellence Framework. 3   
You may start with any one of the categories and revisit each one multiple times. In doing so, you may change your research question slightly or completely.

The study emphasizes the need for further experimentation in the area. 

We provide an overview of these perspectives
in Section 6. Typically, for technical scientific results, peer reviewers are often expected to
comment on the novelty of the work, the soundness and reliability of the work, and the pre-
sentation of the work. These expectations are summarized by the maxim “is it new, and is it
true?” (MacAuley 2012 ).

and in development and extension (still more time is spent in basic research, but it is 
very difficult to identify the beginning of this phase). As a result, full understanding of research strategy must account for the accumulation of evidence over time as well as for the form and content of individual projects and papers. The IMPACT project [3] is tracing the path from research into practice. The objec-
tives of the project include identifying the kinds of contributions that have substantial impact and the types of research that are successful.

The rest of this 
section addresses  scope and then tackles two frequently  asked questions  
about this work. Section  2 explores  historical  work on writing  ‘good’ ar- 
ticles and methodological  guidance,  coupled  with how other data-driven  
disciplines  have addressed  experimental  reliability  concerns. Next, in 
Section  3 we describe  our approach  to identifying  “bad smells ”. This is 
followed  by Section  4 that lists some key symptoms  or ‘smells’  in ap- 
proximate  order of importance. Finally,  in Section  5 we consider  the 
implications  for the future and how we, the research  community,  might 
collectively  take ownership  of these “smells  ”, re-prioritise,  re-order,  add 
and subtract  “smells  ”. 1.1. Scope 
Software  analytics  research  is relatively  new and growing  fast, so 
much of our advice relates to articles  discussing  induction  from soft- 
ware project data. We discuss problems  with empirical  data analysis  
via experiments,  quasi-experiments  and correlation  studies [91] rather 
than issues relating  to e.g., qualitative  studies. Other articles  should be 
consulted  for tips and traps relating  to qualitative  research  and experi- 
mentation  involving  human participants,  e.g., [25,60,84,102]  . Finally,  in order to delineate  the scope of this article we feel it helpful 
to add some remarks  concerning  the role of theory in software  analyt- 
ics type research. Naturally  much analytics  research  will be data-driven  
or inductive  in nature. In such circumstances  the role of theory or de- 
ductive  reasoning  will be secondary. This of course begs the question  of 
36 

&Selection strategy —where to seek data? The objective of the study may be, for example, exploratory, descriptive, explanatory, or
improving. The objective is naturally more generally formulated and less precise than infixed research designs. The objective is initially more like a focus point which evolvesduring the study. The research questions state what is needed to know in order to fulfill theobjective of the study. Similar to the objective, the research questions evolve during thestudy and are narrowed to specific research questions during the study iterations(Andersson and Runeson 2007b ). The case may in general be virtually anything which is a “contemporary phenomenon in
its real-life context ”(Yin 2003 ). In software engineering, the case may be a software
development project, which is the most straightforward choice. It may alternatively be an
individual, a group of people, a process, a product, a policy, a role in the organization, an
event, a technology, etc. The project, individual, group etc. may also constitute a unit ofanalysis within a case. In the information systems field, the case may be “individuals,
groups …or an entire organization. Alternatively, the unit of analysis may be a specific
project or decision ”(Benbasat et al. 1987 ). Studies on “toy programs ”or similarly are of
course excluded due to its lack of real-life context. Yin ( 2003 ) distinguishes between
holistic case studies , where the case is studied as a whole, and embedded case studies
where multiple units of analysis are studied within a case, see Fig. 1. Whether to define a
study consisting of two cases as holistic or embedded depends on what we define as thecontext and research goals. In our XP example, two projects are studied in two differentcompanies in two different application domains, both using agile practices (Karlström and
Runeson 2006 ). The projects may be considered two units of analysis in an embedded case
study if the context is software companies in general and the research goal is to study agilepractices. On the contrary, if the context is considered being the specific company orapplication domain, they have to be seen as two separate holistic cases. Benbasat et al.comment on a specific case study, “Even though this study appeared to be a single-case,
embedded unit analysis, it could be considered a multiple-case design, due to thecentralized nature of the sites. ”(Benbasat et al.

24 The Role of Experimentation in Scientific and Technological Research 
and so on. A technological investigation is, in this sense, more prescribed than a 
scientific investigation. It is also more limited, in that it may end when it has led 
to an adequate solution of a technical problem”. Anyone working in scientific and t echnical research accepts, as a working 
hypothesis, that the world is a cosmos not a chaos, that is, th ere are natural laws 
that can be comprehended and thought out. In the case of SE, researchers can be 
said to assume that precepts that desc ribe and prescribe the optimum means of 
building software can be discovered  and established. As Pfleeger (1999) said, 
the basis of all empirical software engineering is “if we look long enough and hard enough, we will find rational rules that  show us the best ways to build the 
best software”. For a body of knowledge to be considered scientific, its truth and validity must be proven.

How can one judgeit? It is very diﬃcult to give a satisfactory and deﬁnitiveanswer to such questions without the consensus of a greatpart of the community, which requires time and greateﬀort. A possible alternative is to summarize and organizethe written information currently available on the matter.And this is precisely the aim of this study. 2. Methodology
The present description of the major types of research
papers published in SE is based exclusively on existing writ-
ten sources, and in particular on the following:(1) Published, relevant papers, such as, for example,
Zelkowitz and Wallace (1998) orSjøberg et al. (2005) .

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

Are the case and its units of analysis well defined? 1, 5, 2841. Is the suitability of the case to address the research questions clearly motivated? 8, 9, 1442. Is the case study based on theory or linked to existing literature?

Question: How is 'precisely' interpreted in terms of research scope and focus?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

This document does  not go to this level of detail. The critical issue in any systematic review is  to ask the right question. In this context, 
the right question is usually one that: 
• Is meaningful and important to practitione rs as well as researchers. For example, 
researchers might be interested in whether a specific analysis technique leads to a 
significantly more accurate estimate of re maining defects after design inspections. However, a practitioner might want to know whether adopting a specific analysis 
technique to predict remaining defects is more effective than expert opinion at 
identifying design documents that require re-inspection. • Will lead either to changes in current software engineering practice or to 
increased confidence in the value of curre nt practice. For example, researchers 
and practitioners would like  to know under what conditi ons a project can safely 
adopt agile technologies and under what conditions it should not. • Will identify discrepancies between commonly held beliefs and reality. Nonetheless, there are systematic reviews that ask questions that are primarily of 
interest to researchers. Such  reviews ask questions that identify and/or scope future 
research activities. For example, a systema tic review in a PhD thesis should identify 
the existing basis for the research stud ent’s work and make it clear where the 
proposed research fits into the current body of knowledge. Examples 
 
Kitchenham et al. 190H[21] had three research questions: 
Question 1: What evidence is there that cross-company estimation models are not 
significantly different from within-company estimation models for predicting effort for 
software/Web projects? Question 2: What characteristics of the study data sets and the data analysis methods used in 
the study affect the outcome of within- a nd cross-company effort estimation accuracy 
studies? Question 3: Which experimental procedure is most appropriate for studies comparing within- 
and cross-company estimation models? Jørgensen 191H[17] had two research questions: 
1. Should we expect more accurate effort estimates when applying expert judgment or 
models? 2. When should software development effort estimates be based on expert judgment, 
when on models, and when on a combination of expert judgment and models? In both cases, the authors were aw are from previous research that  results were mixed, so in 
each case they added a question aimed at invest igating the conditions under which different 
results are obtained. 5.3.2 Question Structure 
Medical guidelines recommend considering a question about the effectiveness of a 
treatment from three viewpoints:  
• The population, i.e. the people affected by the intervention.

3. Impact Project. "Determining the impact of software engineering research upon practice. Panel summary, Proc. 23rd International Conference on Software Engineering (ICSE 2001) , 
2001  
4. William Newman. A preliminary analysis of the products of HCI research, using pro forma 
abstracts. Proc 1994 ACM SIGCHI Human Factors in Computer Systems Conference (CHI 
'94), pp.278-284. 5. Samuel Redwine, et al. DoD Related Software Technology Requirements, Practices, and 
Prospects for the Future . IDA Paper P-1788, June 1984. 6. S. Redwine & W. Riddle. Software technology maturation. Proceedings of the Eighth Inter-
national Conference on Software Engineering,  May 1985, pp. 189-200.

 33% (38) 
 8% (9)  65% (74) 
 36% (41) 
 25% (29) 
 12% (14) 
 14% (16) 
 4% (4)  25% (15) 
 7% (4)  74% (45) 
 52% (32) 
 26% (16) 
 13% (8) 
 8% (5) 
 5% (3) Award
NoAward
OtherI carefully calibrate my epistemological
perspective with the authorsI do not validate these artifacts.I do not review these types of papersI demand and/or access data/ scripts and
experimental packageI attempt to run the source code/
container etc. on my machineI check for data consistencyI check for data availability
0% 20% 40% 60%
Number of respondents using that approach
Fig.8 Validate artifacts and claims
look at the chain of evidence from observation to conclusion and consider whether it is clear
and unbroken. I consider different quality criteria and threats to validity depending on the
type of study and its (often implicit) philosophical position.”
Characteristics of impactful papers Figure 9shows the distribution of how respondents
judge the impact of papers. It is interesting to observe that the top three characteristics are
novelty ,relevance ,a n d methodological rigor , which can sometimes be in conflict with each
other.

2. What, precisely, was your contribution? Before reporting what you did, explain what problem 
you set out to solve or what question you set out to answer 
--and why this is important. 2.1 What kinds of questions do software 
engineers investigate? Generally speaking, software engineering researchers 
seek better ways to develop and evaluate sottware. Devel- 
opment includes all the synthetic activities that involve 
creating and modifying the software, including the code, 
design documents, documentation, etc. Evaluation includes all the analytic activities associated with predict- 
ing, determining, and estimating properties of the software 
systems, including both functionality and extra-functional 
properties such as performance or reliability. Software engineering research answers questions about 
methods of development or analysis, about details of 
designing or evaluating a particular instance, about gener- 
alizations over whole classes of systems or techniques, or 
about exploratory issues concerning existence or feasibil- 
ity. Table 1 lists the types of research questions that are 
asked by software engineering research papers and 
provides specific question templates. Table 1. Types of software engineering research questions 
Type of question Examples 
Method or means of How can we do/create/modify/evolve (or automate doing) X? development What is a better way to do/create/modify/evolve X?

Basics of Software Engi neeri ng Experi ment ation 291 
  
Solution : The effect of experience and of the interaction 
is significant, whereas the effect of the technique is not; 
Technique I used by an experienced developer  
10.7.3. The effect of two modelling techni ques (-A, +A), used by experienced and 
inexperienced people (-B, +B), working in two different domains (-C, +C), 
on small-sized problems (-D, +D) is to be examined. Table 10.33 contains 
a measure of the effort put into gi ven development projects with these 
characteristics. What factors and interactions have significant effects?

Open reviews (e.g., OpenReviews.net) are post-publication reviews in
which the reviewer’s identity is usually known to the author. The International Conference
on Software Engineering conducts post-mortems of each year’s review process. Although
primarily focusing on the mechanism for deciding on paper acceptance, these reports also
explain types of papers accepted and what review information was important in making a
decision16. A recent series by Jacopo Soldani and colleagues in ACM SIGSOFT’s Software
Engineering Notes (Soldani et al. 2020 ) summarizes the “pains and gains” of peer review
and provides discussion about the concerns many scientists have with peer review, such as
workload. 6.2 PeerReviewCriteria
It is common for a peer review request to include some criteria by which the paper is to
be judged. These criteria are particularly important when the paper is a non-standard (i.e.,
non-technical research) format. For example, the Empirical Software Engineering Journal
allows for Technical Papers and Engineering in Practice papers. The Journal of Systems
and Software accepts technical papers, New Ideas and Trends, and In Practice papers. The
International Conference on Software Engineering (ICSE) has numerous tracks, including
Software in Society, Software Engineering in Practice, the main technical track, New Ideas
and Emerging Results, Software Engineering Education and Training, and more. Most conferences have the same or similar reviewing criteria for their research tracks,
typically summarized along the call for papers. The ICSE technical review criteria, for
instance, are available on the conference call for papers. For the 2021 edition, the criteria17
are
– Soundness: The extent to which the paper’s contributions are supported by rigorous
application of appropriate research methods. – Significance: The extent to which the paper’s contributions are important with respect
to open software engineering challenges. – Novelty: The extent to which the contribution is sufficiently original and is clearly
explained with respect to the state-of-the-art. – Verifiability: The extent to which the paper includes sufficient information to support
independent verification or replication of the paper’s claimed contributions. – Presentation: The extent to which the paper’s quality of writing meets the high standards
of ICSE [...]. Common to most review guidelines is a focus on presentation and clarity of the paper,
adherence to the conference or journal topics, and length. Increasingly guidelines are also
emphasizing the importance of constructive criticism (the Reviewer 2 problem (Peterson
16http://www.icse-conferences.org/reports.html
17https://conf.researchr.org/track/icse-2021/icse-2021-papers#Call-for-PapersEmpir Software Eng (2021) 26:  103 103Page26of29

Summary of previous reviews. Description of the software engineering technique being investigated and 
its potential importance. Review questions  Each review question should be 
specified.

56 IEEE SOFTWARE  |  WWW.COMPUTER.ORG/SOFTWARE   |  @IEEESOFTWARE
FOCUS:  SOFTWARE ENGINEERING’S 50TH ANNIVERSARYAs an engineering discipline, SE 
research should strive to impact 
practice. Favoring certain types of 
evidence over others will not suf -
fice. Instead, we require a frame -
work for aggregating the results of multiple pieces of work with differ -
ent types of evidence into action -
able practical feedback. In addition to encouraging technology trans -
fer and true research impact, such a framework can simultaneously open our field to accepting a wider variety of research, including re -
sults that constitute the less rigor -
ous (but still important!) codified knowledge that engineers use in practice. ABOUT THE AUTHORS
CLAIRE LE GOUES  is an assistant 
professor of computer science at Carnegie 
Mellon University’s Institute for Software 
Research. Her research interests lie in 
automatically reasoning about and improv -
ing software quality in real-world, evolving 
systems. Le Goues received a PhD in 
computer science from the University of 
Virginia. Contact her at clegoues@cs.cmu  
.edu, @clegoues. MARY SHAW is the Alan J. Perlis Uni -
versity Professor of Computer Science at Carnegie Mellon University’s Institute for 
Software Research. Her research focuses 
on software engineering and software de
-
sign, particularly software architecture and the design of systems used by real people. Shaw received a PhD in computer science 
from Carnegie Mellon University. She’s a 
Life Fellow of ACM and IEEE. Contact her at 
mary.shaw@cs.cmu.edu. CIERA JASPAN is a senior software engi -
neer at Google. She leads the Engineering Productivity Research team, which aims to 
identify inefficiencies in development tools 
and processes and improve the productiv
-
ity of Google engineers and engineers who use Google products. Jaspan received her 
PhD in software engineering from Carnegie 
Mellon University. Contact her at ciera@
google.com.

[19] R. Kwok, "Research impact: Altmetrics make their mark," 
Nature, vol. 500, pp. 491 -493, 2013. [20] C. Marshall, P. Brereton, and B. Kitchenham, "Tools to 
support systematic reviews in software engineering: a cross -domain 
survey using sem i-structured interviews," presented at the Proceedings 
of the 19th International Conference on Evaluation and Assessment in 
Software Engineering, Nanjing, China, 2015. [21] P. Runeson and M. Höst, "Guidelines for conducting and 
reporting case study research in software engineering," Empirical 
Software Engineering, vol. 14, pp. 131 -164, 2008. [22] A. Sampaio, "Improving Systematic Mapping Reviews," 
SIGSOFT Softw. Eng. Notes, vol. 40, pp. 1 -8, 2015. [23] M. Shaw, "What makes good research in software 
engineering?," J Software Tools for Technology Transfer STTT, vol. 4, 
pp. 1 -7, 2002. [24] M. Shaw, "Writing Good Software Engineering Research 
Papers," Proceedings of 25th Internat ional Conference on Software 
Engineering (ICSE'03), pp. 726 -736, 2003. [25] J. Singer and N. G. Vinson, "Ethical issues in empirical 
studies of software engineering," IEEE Transactions on Software 
Engineering, vol. 28, pp.

• Why should the reader care? • What larger question does this address? • What is your new result? • What new knowledge have you contributed that 
the reader can use elsewhere? • What previous work (yours or someone else's) 
do you build on? What do you provide a superior 
alternative to? • How is your result different from and better than 
this prior work? • What, precisely and in detail, is your new result? • Why should the reader believe your result? • What standard should be used to evaluate your 
claim? • What concrete evidence shows that your result 
satisfies your claim? If you answer these questions clearly, you'll probably 
communicate your result well. If in addition your result 
represents an interesting, sound, and significant contribu- 
tion to our knowledge of software engineering, you'll 
have a good chance of getting it accepted for publication 
in a conference or journal. Other fields of science and engineering have well- 
established research paradigms. For example, the 
experimental model of physics and the double-blind 
studies of medicines are understood, at least in broad 
outline, not only by the research community but also by 
the public at large. In addition to providing guidance for 
the design of research in a discipline, these paradigms 
establish the scope of scientific disciplines through a 
social and political process of "boundary setting" [5]. Software engineering, however, has not yet developed 
this sort of well-understood guidance. I previously [19, 
20] discussed early steps toward such understanding, 
including a model of the way software engineering 
techniques mature [17, 18] and critiques of the lack of 
rigor in experimental software engineering [1, 22, 23, 24, 
25]. Those discussions critique software engineering 
research reports against the standards of classical 
paradigms. The discussion here differs from those in that 
this discussion reports on the types of papers that are 
accepted in practices as good research reports. Another 
current activity, the Impact Project [7] seeks to trace the 
influence of software engineering research on practice. The discussion here focuses on the paradigms rather than 
the content of the research 
This report examines how software engineers answer 
the questions above, with emphasis on the design of the 
research project and the organization of the report. Other 
sources (e.g., [4]) deal with specific issues of technical 
writing. Very concretely, the examples here come from 
the papers submitted to ICSE 2002 and the program 
committee review of those papers. These examples report 
research results in software engineering. Conferences 
often include other kinds of papers, including experience 
reports, materials on software engineering education, and 
opinion essays. 0-7695-1877-X/03 $17.00 © 2003 IEEE 726 


Question: What criteria determine whether a research question is impactful or significant in Software Engineering?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

What is your new result? Explain precisely what you have contributed to the 
store of sot~ware engineering knowledge and how this is 
useful beyond your own project. oOOio -i, i 
80*/0 
60% 
40% 
20% -I 
Iii Dewl Question 
i:G i:~ii i , 
Analy Eval Gener Feas Total 
• Accepted [] Rejected 
Figure 2. Distribution of acceptances and rejections 
by type of research question 
3.1 What kinds of results do software engineers 
produce? The tangible contributions of software engineering 
research may be procedures or techniques for develop- 
ment or analysis; they may be models that generalize from 
specific examples, or they may be specific tools, solutions, 
or results about particular systems. Table 3 lists the types 
of research results that are reported in software engineer- 
ing research papers and provides specific examples. 3.2 Which of these are most common? By far the most common kind of ICSE paper reports a 
new procedure or technique for development or analysis. Models of various degrees of precision and formality were 
also common, with better success rates for quantitative 
than for qualitative models. Tools and notations were well 
represented, usually as auxiliary results in combination 
with a procedure or technique. Table 4 gives the distribu- 
tion of submissions to ICSE 2002, based on reading the 
abstracts (but not the papers), followed by graphs of the 
counts and distributions in Figures 3 and 4. 728 


Of course, one must have the resources to do research this way. 3 Realistic Tasks 
When conducting controlled experiments in software engineering, one should 
consider the realism and representativeness of the tasks regarding the size, complexity and duration of the involved tasks. Specifi cation, implementation and verification 
methods also vary considerably between domains, such as accounting software versus flight-control systems. In our opinion, some experimental tasks bear little resemblance to actual tasks in software engineering; others are very similar to actual tasks [32]. In between there is a continuum. Larger development tasks may take 
months, while many maintenance tasks may take only a couple of hours. 

103–112. Dyba, T., Kitchenham, B.A., Jorgensen, M., 2005. Evidence-based
software engineering for practitioners. IEEE Softw. 22 (1), 58–65. Fenton, N., 1993. How eﬀective are software engineering methods? J. Syst. Softw. 22 (2), 141–146. Fletcher, P., 1995. The role of experiments in computer science. J. Syst. Softw. 30 (1–2), 161–163. Glass, R.L., 1995. Structure-based critique of contemporary computing
research. J. Syst. Softw. 28 (1), 3–7. Glass, R.L., Vessey, I., Ramesh, V., 2002. Research in software engineer-
ing: an analysis of the literature. Information Softw. Technol. 44 (8),491–506. Glass, R.L., Ramesh, V., Vessey, I., 2004. An analysis of research in
computing disciplines. Commun. ACM 47 (6), 89–94. Goldberg, A., 1982. Editorial policy. ACM Comput. Surveys 14 (2), 151–
157. Haddow, G., Klobas, J., 2004. Communication of research to practice in
library and information science: closing the gap. Library Information
Sci.

•They interact with many other development 
techniques and procedures. For example a design method depends on a preceding requirements analysis. It must consider constraints imposed by the software and hardware platform and programming languages, timescales, and budget. It must be integrated with appropriate coding and testing techniques. Thus, it would be difficult to confirm that a design technique had a significant impact on final product reliability. In general, it is difficult to determine a causal link between a particular technique and a desired project outcome when the application of the technique and the final outcome are temporally removed from one another, and there are many other tasks and activities that could also affect the final outcome. •The immediate outcomes of a software engineering 
technique will not necessarily have a strong relationship with final project outcomes. E.g. if you 
are interested in the effect design techniques have on 
application reliability (i.e. probability of failure in a given time period under defined operational conditions), measures of the design product (or 
design process) have no obvious relationship with 
the desired outcome. There are no good surrogate 
measures of product reliability that can be measured 
at the end of the design process. There seem to be two major approaches to this issue: 1.

• What kinds of evidence can demonstrate the validity of a result, and how are 
good results distinguished from bad ones? In this paper I attempt to make generally accepted research strategies in software en-
gineering explicit by examining research in the area to identify what is widely ac-cepted in practice. 1.1   Software Technology Maturation 
Redwine and Riddle [5,6] reviewed a number of software technologies to see how 
they develop and propagate. They found that it typically takes 15-20 years for a tech-nology to evolve from concept formulation to the point where it's ready for populari-zation. They identify six typical phases: 
• Basic research . Investigate basic ideas and concepts, put initial structure on 
the problem, frame critical research questions. • Concept formulation . Circulate ideas informally, develop a research commu-
nity, converge on a compatible set of ideas, publish solutions to specific sub-problems. • Development and extension . Make preliminary use of the technology, clarify 
underlying ideas, generalize the approach. • Internal enhancement and exploration . Extend approach to another domain, 
use technology for real problems, stabilize technology, develop training mate-rials, show value in results. • External enhancement and exploration . Similar to internal, but involving a 
broader community of people who weren’t developers, show substantial evi-dence of value and applicability. • Popularization .

  12 outcomes should be specified. For example, in some cases we require interventions 
that improve some aspect of software  production without affecting another e.g. improved reliability with no increase in cost. A particular problem for soft ware engineering experiments is the widespread use of 
surrogate measures for example, defects found during system testing as a surrogate 
for quality, or coupling measures for de sign quality. Studies that use surrogate 
measures may be misleading and conclusi ons based on such studies may be less 
robust. Context 
For Software Engineering, this is the c ontext in which the co mparison takes place 
(e.g. academia or industry), the participants taking part in  the study (e.g.

have criticized common practice in the field for failing to collect, analyze, and report 
experimental measurements in research reports [9,10,11,12]. In 2001 I [8] presented preliminary sketches of some of the successful paradigms for software engineering research, drawing heavily on examples from software architecture. Scientific and engineering research fields can be characterized by identifying what 
they value: 
• What kinds of questions are "interesting"? • What kinds of results help to answer these questions, and what research 
methods can produce these results?

2   Questions, Results, and Validation in Software Engineering 
Generally speaking, software engineering researchers seek better ways to develop and 
evaluate software. They are motivated by practical problems, and key objectives of the research are often quality, cost, and timeliness of software products. This section presents a model that explains software engineering research papers by 
classifying the types of research questions they ask, the types of results they produce, 
and the character of the validation they provide. This model has evolved over several 
years.

4  Introduct ion 
  Confronted by a confusing array of op tions for producing software, software 
engineers need proof that a particular approach or technique is really better than 
another. They need to know the clear-cut be nefits of one approach versus another. They need reliable evidence that one appr oach clearly works better than another. This need to work with facts rather than assumptions is a property of any 
engineering discipline. Very few ideas in SE are matched with empirical data. Ideas, whose truthfulness has not been tested against reality, are continually assumed as evident. For example, the 
famous software crisis (Naur, 1969) was and still is more a question of a subjective 
customer and/or developer impression than a confirmed phenomenon. In fact, we can find some claims that there is no software crisis (see comments about Turski and 
Hoare opinions given in (Maibaum, 1997)). Another example is the idea that traditional engineering disciplines behave  much better than SE (Pezeé, 1997). For 
example, has any empirical study been pe rformed to compare the failure rate of 
products obtained by SE and those produced by other engineering disciplines? Table 
1.1 presents the percentage of faults in the car industry shown in a study carried out by Lex Vehicle Leasing, one of the major world vehicle hire and leasing companies, 
on a universe of 73,700 cars in England. This fault rate could call the reliability of 
the engineering used in this industry into question; however, this is not the case. Therefore, if we are really to  speak of a software crisis, we  need studies to show that 
SE is less reliable than other branches of engineering, that is, the number of faults in SE should be checked against the number of faults in other branches of engineering. Other examples of computer science theo ries that have not been tested are 
functional programming, object-oriented programming or formal methods.

 SEPTEMBER/OCTOBER 2018  |  IEEE SOFTWARE  51
imagining a dialogue between 
researchers and practitioners (see 
the sidebar). This pattern is common: engi -
neers often rely on their experience 
and a priori beliefs1 or turn to co -
workers for advice. This is better than guessing or giving up. But what 
if incompletely validated research outcomes could be distilled into re -
liable sources, intermediate between 
validated results and folk wisdom? To impact practice, SE research 
results must lead to pragmatic, ac -
tionable advice. This involves syn -
thesizing recommendations from results with different assumptions 
and levels of rigor, assigning appro -
priate levels of confidence to the rec -
ommendations. Here, we examine how these tradeoffs between rigor 
and pragmatism have been handled 
in medicine, where risk is often ac -
ceptable in the face of urgency. We propose an approach to describ -
ing SE research results with varied quality of evidence and synthesizing those results into codified knowledge 
for practitioners. This approach can 
both improve practice and increase the pace of research, especially in ex -
ploratory topics. Software Engineering 
Research Expectations over Time
When the 1968 NATO Conference 
introduced “software engineering” 
to our vocabulary,2 research often 
focused on designing and building programs. There were guidelines for writing programs; the concept of rea -
soning mathematically about a pro -
gram had just been introduced. The 
emphasis was on demonstrated capa -
bility—what we might now call feasi -
bility—rather than rigorous validation. This is visible in a sampling of 
major results of the period. For ex -
ample, Carnegie Mellon University identified a set of canonical papers published between 1968 and 2002. 3 
Several are formal analyses or em -
pirical studies, and a few are case studies. However, the majority are carefully reasoned essays that pro -
pose new approaches based on the authors’ experience and insight. The field has historically built 
on results with varying degrees of certainty. Indeed, Fred Brooks pro -
posed a “certainty-shell” structure RESEARCHERS AND PRACTITIONERS 
VOLLEY ABOUT MAKING RESEARCH USEFUL
Dear Practitioners:
The research community is actually discovering things you might find useful. Please help us organize this knowledge so that 
it’s actually useful to you.

Question: How do we define 'concrete' results in software engineering research?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

objectives, (b) demonstration of novelty and superiority
compared to related work, (c) evidence of eﬀectiveness,reproducibility, scalability, and practical relevance.Reviewers should also ascertain if papers meet quality cri-teria such as conciseness and technical precision, the con-sistent use of terminology, and the deﬁnition of keyconcepts. Focus and limitation of length must often beevaluated together, as the ACM T Database Syst R sug-
gests: ‘‘TODS would like to discourage excessively longpapers (longer than 50 doubled-spaced pages including ﬁg-ures, references, etc.) and unnecessary digressions even inshorter papers. This should help authors to focus on themost important aspects of their submission [ ...]”. For the
rest, practically the same quality criteria as for authorsare brought up, such as clarity, readability and the abilityto target as broad an audience as possible. Studying the abstracts of papers published at ICSE
2002, Shaw (2003) describes how accepted research papers
answered questions such as: What was your contribution?What is your new result? How is your result diﬀerent fromand better than prior work?, etc. According to Shaw, whenit comes to evaluating submitted papers, programme com-mittees look for interesting, novel, and exciting results. Inaddition, as SE knowledge grows incrementally, like in
other areas of science and engineering, programme com-
mittees are very interested in the authors’ interpretationof prior work. 3.2. Extended versions of conference papersA special type of articles are those based on one or
more papers previously presented at a workshop, a sym-
posium or a conference. For instance, van Vliet (2006)
is an extended version of van Vliet (2005) . In the study
previously mentioned ( Montesi and Mackenzie-Owen,
2008), comparing article genres in SE, education and
biology, it was found that extended versions of confer-ence papers played a role only in SE. A distinguishing
feature of extended papers is the double round of
reviews they need to pass before journal publication(i.e. conference and journal reviews). In a survey withauthors of extended versions and journal editors, Mon-
tesi and Mackenzie-Owen (accepted for publication)found that the extension process may change dependingon the type of conference paper to extend (empirical,theoretical, ...). Some journal instructions to authors regulate the pub-
lication of these articles. Among the other conditions, onethird of the content approximately must be new and ori-ginal, and sometimes authors must submit both versions,a description of the diﬀerences between the submittedmanuscript and the preliminary version, and referencethe previous version in the paper (SoSyM). Papers pub-lished as such must be ‘‘important ”(Theor Pract Log
Prog), ‘‘outstanding ”(ACM T Database Syst ), and ‘‘of
particularly high quality ”(J Comput Sci Technol ). New
material takes in explorations of further research, theo-rems, proofs and/or implementation details ( ACM T
Graphic ). If journals address reviewers the general tone doesn’t
change, though they oﬀer some more clues on what to lookfor. In the ACM T Database Syst -R , for instance, extended
versions of conference papers ‘‘should thoroughly consoli-date the material, should extend it to be broader, andshould more carefully cover related research ”. In addition,
the novel approach should be described more deeply, andalternatives should also be considered more comprehen-sively. In IEEE T Vis Comput Gr -R , while no new results
are required, key ideas, examples, and further elaborationare expected. This conﬁrms Shaw’s observation that in con-
ferences time and space limitations do not allow authors to
explore all the possible results of their research ( Shaw,
2003).Table 2
Excerpt from the instructions of IEE Proc-Softw
Originality: is the work scientiﬁcally rigorous, accurate and novel? Does the work contain signiﬁcant additional material to that already published?

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

• Papers not subject to peer-review. When an SLR has been published in more than one journal/conference, the most 
complete version of the survey will be used. Primary study selection process 
 
The results will be tabulated as follows: 
• Number of papers per year per source 
• Number of candidate pape rs per year per source 
• Number of selected papers per year per source. The relevant candidate and selected studies will be selected by a single researcher. The rejected studies will be checked by a nother researcher. We will maintain a list 
candidate papers that were rejected  with reasons for the rejection. Quality Assessment 
 Each SLR will be evaluated using the York University, Centre for Reviews and Dissemination (CDR) Database of Abstracts of Reviews of Effects (DARE) criteria 
(http://www.york.ac.uk/inst/crd/crddatabase .htm#DARE). The criteria are based on 
four questions: 
 
• Are the review’s inclusion and exclusi on criteria describe d and appropriate? • Is the literature search likely to have covered all relevant studies? • Did the reviewers assess the quality/v alidity of the included studies? • Were the basic data/studies adequately described? The questions are scored as follows:  
• Question 1: Y (yes), the inclusion criteria  are explicitly defined in the paper, P 
(Partly), the inclusion criteria are implic it; N (no), the inclusion criteria are not 
defined and cannot be readily inferred. • Question 2: Y, the authors have either s earched 4 or more digital libraries and 
included additional search strategies or identified and referenced all journals 
addressing the topic of interest; P, the authors have searched 3 or 4 digital 
libraries with no extra search strategies, or searched a define d but restricted set 
of journals and conference proceedings;  N, the authors have search up to 2 
digital libraries or an extremel y restricted set of journals. • Question 3: Y, the authors have explicit ly defined quality criteria and extracted 
them from each primary study; P, the re search question involves quality issues 
that are addressed by the study; N no e xplicit quality assessm ent of individual 
papers has been attempted. 

Has its value been demonstrated? Relevance: is the material appropriate to the scope of the journal to which it is submitted? Motivation: does the problem considered have a sound motivation? Does the paper clearly demonstrate the scientiﬁc interest of the results? Referencing: has reference been made to the most recent and most appropriate work? Is the present work set in the context of the previous work? Clarity: is the English clear and well written? Poorly written English may obscure the scientiﬁc merit of your paper. Are the ideas expressed clearly
and concisely?

Characteristicsofhighqualitypapers Table 4shows the most common results from our
coding of the free-text responses to this question. Overall, the most common characteris-
tics reviewers want in a high quality paper relate to aspects of the quality of the research
methodology and how well it is presented in the paper. It is also interesting to note which
characteristics were not among the most commonly given answers, including the availability
of the data (5), the findings themselves (1), and the generalizability of the results (1). Characteristicsleadingtonegativereviews Figure 11shows the ranking of the charac-
teristics that tend to lead to negative reviews. The highest ranked characteristics include (1)
a mismatch of methods with claims raised in the contribution, (2) overly grandiose claims,
(3) a writing style that is hard to follow, (4) a research methodology which is difficultEmpir Software Eng (2021) 26:  103 103Page14of29

Tools that originated from a new technique also described in the 
paper would fall under this category. • “Specific Solution, Prototype, Answer, or Jud gment” covers many 
different types of results, including analysis of existing systems or 
specific analysis of a technique. • •“Empirical Report” covers papers that focus on a set of 
observations, either quantitative or qualitative, and could 
potentially incl ude a general prescription of guidelines based on 
the data in the report. The “Empirical Report” category includes 
the presentation of data or insights typically found in MSR papers. “Empirical Report” is a modification of the original classification scheme, which was called “Report”. The new name emphasizes 
the type of contribution that results of MSR papers.

3   Research Strategies 
Section 2 identifies the three important aspects of an individual research result as 
reported in a typical conference or journal paper. It is clear that the spectrum of good research strategies includes experimental computer science in the sense of [9,10,11, 12]; it is also clear that the spectrum is much broader than just experimental research. Of course, not all the combinations of question, result, and validation make sense.

How can one judgeit? It is very diﬃcult to give a satisfactory and deﬁnitiveanswer to such questions without the consensus of a greatpart of the community, which requires time and greateﬀort. A possible alternative is to summarize and organizethe written information currently available on the matter.And this is precisely the aim of this study. 2. Methodology
The present description of the major types of research
papers published in SE is based exclusively on existing writ-
ten sources, and in particular on the following:(1) Published, relevant papers, such as, for example,
Zelkowitz and Wallace (1998) orSjøberg et al. (2005) .

Use them consistently. The more formal or analytic the paper, the more important 
this is. Here are some questions that the program committee 
may ask about your paper: 
What, precisely, do you claim to contribute? Does your result fiJlly satisfy your claims? Are the 
definitions precise, and are terms used consistently? Authors tend to have trouble in some specific 
situations. Here are some examples, with advice for 
staying out of trouble:" 
• If your result ought to work on large systems, explain 
why you believe it scales. • If you claim your method is "automatic", using it 
should not require human intervention. If it's 
automatic when it's operating but requires manual 
assistance to configure, say so. If it's automatic 
except for certain eases, say so, and say how often 
the exceptions occur. • If you claim your result is "distributed", it probably 
should not have a single central controller or server. If it does, explain what part of it is distributed and 
what part is not. • If you're proposing a new notation for an old 
problem, explain why your notation is clearly 
superior to the old one. • If your paper is an "experience report", relating the 
use of a previously-reported tool or technique in a 
practical software project, be sure that you explain 
what idea the reader can take away from the paper to use in other settings. If that idea is increased 
confidence in the tool or technique, show how your 
experience should increase the reader's confidence 
for applications beyond the example of the paper. What ~ new here? The program committee wants to know what is novel 
or exciting, and why. What, specifically, is the 
contribution? What is the increment over earlier work by 
the same authors? by other authors? Is this a sufficient 
increment, given the usual standards of subdiscipline? Above all, the program committee also wants to know 
what you actually contributed to our store of knowledge 
about software engineering. Sure, you wrote this tool and 
tried it out. But was your contribution the technique that is 
embedded in the tool, or was it making a tool that's more 
effective than other tools that implement the technique, or 
was it showing that the tool you described in a previous 
paper actually worked on a practical large-scale problem? It's better for you as the author to explain than for the 
program committee to guess. Be clear about your claim ... Awful • • I completely and generally solved ... I (unless you actually did!) 
Bad • • I worked on galumphing. (or studied, investigated, sought, 
explored) 
Poor • • I worked on improving galumphing. (or contributed to, participated in, 
helped with) 
Good • • I showed the feasibility of composing 
blitzing with flitzing. i • I significantly improved the accuracy of 
the standard detector. (or proved, demonstrated, created, 
established, found, developed) 
Better • • I automated the production of ritz 
tables from specifications. • With a novel application of the blivet 
transform, I achieved a 10% increase 
in speed and a 15% improvement in 
coverage over the standard method. Use verbs that show results and achievement, not just 
effort and activity.

Examples 
Kitchenham et al. used the following inclusion criteria: 
• any study that compared predictions of cross-company models with within-
company models based on analysis of single company project data. They used the following exclusion criteria: 
• studies where projects were only collect ed from a small number of different sources 
(e.g. 2 or 3 companies),  
• studies where models derived from a with in-company data set were compared with 
predictions from a general cost estimation model. Jørgensen 206H[17] included papers that compare j udgment-based and model-based software 
development effort estimation. He also ex cluded one relevant paper due to “incomplete 
information about how the estimates were derived”. Issues: 
• Medical standards make a point that it is important to avoid, as far as possible, 
exclusions based on the language of th e primary study. This may not be so 
important for Software Engineering. • It is possible that inclus ion decisions could be affected by knowledge of the 
authors, institutions, journals or year of publication. Some medical researchers 
have suggested reviews should be done af ter such information has been removed. However, it takes time to do this and experimental evidence suggests that 
masking the origin of primary studies does not improve reviews 207H[4]. 6.2.2 Study selection process 
Study selection is a multistage process. Initially, selection criteria should be 
interpreted liberally, so that unless a study identifie d by the electronic and hand 
searches can be clearly excluded based on title and abstract, a full copy should be 
obtained. However, Brereton et al. 208H[5] point out that  “The standard of IT and software 
engineering abstracts is too poor to rely  on when selecting primary studies. You 
should also review the conclusions.” 
 
The next step is to apply inclusion/excl usion criteria based on practical issues 209H[11] 
such as: 
• Language 
• Journal 
• Authors 
• Setting 
• Participants or subjects 
• Research Design 
• Sampling method 
• Date of publication. Staples and Niazi point out th at it is sometimes necessary to consider the questions 
that are not being addressed in orde r to refine your exclusion criteria 210H[27]. Example 
Staples and Niazi’s research question was 

Question: What are the criteria for 'specific' results in academic papers?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

methodology in software engineering has a strong focus on experimental research; the
earliest by Moher and Schneider ( 1981 ), Basili et al. ( 1986 ), the first methodology
handbook by Wohlin et al. ( 2000 ), and promoted by Tichy ( 1998 ). All have a tendency
towards quantitative approaches, although also qualitative approaches are discussed duringthe later years, e.g. by Seaman ( 1999 ). There exist guidelines for experiments ’conduct
(Kitchenham et al. 2002 ; Wohlin et al. 2000 ) and reporting (Jedlitschka and Pfahl 2005 ),
measurements (Basili and Weiss 1984 ; Fenton and Pfleeger 1996 ; van Solingen and
Berghout 1999 ), and systematic reviews (Kitchenham 2007 ), while only little is written on
case studies in software engineering (Höst and Runeson 2007 ; Kitchenham et al. 1995 ;
Wohlin et al.

2. What, precisely, was your contribution? Before reporting what you did, explain what problem 
you set out to solve or what question you set out to answer 
--and why this is important. 2.1 What kinds of questions do software 
engineers investigate? Generally speaking, software engineering researchers 
seek better ways to develop and evaluate sottware. Devel- 
opment includes all the synthetic activities that involve 
creating and modifying the software, including the code, 
design documents, documentation, etc. Evaluation includes all the analytic activities associated with predict- 
ing, determining, and estimating properties of the software 
systems, including both functionality and extra-functional 
properties such as performance or reliability. Software engineering research answers questions about 
methods of development or analysis, about details of 
designing or evaluating a particular instance, about gener- 
alizations over whole classes of systems or techniques, or 
about exploratory issues concerning existence or feasibil- 
ity. Table 1 lists the types of research questions that are 
asked by software engineering research papers and 
provides specific question templates. Table 1. Types of software engineering research questions 
Type of question Examples 
Method or means of How can we do/create/modify/evolve (or automate doing) X? development What is a better way to do/create/modify/evolve X?

have criticized common practice in the field for failing to collect, analyze, and report 
experimental measurements in research reports [9,10,11,12]. In 2001 I [8] presented preliminary sketches of some of the successful paradigms for software engineering research, drawing heavily on examples from software architecture. Scientific and engineering research fields can be characterized by identifying what 
they value: 
• What kinds of questions are "interesting"? • What kinds of results help to answer these questions, and what research 
methods can produce these results?

Chapter 11
Selecting Empirical Methods 
for Software Engineering Research
Steve Easterbrook, Janice Singer, Margaret-Anne Storey, 
and Daniela Damian
Abstract  Selecting a research method for empirical software engineering research 
is problematic because the benefits and challenges to using each method are not yet 
well catalogued. Therefore, this chapter describes a number of empirical methods 
available. It examines the goals of each and analyzes the types of questions each 
best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understand-ing and selecting from the variety of methods applicable to empirical software 
engineering. 1. Introduction
Despite widespread interest in empirical software engineering, there is little guid-
ance on which research methods are suitable to which research problems, and how to choose amongst them. Many researchers select inappropriate methods because 
they do not understand the goals underlying a method or possess little knowledge 
about alternatives. As a first step in helping researchers select an appropriate method, this chapter discusses key questions to consider in selecting a method, from philosophical considerations about the nature of knowledge to practical con-siderations in the application of the method. We characterize key empirical methods 
applicable to empirical software engineering, and explain the strengths and weak nesses 
of each. Software engineering is a multi-disciplinary field, crossing many social and 
technological boundaries. To understand how software engineers construct and maintain complex, evolving software systems, we need to investigate not just the tools and processes they use, but also the social and cognitive processes surround-
ing them. This requires the study of human activities. We need to understand how 
individual software engineers develop software, as well as how teams and organiza tions 
coordinate their efforts. 285
F.

• What kinds of evidence can demonstrate the validity of a result, and how are 
good results distinguished from bad ones? In this paper I attempt to make generally accepted research strategies in software en-
gineering explicit by examining research in the area to identify what is widely ac-cepted in practice. 1.1   Software Technology Maturation 
Redwine and Riddle [5,6] reviewed a number of software technologies to see how 
they develop and propagate. They found that it typically takes 15-20 years for a tech-nology to evolve from concept formulation to the point where it's ready for populari-zation. They identify six typical phases: 
• Basic research . Investigate basic ideas and concepts, put initial structure on 
the problem, frame critical research questions. • Concept formulation . Circulate ideas informally, develop a research commu-
nity, converge on a compatible set of ideas, publish solutions to specific sub-problems. • Development and extension . Make preliminary use of the technology, clarify 
underlying ideas, generalize the approach. • Internal enhancement and exploration . Extend approach to another domain, 
use technology for real problems, stabilize technology, develop training mate-rials, show value in results. • External enhancement and exploration . Similar to internal, but involving a 
broader community of people who weren’t developers, show substantial evi-dence of value and applicability. • Popularization .

What Makes Good Research in Software Engineering? Mary Shaw 
School of Computer Science, Carnegie Mellon University, 
Pittsburgh PA 15213 USA 
mary.shaw@cs.cmu.edu
http://www.cs.cmu.edu/~shaw/
Abstract. Physics, biology, and medicine have well-refined public explanations 
of their research processes. Even in simplified form, these provide guidance 
about what counts as "good research" both inside and outside the field. Soft-
ware engineering has not yet explicitly identified and explained either our re-
search processes or the ways we recognize excellent work. Science and engineering research fields can be characterized in terms of the kinds of questions they find worth investigating, the research methods they 
adopt, and the criteria by which they evaluate their results. I will present such a 
characterization for software engineering, showing the diversity of research strategies and the way they shift as ideas mature. Understanding these strategies 
should help software engineers design research plans and report the results 
clearly; it should also help explain the character of software engineering re-search to computer science at large and to other scientists. 1   Introduction 
Many sciences have good explanations of their research strategies. These explanations 
include not only detailed guidance for researchers but also simplified views for the 
public and other observers. Acceptance of their results relies on the process of obtain-ing the results as well as analysis of the results themselves. Schoolchildren learn the experimental model of physics: hypothesis, controlled experiment, analysis, and pos-sible refutation. The public understands large-scale double-blind medical studies well enough to discuss the risks of experimental treatment, the ethics of withholding prom-ising treatment from the control group, and the conflicts of interest that are addressed by the blinding process. Software engineering does not have this sort of well-understood guidance. Software 
engineering researchers rarely write explicitly about their paradigms of research and their standards for judging quality of results. A number of attempts to characterize software engineering research have contributed elements of the answer, but they do not yet paint a comprehensive picture. In 1980, I [7] examined the relation of engi-neering disciplines to their underlying craft and technology and laid out expectations 
for an engineering discipline for software. In 1984-5, Redwine, Riddle, and others [5,6] proposed a model for the way software engineering technology evolves from 
research ideas to widespread practice. More recently, software engineering researchers 
International Journal of Software Tools for Technology Transfer, 2002, vol. 4, no.

Empirical Research Methods in Software Engineering         23 
 References 
[1] D. A. Garvin, “Building a Learning Organization”, in Harward Business Review on 
Knowledge Management, pp. 47–80, Harward Business School Press, Boston, USA, 1998. [2] V. R. Basili, G. Caldiera, and H. D. Rombach, ”E xperience Factory” in Encyclopaedia of 
Software Engineering, editor John J. Marciniak, John Wiley & Sons, Inc., Hoboken, N.J., USA, 2002. [3] N. K. Denzin and Y. S. Lincoln, Handbook of Qualitative Research, Sage Publications, 
London, UK, 1994. [4] J. W. Creswell, Research Design, Qua litative and Quantitative Approaches, Sage 
Publications, 1994. [5] N. Fenton, and S. L. Pfleeger, Software Metrics: A Rigorous & Practical Approach, 2nd 
edition, International Thomson Computer Press, 1996. [6] B.

Software measurement forms the basis, but it is not sufficient. Empirical methods 
such as controlled experiments, case studies, surveys and post-mortem analyses are 
needed to help us evaluate and validate the research results. These methods are needed so that it is possible to scientifically state whether something is better than 
something else. Thus, empirical methods provide one important scientific basis for 
software engineering. For some type of pr oblems other methods, for example the use 
of mathematical models for predicting software reliability, is better suited, but in most cases the best method is applying empirici sm. The main reason being that software 

2003 ) and qualitative methods (Dittrich 2007 ; Seaman 1999 ; Sim et al. 2001 ). Recently, a comprehensive view of empirical research issues for software engineering hasbeen presented, edited by Shull et al. ( 2008 ). The term “case study ”appears every now and then in the title of software engineering
research papers. However, the presented studies range from very ambitious and wellorganized studies in the field, to small toy examples that claim to be case studies.Additionally, there are different taxonomies used to classify research. The term case study is
used in parallel with terms like field study and observational study, each focusing on a
particular aspect of the research methodology. For example, Lethbridge et al. use field
studies as the most general term (Lethbridge et al. 2005 ), while Easterbrook et al. ( 2008 )
callcase studies one of five “classes of research methods ”. Zelkowitz and Wallace propose
a terminology that is somewhat different from what is used in other fields, and categorizeproject monitoring, case study and field study as observational methods (Zelkowitz and
Wallace 1998 ). This plethora of terms causes confusion and problems when trying to
aggregate multiple empirical studies. The case study methodology is well suited for many kinds of software engineering
research, as the objects of study are contemporary phenomena, which are hard to study inisolation. Case studies do not generate the same results on e.g.

Easterbrook et al. Because of the importance of human activities in software development, many 
of the research methods that are appropriate to software engineering are drawn 
from disciplines that study human behaviour, both at the individual level (e.g. psy-
chology) and at the team and organizational levels (e.g. sociology).These methods 
all have known flaws, and each can only provide limited, qualified evidence about the phenomena being studied. However, each method is flawed differently (McGrath, 1995) and viable research strategies use multiple methods, chosen in such a way that the weaknesses of each method are addressed by use of comple-
mentary methods (Creswell, 2002). Describing in detail the wide variety of possible empirical methods and how to 
apply them is beyond the scope of the chapter. Instead, we identify and compare five classes of research method that we believe are most relevant to software engineering:
● Controlled Experiments  (including Quasi-Experiments )
● Case Studies  (both exploratory  and confirmatory )
● Survey Research
● Ethnographies
● Action Research
We describe the tradeoffs involved in choosing between these methods, but do not 
provide a recipe for building research strategies, as we doubt that such recipes exist. The selection of methods for a given research project depends on many local con-tingencies, including available resources, access to subjects, opportunity to control the variables of interest, and, of course, the skills of the researcher. To illustrate the steps involved in deciding which method or methods to use, we 
present two guiding examples. Two fictional software engineering researchers, Joe and 
Jane, will explore how the various research methods can be applied to their work:
● Jane is a new PhD student interested in the effectiveness of a novel fisheye-view 
file navigator. Her research is motivated by the fact that navigation is a primary 
activity of software developers requiring a lot of scrolling and many clicks to 
find files. “Fisheye-views” use a distortion technique that, if applied correctly, 
display information in a compact format that could potentially reduce the amount of scrolling required. Jane’s intuition is that the fisheye-view file navi-gator is more efficient for file navigation, but critics argue that the more compact information is difficult to read and that developers will not adopt it over the tra-
ditional file navigator. Her research goal, therefore, is to find evidence that sup-ports or refutes her intuition that fisheye-view file navigators are more efficient than traditional file navigators for navigation. ● Joe is a researcher in an industrial lab. His current interests are in understanding 
how developers in industry use (or not) UML diagrams during software design. This is because, as a student, his professors recommended UML diagrams be used 
during software design, but his recent exposure to industrial practices indi cates 
that UML is rarely used. His research goal is to explore how widely UML 

Question: What kinds of 'methodology' are there in software engineering research?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

What kind of Research Question are You Asking? One of the first steps in choosing an appropriate research method is to clarify the 
research question. While Jane and Joe have identified the problems they wish to work on, neither has pinned down a precise question. In each case, they could focus 
on a number of different research questions, each of which leads to a different 
direction in developing research strategies. The classification of research questions we use in this section is adapted from Meltzoff (1998). Often, the most obvious question is not the best choice for a starting point. Jane’s first attempt to formulate her research question is “Is a fisheye-view file 
navigator more efficient than the traditional view for file navigation?” , while Joe 
asks “how widely are UML diagrams used as collaborative shared artifacts during 
design?” . Both questions are vague, because they make assumptions about the 
phenomena to be studied, and kinds of situation in which these phenomena occur. For example, Jane’s question only makes sense if we already know that some people (who?) need to do file navigation (whatever that is?), under some circum-
stances (which are?), and that efficiency (measured how?) is a relevant goal for 
these people (how would we know that?). Joe’s question presupposes that we know what a “collaborative shared artifact” is, and can reliably identify one, and even reliably say which things are UML diagrams. Defining the precise meaning of terms is a crucial part of empirical research, and is closely tied with the idea of developing (or selecting) an appropriate theory . In the early stages of a research program, we usually need to ask exploratory  
questions, as we attempt to understand the phenomena, and identify useful distinctions that clarify our understanding. Suitable research methods for explor-atory questions tend to be those that offer rich, qualitative data, which help us to build tentative theories. Unless they are building on existing work that already offers clear definitions, both Jane and Joe need to formulate exploratory ques-tions, such as:

There seems to be a balance between the need for relevance and scientific rigor, which
has also been a perennial discussion in conferences with industry tracks or sessions. It is
Table 2 How Reviewers assess the quality and/or validity of a paper’s research method (showing only
selected codes with at least five responses)
Response Code Explanation Count
Rigor Assess rigor, validity of methodology relative to published literature or
standard practice57
Appropriateness Is the method appropriate 27
Detail Present appropriate level of detail; Transparent process 20
Validity Discussion of threats to validity 18
Conclusions Do the conclusions follow from the evidence 10
Reproducible Determine if the study is reproducible / replicable 10
Support material Includes support materials (e.g. tools, instruments, scripts, ...) 8
Knowledge Use own knowledge to judge validity 8
Data availability Availability of data 5
Sample Valid sample 5Empir Software Eng (2021) 26:  103 Page 11 of 29     103

It may be a complementary approach. However, it is important to be aware of the difference and also to consider how this affects the statistical analysis, si nce most statistical methods have developed 

12. Are multiple data sources and collection methods planned (triangulation)? 13. Are measurement instruments and procedures well defined (measurement definitions, interview
questions)? 14. Are the planned methods and measurements sufficient to fulfill the objective of the study?15. Is the study design approved by a review board, and has informed consent obtained from individuals and
organizations?Empir Software Eng (2009) 14:131 –164 149

How can one judgeit? It is very diﬃcult to give a satisfactory and deﬁnitiveanswer to such questions without the consensus of a greatpart of the community, which requires time and greateﬀort. A possible alternative is to summarize and organizethe written information currently available on the matter.And this is precisely the aim of this study. 2. Methodology
The present description of the major types of research
papers published in SE is based exclusively on existing writ-
ten sources, and in particular on the following:(1) Published, relevant papers, such as, for example,
Zelkowitz and Wallace (1998) orSjøberg et al. (2005) .

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

B. Conclusion
Existing guidelines offer various and limited sets of different
research approaches, which are of varying levels of granularity
(case study v. exploratory/evaluative case study; experiment v. controlled/quasi experiment, etc.), and thus represent a “mixed
bag” of methods. We have adopted a framework from the
social sciences, which provides a holistic overview of different
research strategies. While existing overviews provide overviews
of different methods, they do not systematically explain how
these methods relate to one another. The framework in Fig. 1
positions a set of eight different research strategies along two
dimensions: the level of ‘obtrusiveness’ (i.e., control by the
researcher), and the level of generalizability. We illustrate
each of these strategies with examples from the software
engineering literature which have been published in the ﬁeld’s
top outlets—an indication that these strategies are already
prominent, merely lacking a common terminology. These
strategies can be considered to be the architectures of research
designs, which have associated weaknesses and strengths due
to their relation with the level of the researcher’s ‘control’
(the obtrusiveness dimension) and the level of generalizability

8. Practical Considerations
In addition to the question of how well the methods fit a given type of research 
question and philosophical stance, the choice of methods also depends on prac-tical considerations. Often these practical considerations force the researcher to 
change the original research design in terms of the choice of method, data 
collection and analysis procedures. It is important to document the original planned research protocol, and all subsequent deviations to it, to allow other researchers to understand the study design, interpret the research results, and replicate the study. Most of the practical challenges relate to time, budget and personnel resources, 
and access to data. Rather than describe the challenges for each method individu-
ally, we summarize the challenges related to groups of methods, according to the type of data they deal with:
Methods that are primarily qualitative  include ethnography, case study, and action 
research. These methods rely on fieldwork, using techniques such as participant 
observation and interviews. Key challenges include preparing good questions for 

This document does  not go to this level of detail. The critical issue in any systematic review is  to ask the right question. In this context, 
the right question is usually one that: 
• Is meaningful and important to practitione rs as well as researchers. For example, 
researchers might be interested in whether a specific analysis technique leads to a 
significantly more accurate estimate of re maining defects after design inspections. However, a practitioner might want to know whether adopting a specific analysis 
technique to predict remaining defects is more effective than expert opinion at 
identifying design documents that require re-inspection. • Will lead either to changes in current software engineering practice or to 
increased confidence in the value of curre nt practice. For example, researchers 
and practitioners would like  to know under what conditi ons a project can safely 
adopt agile technologies and under what conditions it should not. • Will identify discrepancies between commonly held beliefs and reality. Nonetheless, there are systematic reviews that ask questions that are primarily of 
interest to researchers. Such  reviews ask questions that identify and/or scope future 
research activities. For example, a systema tic review in a PhD thesis should identify 
the existing basis for the research stud ent’s work and make it clear where the 
proposed research fits into the current body of knowledge. Examples 
 
Kitchenham et al. 190H[21] had three research questions: 
Question 1: What evidence is there that cross-company estimation models are not 
significantly different from within-company estimation models for predicting effort for 
software/Web projects? Question 2: What characteristics of the study data sets and the data analysis methods used in 
the study affect the outcome of within- a nd cross-company effort estimation accuracy 
studies? Question 3: Which experimental procedure is most appropriate for studies comparing within- 
and cross-company estimation models? Jørgensen 191H[17] had two research questions: 
1. Should we expect more accurate effort estimates when applying expert judgment or 
models? 2. When should software development effort estimates be based on expert judgment, 
when on models, and when on a combination of expert judgment and models? In both cases, the authors were aw are from previous research that  results were mixed, so in 
each case they added a question aimed at invest igating the conditions under which different 
results are obtained. 5.3.2 Question Structure 
Medical guidelines recommend considering a question about the effectiveness of a 
treatment from three viewpoints:  
• The population, i.e. the people affected by the intervention.

Easterbrook et al. program involves a mix of design questions and knowledge questions as the 
researchers investigate specific problems, how best to solve them, and which solu-tions work best (Wieringa and Heerkens, 2006). 3. What will You Accept as an Empirical Truth? Having specified the research question(s), it is worth considering what to accept as valid  answers.

Question: How is a methodology viewed as appropriate for a specific research question?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m
[38;5;200m[1;3mcontent='```\nManuscript Text: We answer our RQ by presenting a taxonomy of the types of documentation issues found in our analysis. Such a taxonomy was defined in an open discussion involving all the authors and aimed at merging similar labels and hierarchically organizing them. We focus our qualitative analysis on specific categories of issue types. For each category, we present interesting examples and common solutions, and discuss implications for researchers and practitioners.\n```\n```\nReview: \nIn the research question, the authors claim to contribute a taxonomy of the types of documentation issues identified in their analysis. This taxonomy was developed through collaboration among the authors to merge similar labels and organize them hierarchically. The authors focus on specific categories of issue types, provide examples, common solutions, and discuss implications for researchers and practitioners. The contribution is clearly outlined in terms of organizing and presenting the identified documentation issues.\n```\n\n```\nManuscript Text: We focus our qualitative analysis on specific categories of issue types. For each category, we present interesting examples and common solutions, and discuss implications for researchers and practitioners.\n```\n```\nReview:\nThe authors provide concrete and specific results by focusing their qualitative analysis on specific categories of issue types. By presenting interesting examples, common solutions, and discussing implications for both researchers and practitioners, the results are tangible and directly applicable to the field of software engineering. This specificity enhances the practical relevance of the research findings.\n```\n\n```\nManuscript Text: Such a taxonomy was defined in an open discussion involving all the authors and aimed at merging similar labels and hierarchically organizing them.\n```\n```\nReview:\nThe methodology used in the research involved an open discussion among all authors to define the taxonomy of documentation issues. This collaborative approach ensures a comprehensive and diverse perspective in merging similar labels and organizing them hierarchically. By involving all authors in this process, the methodology enhances the credibility and reliability of the taxonomy developed in the study.\n```' response_metadata={'token_usage': {'completion_tokens': 389, 'prompt_tokens': 1167, 'total_tokens': 1556}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-eef21dac-7015-4b2a-aef3-e8c0a36f18e6-0'[0m[32;1m[1;3mAction: Fetch Section Content by Title
Action Input: {"section_title": "C. Replication Package", "path": "/Users/crystalalice/Desktop/ICSHP_Research/SE_paper/Software_Documentation_Issues_Unveiled.pdf"}[0m[33;1m[1;3mAll the data used in our study is publicly available [56].
[0m[32;1m[1;3mAction: Generate Review
Action Input: All the data used in our study is publicly available.[0m

[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

• What kinds of evidence can demonstrate the validity of a result, and how are 
good results distinguished from bad ones? In this paper I attempt to make generally accepted research strategies in software en-
gineering explicit by examining research in the area to identify what is widely ac-cepted in practice. 1.1   Software Technology Maturation 
Redwine and Riddle [5,6] reviewed a number of software technologies to see how 
they develop and propagate. They found that it typically takes 15-20 years for a tech-nology to evolve from concept formulation to the point where it's ready for populari-zation. They identify six typical phases: 
• Basic research . Investigate basic ideas and concepts, put initial structure on 
the problem, frame critical research questions. • Concept formulation . Circulate ideas informally, develop a research commu-
nity, converge on a compatible set of ideas, publish solutions to specific sub-problems. • Development and extension . Make preliminary use of the technology, clarify 
underlying ideas, generalize the approach. • Internal enhancement and exploration . Extend approach to another domain, 
use technology for real problems, stabilize technology, develop training mate-rials, show value in results. • External enhancement and exploration . Similar to internal, but involving a 
broader community of people who weren’t developers, show substantial evi-dence of value and applicability. • Popularization .

H. Erdogmus, “How Important Is Evidence, Really?,” IEEE Software , 
vol. 27, no. 3, 2010, pp. 2–5. 12. B.A. Kitchenham, D. Budgen, and P. Brereton, Evidence-Based Software Engineering and Systematic Reviews , 
Chapman & Hall, 2015. 13.

Criminology, Social policy, Economics, Nursing etc. Based on Evidence-based 
medicine, the goal of Evidence-based Software Engineering is: 
“To provide the means by which current best evidence from research can be integrated with practical experience a nd human values in the decision making 
process regarding the devel opment and maintenance of software.” (Dybå et al., 
2005) 
 In this context evidence is defined as a synt hesis of best quality scientific studies on a 
specific topic or research que stion. The main method of synthesis is a Systematic 
Literature Review (SLR). In contrast to an ad hoc literature review, an SLR is a 
methodologically rigorous revi ew of research results. Research Questions 
 
The research questions to be  addressed by this study are: 
• How much EBSE activity ha s there been since 2004? • What research topics are being addressed? • Who is leading EBSE research? 

Technol. 40 (14), 811–821. Posteguillo, S., 1999. The schematic structure of computer science research
articles. English Speciﬁc Purposes 18 (2). Punter, T., 2003. What information do software engineering practitioners
need? In: Proceedings of Second Workshop on Empirical Studies inSoftware Engineering. Rome, Italy. Segal, J., 2005. When software engineers met research scientists: A case
study. Empirical Softw. Eng. 10 (4), 517–536. Segal, J., Grinyer, A., Sharp, H., 2005. The type of evidence produced by
empirical software engineers. In: Proceedings of the 2005 Workshop on
Realising Evidence-based Software Engineering (REBSE). ACM
Press, New York, NY, USA, pp.

Basili et al. performed early work describing 
standards for experimentation for software engineering researchers [2]. Kitchenham’s guidelines for evidence -based software engineering 
research [18] and systematic literature reviews in software engineering 
[16] are widely cited, along with her review of the state of systematic 
literature reviews in software engineering [15]. Marshall et al.

FOCUS:  SOFTWARE ENGINEERING’S 50TH ANNIVERSARY50 IEEE SOFTWARE  |  PUBLISHED BY THE IEEE COMPUTER SOCIETY  0740-7459/18/$33.00 © 2018 IEEEBridging the 
Gap
From Research to Practical 
Advice
Claire Le Goues, Carnegie Mellon University
Ciera Jaspan,  Google
Ipek Ozkaya and Mary Shaw, Carnegie Mellon University
Kathryn T. Stolee, North Carolina State University
// Software developers need actionable 
guidance, but researchers rarely integrate 
diverse types of evidence in a way that indicates 
the recommendations’ strength. A levels-of-
evidence framework might allow researchers 
and practitioners to translate research 
results to a pragmatically useful form. //
SOFTWARE ENGINEERING (SE) 
seeks cost-effective solutions to practi -
cal problems by applying well-codified, 
ideally scientifically validated, knowl -
edge. Scientific validation takes many 
forms, such as experiments, statistics, formal proofs, or some combination thereof; these provide differing degrees of confidence in the knowledge. Emily, a software engineer at 
a mid-sized company, recently 
oversaw a software release with 
a critical user-facing bug. In the postmortem, someone asks Em -
ily to evaluate static analysis for 
preventing this type of bug in the 
future. Web searches about static analysis reveal dozens of compa -
nies selling static-analysis tools. Their claims about identifying a wide variety of bugs are clearly 
marketing material. They boast of 
different technologies, at a range of prices. It’s difficult to compare 
them, let alone decide whether 
they should be used at all. Unfortunately, when engineers 
seek answers to their practical prob -
lems, “perfect” scientific knowledge 
is not always available. If it’s not, en -
gineers readily accept “good-enough” evidence: case studies, small-scale  
experiments, blog posts, or even ad -
vice from acknowledged experts. Emily turns to an unbiased 
source: the research literature. Un -
fortunately, searching yields thou -
sands of papers, each evaluating a different technique in a different 
way. Emily is certain the answer 
exists, but she’s not enough of an expert to find it. What happens if even codified 
knowledge is not available? Or if the 
results are unclear, contradictory, or 
fragmented and distributed in many places? Exasperated, Emily consults a 
colleague who used static analysis 
previously at another company. He remembers that it never found enough important issues to justify 
the cost and recommends to just 
write more integration tests. In the end, Emily gives up on static 
analysis, unable to translate de -
cades of research into actionable 
knowledge. She sighs wistfully, 
Authorized licensed use limited to: Access paid by The UC Irvine Libraries.

have criticized common practice in the field for failing to collect, analyze, and report 
experimental measurements in research reports [9,10,11,12]. In 2001 I [8] presented preliminary sketches of some of the successful paradigms for software engineering research, drawing heavily on examples from software architecture. Scientific and engineering research fields can be characterized by identifying what 
they value: 
• What kinds of questions are "interesting"? • What kinds of results help to answer these questions, and what research 
methods can produce these results?

Evidence-based Software Engineering 
 Barbara A. Kitchenham1,3 Tore Dybå2,4 Magne Jørgensen2
 barbara@cs.keele.ac.uk tore.dyba@sintef.no magnej@simula.no 
1National ICT Australia, Locked Bag 9013 Alexandria, NSW 1435, Australia 
2Simula Research Laboratory, P.O. Box 134, NO-1325 Lysaker, Norway 
3Dept.

Writing Good Software Engineering Research Papers 
Minitutorial 
Mary Shaw 
Carnegie Mellon University 
mary.shaw@cs.cmu.edu 
Abstract 
Software engineering researchers solve problems of 
several different kinds. To do so, they produce several 
different kinds of results, and they should develop 
appropriate evidence to validate these results.

4  Introduct ion 
  Confronted by a confusing array of op tions for producing software, software 
engineers need proof that a particular approach or technique is really better than 
another. They need to know the clear-cut be nefits of one approach versus another. They need reliable evidence that one appr oach clearly works better than another. This need to work with facts rather than assumptions is a property of any 
engineering discipline. Very few ideas in SE are matched with empirical data. Ideas, whose truthfulness has not been tested against reality, are continually assumed as evident. For example, the 
famous software crisis (Naur, 1969) was and still is more a question of a subjective 
customer and/or developer impression than a confirmed phenomenon. In fact, we can find some claims that there is no software crisis (see comments about Turski and 
Hoare opinions given in (Maibaum, 1997)). Another example is the idea that traditional engineering disciplines behave  much better than SE (Pezeé, 1997). For 
example, has any empirical study been pe rformed to compare the failure rate of 
products obtained by SE and those produced by other engineering disciplines? Table 
1.1 presents the percentage of faults in the car industry shown in a study carried out by Lex Vehicle Leasing, one of the major world vehicle hire and leasing companies, 
on a universe of 73,700 cars in England. This fault rate could call the reliability of 
the engineering used in this industry into question; however, this is not the case. Therefore, if we are really to  speak of a software crisis, we  need studies to show that 
SE is less reliable than other branches of engineering, that is, the number of faults in SE should be checked against the number of faults in other branches of engineering. Other examples of computer science theo ries that have not been tested are 
functional programming, object-oriented programming or formal methods.

Question: What defines 'evidence' in software engineering research?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

9. Is there evidence that the 
E/QE can be used by other researchers / practitioners? Do the authors discuss whether or how the findi ngs can be transferred to other populations, or 
consider other ways in whic h the research can be used? To what extent do authors interpret results in the context of other studies / the existing body of 
knowledge?

Analysis of claim (d) 

4. Why should the reader believe your result? Show evidence that your result is valid--that it actually 
helps to solve the problem you set out to solve. 4.1. What kinds of validation do software 
engineers do? Software engineers offer several kinds of evidence in 
support of their research results. It is essential to select a 
form of validation that is appropriate for the type of research result and the method used to obtain the result. As an obvious example, a formal model should be 
supported by rigorous derivation and proof, not by one or 
two simple examples. On the other hand, a simple 
example derived from a practical system may play a major 
role in validating a new type of development method. Table 5 lists the types of research validation that are used 
in software engineering research papers and provides 
specific examples. In this table, the examples are keyed to 
the type of result they apply to. Table 5. Types of software engineering research validation 
Type of validation Examples 
Analysis 
Evaluation 
Experience 
Example 
Persuasion I have analyzed my result and find it satisfactory through rigorous analysis, e.g .... For a formal model ...

Theories also play a role in connecting research to the relevant literature. By 
defining the key terms, the results of empirical studies can be compared. Furthermore, theories support the process of empirical induction because an indi-vidual study can never offer conclusive results. Each study adds more evidence for 

 33% (38) 
 8% (9)  65% (74) 
 36% (41) 
 25% (29) 
 12% (14) 
 14% (16) 
 4% (4)  25% (15) 
 7% (4)  74% (45) 
 52% (32) 
 26% (16) 
 13% (8) 
 8% (5) 
 5% (3) Award
NoAward
OtherI carefully calibrate my epistemological
perspective with the authorsI do not validate these artifacts.I do not review these types of papersI demand and/or access data/ scripts and
experimental packageI attempt to run the source code/
container etc. on my machineI check for data consistencyI check for data availability
0% 20% 40% 60%
Number of respondents using that approach
Fig.8 Validate artifacts and claims
look at the chain of evidence from observation to conclusion and consider whether it is clear
and unbroken. I consider different quality criteria and threats to validity depending on the
type of study and its (often implicit) philosophical position.”
Characteristics of impactful papers Figure 9shows the distribution of how respondents
judge the impact of papers. It is interesting to observe that the top three characteristics are
novelty ,relevance ,a n d methodological rigor , which can sometimes be in conflict with each
other.

Understand that this isn’t absolute truth, but rather the best we can do at the moment. You must be thoughtful about using this knowledge, but it’s a lot better than guessing. Sincerely,
The Researchers
Dear Researchers,
We have a lot of questions, and we suspect you have answers. Unfortunately, the answers are scattered among thousands of 
papers, and we can’t tell fact from fiction. Worse, there are entire topics that no one is studying because they aren’t “scientific 
enough.” We have fallen back on getting insights from Hacker News , Stevey’s Drunken Blog Rants , and Jeff, who just transferred 
from Accounting. We’re pretty sure Jeff doesn’t know anything, but he’s the loudest person in our stand-up, and we don’t have 
any evidence to dispute him. We’ll take whatever evidence you have. Sincerely,
The Practitioners
Authorized licensed use limited to: Access paid by The UC Irvine Libraries. Downloaded on May 01,2024 at 00:41:46 UTC from IEEE Xplore. Restrictions apply.

These innovative practitioners 
publish their experimental projects, es tablishing more accurately when the 
researchers’ proposal worked and when it did not, and what improvements were 
observed. The limits and boundaries of the proposal can be defined by 
accumulating real cases. − Routine developers use the new pr oposals at little risk, knowing what 
improvements they can expect from their use (as the improvements are 
supported by experimental studies and not by mere opinions). Some of these developers collect data from their projects and publish the behaviour of the new 
proposals in a host of different circumstances. Thanks to the evidence gathered, 
the researchers’ original proposal is accepted by the community after a few years 
and is considered to be an established fact rather than a mere speculation. 

H. Erdogmus, “How Important Is Evidence, Really?,” IEEE Software , 
vol. 27, no. 3, 2010, pp. 2–5. 12. B.A. Kitchenham, D. Budgen, and P. Brereton, Evidence-Based Software Engineering and Systematic Reviews , 
Chapman & Hall, 2015. 13.

  21 • As a means of weighting the importance of individual studies when results are 
being synthesised. • To guide the interpretation of findings a nd determine the strength of inferences.

generated tools in support of some of these practices [20]. Zhou et al. evaluated 
the quality of papers included in the corpus for a variety of systematic 
literature reviews [31]. Researchers followed these general guidelines 
for review of specific subfields of software engineering, such as use 
case specifications [27]. Sampaio extended guidelines for mapping 
reviews in addition to literature reviews [22]. Runeson et al. followed 
with a set of guidelines for case study research [21].

Question: How is evidence used to support claims in academic contexts?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m


[1m> Entering new StuffDocumentsChain chain...[0m


[1m> Entering new LLMChain chain...[0m
Prompt after formatting:
[32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During the analysis it is important to maintain a chain ofevidence from the findings to the original data. The report should include sufficient dataand examples to allow the reader to understand the chain of evidence. Table 11 Reader ’s checklist items
39. Are the objective, research questions, and hypotheses (if applicable) clear and relevant? 1, 2, 5, 29, 30
40. Are the case and its units of analysis well defined? 1, 5, 28
41. Is the suitability of the case to address the research questions clearly motivated? 8, 9, 1442. Is the case study based on theory or linked to existing literature? 343. Are the data collection procedures sufficient for the purpose of the case study (data sources, collection,
validation)? 11, 13, 16, 18, 21, 31
44. Is sufficient raw data presented to provide understanding of the case and the analysis? 3245. Are the analysis procedures sufficient for the purpose of the case study (repeatable, transparent)? 22, 3346.

Analysis of claim (d) 

Evaluating experiment quality. Even in EBM, it 
is recognised that it is sometimes impossible to perform randomised trials and evidence from other types of 
experiment may need to be considered. The Australian 
National Health and Medical Research Council have published guidelines for evaluating the quality of evidence [2]. They consider:  
•The strength of the evidence . This has three 
elements: Level, Quality, and Statistical Precision. Level relates to the choice of study design and is used as an indicator to which bias has been eliminated by design. Quality refers to the methods used by the investigators to minimize bias within the study design. Statistical Precision refers to the P-value or the confidence interval. •Size of effect . The distance of the estimated treatment 
effect from the null value and the inclusion of 
clinically important effects in the confidence interval. •Relevance of evidence . The usefulness of the 
evidence in clinical practice, particularly the appropriateness of the outcome measures used. These criteria appear to be equally valid for software 
engineering evidence.

 33% (38) 
 8% (9)  65% (74) 
 36% (41) 
 25% (29) 
 12% (14) 
 14% (16) 
 4% (4)  25% (15) 
 7% (4)  74% (45) 
 52% (32) 
 26% (16) 
 13% (8) 
 8% (5) 
 5% (3) Award
NoAward
OtherI carefully calibrate my epistemological
perspective with the authorsI do not validate these artifacts.I do not review these types of papersI demand and/or access data/ scripts and
experimental packageI attempt to run the source code/
container etc. on my machineI check for data consistencyI check for data availability
0% 20% 40% 60%
Number of respondents using that approach
Fig.8 Validate artifacts and claims
look at the chain of evidence from observation to conclusion and consider whether it is clear
and unbroken. I consider different quality criteria and threats to validity depending on the
type of study and its (often implicit) philosophical position.”
Characteristics of impactful papers Figure 9shows the distribution of how respondents
judge the impact of papers. It is interesting to observe that the top three characteristics are
novelty ,relevance ,a n d methodological rigor , which can sometimes be in conflict with each
other.

  24 They also considered the reporting  quality based on 4 questions: 
1. Is it clear what projects were used to construct each model? 2. Is it clear how accuracy was measured? 3. Is it clear what cross-validation method was used? 4.

9. Is there evidence that the 
E/QE can be used by other researchers / practitioners? Do the authors discuss whether or how the findi ngs can be transferred to other populations, or 
consider other ways in whic h the research can be used? To what extent do authors interpret results in the context of other studies / the existing body of 
knowledge?

Has its value been demonstrated? Relevance: is the material appropriate to the scope of the journal to which it is submitted? Motivation: does the problem considered have a sound motivation? Does the paper clearly demonstrate the scientiﬁc interest of the results? Referencing: has reference been made to the most recent and most appropriate work? Is the present work set in the context of the previous work? Clarity: is the English clear and well written? Poorly written English may obscure the scientiﬁc merit of your paper. Are the ideas expressed clearly
and concisely?

  19 Inclusion and exclusion crit eria should be based on th e research question. They 
should be piloted to ensure that they can be  reliably interpreted and that they classify 
studies correctly.

Figure 6. Distribution of acceptances and rejections 
by type of validation 
Is the validation related to the claim? If you're claiming 
performance improvement, validation should analyze 
performance, not ease of use or generality. And 
conversely. Is this such an interesting, potentially powerful idea 
that it should get exposure despite a shortage of concrete 
evidence? Authors tend to have trouble in some specific 
situations. Here are some examples, with advice for 
staying out of trouble: 
• If you claim to improve on prior art, compare your 
result objectively to the prior art. • If you used an analysis technique, follow the rules of 
that analysis technique. If the technique is not a 
common one in soRware engineering (e.g., meta- 
analysis, decision theory, user studies ° or other 
behavioral analyses), explain the technique and 
standards of proof, and be clear about your 
adherence to the technique. • If you offer practical experience as evidence for your 
result, establish the effect your research has. If at all 
possible, compare similar situations with and without 
your result. 733 


Are the review’s inclusion and exclus ion criteria described and appropriate? 2. Is the literature search likely to ha ve covered all relevant studies? 3. Did the reviewers assess the quality/v alidity of the in cluded studies? 4. Were the basic data/studies adequately described? Examples 
 We applied the DARE criteria bot h to Kitchenham et al.’s study 
186H[21] and to Jørgensen’s study 
187H[17]. We gave Kitchenham et al.’s study a score of 4 and Jørgensen’s study a score of 3.5. Other studies scored using the DARE criteria are listed in Appendix 2. From a more general viewpoint, Greenlaugh 188H[12] suggests the following questions: 
• Can you find an important clinical qu estion, which the review addressed? (Clearly, in software engin eering, this should be adapted to refer to an important 
software engineering question.) 
• Was a thorough search done of the a ppropriate databases and were other 
potentially important sources explored? • Was methodological quality assessed and the trials weighted accordingly? • How sensitive are the results to the way that the review has been done?

Question: How should 'relatedness' between evidence and claims be defined and assessed?
Helpful Answer:[0m

[1m> Finished chain.[0m

[1m> Finished chain.[0m
[38;5;200m[1;3mcontent='```\nManuscript Text: All the data used in our study is publicly available.\nReview: The evidence provided to support the claim that all data used in the study is publicly available is the statement itself. However, it would be beneficial to mention where this publicly available data can be accessed or provide references to ensure transparency and reproducibility of the study.\n```\n```\nManuscript Text: All the data used in our study is publicly available.\nReview: The relatedness between the evidence (data being publicly available) and the claim (all data used in the study is publicly available) is clear. However, to strengthen the relatedness, it would be helpful to provide details on how the data was accessed or where it can be found to allow for verification by other researchers.\n```' response_metadata={'token_usage': {'completion_tokens': 159, 'prompt_tokens': 538, 'total_tokens': 697}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-84050543-7f3b-4736-89ea-54ffb19393de-0'[0m[32;1m[1;3m[0m

[1m> Finished chain.[0m
[1m[7m%[27m[1m[0m                                                                                                                                   [0m[27m[24m[J(base) crystalalice@LostRabbits-MacBook-Pro SE_Paper_Evaluator % [K[?2004heexit[?2004l

Script done on Thu May 16 15:35:32 2024
